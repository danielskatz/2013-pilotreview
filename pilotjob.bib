@inproceedings{Pilot-Data-2013,
 author    = {Andre Luckow and
                Mark Santcroos and
                Ole Weidner and
                Ashley Zebrowski and
                Shantenu Jha},
   title     = {Pilot-Data: An Abstraction for Distributed Data},
   journal   = {CoRR},
   volume    = {abs/1301.6228},
   year      = {2013},
   ee        = {http://arxiv.org/abs/1301.6228},
   bibsource = {DBLP, http://dblp.uni-trier.de}
}
  	
@article{1742-6596-331-7-072069,
  author={Xin Zhao and John Hover and Tomasz Wlodek and Torre Wenaus and Jaime Frey and Todd Tannenbaum and Miron Livny and the ATLAS
Collaboration},
  title={PanDA Pilot Submission using Condor-G: Experience and Improvements},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072069},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072069},
  year={2011},
  abstract={PanDA (Production and Distributed Analysis) is the workload management system of the ATLAS experiment, used to run managed production and user analysis jobs on the grid. As a late-binding, pilot-based system, the maintenance of a smooth and steady stream of pilot jobs to all grid sites is critical for PanDA operation. The ATLAS Computing Facility (ACF) at BNL, as the ATLAS Tier1 center in the US, operates the pilot submission systems for the US. This is done using the PanDA "AutoPilot" scheduler component which submits pilot jobs via Condor-G, a grid job scheduling system developed at the University of Wisconsin-Madison. In this paper, we discuss the operation and performance of the Condor-G pilot submission at BNL, with emphasis on the challenges and issues encountered in the real grid production environment. With the close collaboration of Condor and PanDA teams, the scalability and stability of the overall system has been greatly improved over the last year. We review improvements made to Condor-G resulting from this collaboration, including isolation of site-based issues by running a separate Gridmanager for each remote site, introduction of the 'Nonessential' job attribute to allow Condor to optimize its behavior for the specific character of pilot jobs, better understanding and handling of the Gridmonitor process, as well as better scheduling in the PanDA pilot scheduler component. We will also cover the monitoring of the health of the system.}
}

@inproceedings{Silberstein:2009:GEB:1654059.1654071,
 author = {Silberstein, Mark and Sharov, Artyom and Geiger, Dan and Schuster, Assaf},
 title = {GridBot: execution of bags of tasks in multiple grids},
 booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
 series = {SC '09},
 year = {2009},
 isbn = {978-1-60558-744-8},
 location = {Portland, Oregon},
 pages = {11:1--11:12},
 articleno = {11},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1654059.1654071},
 doi = {10.1145/1654059.1654071},
 acmid = {1654071},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Rynge:2011:EUG:2116259.2116599,
 author = {Rynge, Mats and Juve, Gideon and Mehta, Gaurang and Deelman, Ewa and Larson, Krista and Holzman, Burt and Sfiligoi, Igor and Wurthwein, Frank and Berriman, G. Bruce and Callaghan, Scott},
 title = {Experiences Using GlideinWMS and the Corral Frontend across Cyberinfrastructures},
 booktitle = {Proceedings of the 2011 IEEE Seventh International Conference on eScience},
 series = {ESCIENCE '11},
 year = {2011},
 isbn = {978-0-7695-4597-4},
 pages = {311--318},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/eScience.2011.50},
 doi = {10.1109/eScience.2011.50},
 acmid = {2116599},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Condor, glidein, GlideinWMS, Periodograms, CyberShake, Workflow, Corral, OSG, TeraGrid},
}

  	
@article{1742-6596-119-6-062044,
  author={I Sfiligoi},
  title={GlideinWMSâ€”a generic pilot-based workload management system},
  journal={Journal of Physics: Conference Series},
  volume={119},
  number={6},
  pages={062044},
  url={http://stacks.iop.org/1742-6596/119/i=6/a=062044},
  year={2008},
  abstract={The Grid resources are distributed among hundreds of independent Grid sites, requiring a higher level Workload Management System (WMS) to be used efficiently. Pilot jobs have been used for this purpose by many communities, bringing increased reliability, global fair share and just in time resource matching. glideinWMS is a WMS based on the Condor glidein concept, i.e. a regular Condor pool, with the Condor daemons (startds) being started by pilot jobs, and real jobs being vanilla, standard or MPI universe jobs. The glideinWMS is composed of a set of Glidein Factories, handling the submission of pilot jobs to a set of Grid sites, and a set of VO Frontends, requesting pilot submission based on the status of user jobs. This paper contains the structural overview of glideinWMS as well as a detailed description of the current implementation and the current scalability limits.}
}

@techreport{Casanova:1995:NNS:898848,
 author = {Casanova, H. and Dongarra, J.},
 title = {NetSolve: A Network Server for Solving Computational Science Problems},
 year = {1995},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autk_cs%3Ancstrl.utk_cs%2F%2FUT-CS-95-313},
 publisher = {University of Tennessee},
 address = {Knoxville, TN, USA},
}

@inproceedings{Lampson:1983:HCS:800217.806614,
 author = {Lampson, Butler W.},
 title = {Hints for computer system design},
 booktitle = {Proceedings of the ninth ACM symposium on Operating systems principles},
 series = {SOSP '83},
 year = {1983},
 isbn = {0-89791-115-6},
 location = {Bretton Woods, New Hampshire, United States},
 pages = {33--48},
 numpages = {16},
 doi = {10.1145/800217.806614},
 acmid = {806614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@misc{copilot-tr, note = {"Co-Pilot: The Distributed Job Execution
                  Framework", Predrag Buncic, Artem Harutyunyan, Technical Report, 
Portable Analysis Environment using Virtualization Technology (WP9), CERN}}


@article{Berman:2003:ACG:766629.766632,
 author = {Berman, Francine and Wolski, Richard and Casanova, Henri and Cirne, Walfredo and Dail, Holly and Faerman, Marcio and Figueira, Silvia and Hayes, Jim and Obertelli, Graziano and Schopf, Jennifer and Shao, Gary and Smallen, Shava and Spring, Neil and Su, Alan and Zagorodnov, Dmitrii},
 title = {Adaptive Computing on the Grid Using AppLeS},
 journal = {IEEE Trans. Parallel Distrib. Syst.},
 issue_date = {April 2003},
 volume = {14},
 number = {4},
 month = apr,
 year = {2003},
 issn = {1045-9219},
 pages = {369--382},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/TPDS.2003.1195409},
 doi = {10.1109/TPDS.2003.1195409},
 acmid = {766632},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Scheduling, parallel and distributed computing, heterogeneous computing, grid computing.},
}

@misc{venusc-generic-worker,
author={Christian Geuer-Pollmann},
title={The Venus C Generic Worker},
howpublished={SICS Cloud Day},
year="2011"
}

@inproceedings{5171374,
	Author = {Sfiligoi, I. and Bradley, D.C. and Holzman, B. and Mhashilkar, P. and Padhi, S. and Wurthwein, F.},
	Booktitle = {Computer Science and Information Engineering},
	Date-Added = {2012-07-14 09:11:44 +0000},
	Date-Modified = {2012-07-14 09:11:44 +0000},
	Doi = {10.1109/CSIE.2009.950},
	Keywords = {glideinWMS;grid computing;grid middleware;grid resources;job scheduling complexity;open science grid;grid computing;middleware;scheduling;},
	Pages = {428 -432},
	Title = {The Pilot Way to Grid Resources Using GlideinWMS},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CSIE.2009.950}}

@misc{glideinwms,
	Date-Added = {2012-07-14 09:04:56 +0000},
	Date-Modified = {2012-07-14 09:08:36 +0000},
	Howpublished = {\url{http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/index.html}},
	Title = {GlideinWMS - The Glidein-based Workflow Management System},
	Year = {2012}}


@inproceedings{pstar-2012,
	Author = {Andre Luckow and Mark Santcroos and Ole Weider and Andre Merzky and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of The International ACM Symposium on High-Performance Parallel and Distributed Computing},
	Date-Added = {2011-09-04 16:13:49 +0000},
	Date-Modified = {2011-09-04 16:14:42 +0000},
	Title = {Towards a Common Model for Pilot-Jobs},
	Year = {2012}}
	


@article{1742-6596-78-1-012057,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes {\it et al}},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@article{1742-6596-78-1-012057-long,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes and Don Petravick and Bill Kramer and Doug Olson and Miron Livny and Alain Roy and Paul Avery and Kent Blackburn and Torre Wenaus and Frank W{\"u}rthwein and Ian Foster and Rob Gardner and Mike Wilde and Alan Blatecky and John McGee and Rob Quick},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Url = {http://stacks.iop.org/1742-6596/78/i=1/a=012057},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@manual{OMG-CORBA303:2004,
	Key = {OMG},
	Keywords = {2004, corba, omg},
	Month = {M},
	Organization = {Object Management Group},
	Posted-At = {2006-10-04 15:54:44},
	Priority = {0},
	Title = {{Common Object Request Broker Architecture: Core Specification}},
	Year = {2004}}

@article{1742-6596-219-6-062049,
	Abstract = {DIRAC, the LHCb community Grid solution, has pioneered the use of pilot jobs in the Grid. Pilot Jobs provide a homogeneous interface to an heterogeneous set of computing resources. At the same time, Pilot Jobs allow to delay the scheduling decision to the last moment, thus taking into account the precise running conditions at the resource and last moment requests to the system. The DIRAC Workload Management System provides one single scheduling mechanism for jobs with very different profiles. To achieve an overall optimisation, it organizes pending jobs in task queues, both for individual users and production activities. Task queues are created with jobs having similar requirements. Following the VO policy a priority is assigned to each task queue. Pilot submission and subsequent job matching are based on these priorities following a statistical approach.},
	Author = {Adrian Casajus and Ricardo Graciani and Stuart Paterson and Andrei Tsaregorodtsev},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062049},
	Title = {DIRAC pilot framework and the DIRAC Workload Management System},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062049}}

@inproceedings{dare-tg11-gateways,
	Author = {Joohyun Kim and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of TeraGrid'11 Extreme Discovery},
	Title = {Building Gateways for Life-Science Applications using the Distributed Adaptive Runtime Environment (DARE) Framework},
	Year = 2011}

@phdthesis{diane-thesis,
	Author = {Jakub Tomasz Moscicki},
	Date-Added = {2011-04-22 19:11:25 +0200},
	Date-Modified = {2011-04-22 19:15:12 +0200},
	School = {University of Amsterdam},
	Title = {Understanding and Mastering Dynamics in Computing Grids: Processing Moldable Tasks with User-Level Overlay},
	Year = {2011}}

@inproceedings{Doraimani:2008:FGS:1383422.1383429,
	Acmid = {1383429},
	Address = {New York, NY, USA},
	Author = {Doraimani, Shyamala and Iamnitchi, Adriana},
	Booktitle = {Proceedings of the 17th international symposium on High performance distributed computing},
	Doi = {http://doi.acm.org/10.1145/1383422.1383429},
	Isbn = {978-1-59593-997-5},
	Keywords = {caching, data management, file grouping, job scheduling, science grids, trace analysis},
	Location = {Boston, MA, USA},
	Numpages = {12},
	Pages = {153--164},
	Publisher = {ACM},
	Series = {HPDC '08},
	Title = {File grouping for scientific data management: lessons from experimenting with real traces},
	Url = {http://doi.acm.org/10.1145/1383422.1383429},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383422.1383429}}

@inproceedings{ramakrishnan2011,
	Author = {Zacharia Fadika and Elif Dede and Madhusudhan Govindaraju and Lavanya Ramakrishnan},
	Booktitle = {submitted to The International Conference for High Performance Computing, Networking, Storage, and Analysis},
	Title = {Benchmarking MapReduce Implementations for Application Usage Scenarios},
	Year = {2011}}

@inproceedings{weissman2011,
	Author = {Michael Cardosa and Chenyu Wang and Anshuman Nangia and Abhishek Chandra and Jon Weissman},
	Title = {Exploring MapReduce Efficiency with Highly-Distributed Data},
	Year = {2011}}

@inproceedings{jha2011,
	Author = {Shantenu Jha and J.D. Blower and Neil Chue-Hong and Simon Dobson and Daniel S. Katz and Omer Rana},
	Title = {3DPAS: Distributed Dynamic Data-intensive Programming Abstractions and Systems},
	Year = {2011}}

@inproceeding{gray2000,
	Author = {Jim Gray, Prashant Shenoy},
	Howpublished = {\url{http://research.microsoft.com/en-us/um/people/gray/papers/ms_tr_99_100_rules_of_thumb_in_data_engineering.pdf}},
	Title = {Rules of Thumb in Data Engineering},
	Year = {2000}}

@book{hey2009,
	Abstract = {Increasingly, scientific breakthroughs will be powered by advanced computing computingapabilities that help researchers manipulate and explore massive datasets.

The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud computing technologies.

In The Fourth Paradigm: Data-Intensive Scientific Discovery, the collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
	Added-At = {2010-02-16T09:54:37.000+0100},
	Address = {USA},
	Biburl = {http://www.bibsonomy.org/bibtex/28b203c0313656b6ced70c14c86a4c42a/acka47},
	Date-Added = {2011-09-04 20:56:39 +0000},
	Date-Modified = {2011-09-04 20:57:08 +0000},
	Editor = {Tony Hey and Stewart Tansley and Kristin Tolle},
	Interhash = {296450016ca8a5f8ab16ae4d92d1fc15},
	Intrahash = {8b203c0313656b6ced70c14c86a4c42a},
	Keywords = {research scholarly_communication science},
	Publisher = {Microsoft Research},
	Timestamp = {2010-02-16T09:54:37.000+0100},
	Title = {The Fourth Paradigm: Data-Intensive Scientific Discovery},
	Year = 2009,
	Bdsk-Url-1 = {http://research.microsoft.com/en-us/collaboration/fourthparadigm/}}

@article{10.1109/MIC.2011.64,
	Address = {Los Alamitos, CA, USA},
	Author = {Ian Foster},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64},
	Issn = {1089-7801},
	Journal = {IEEE Internet Computing},
	Pages = {70-73},
	Publisher = {IEEE Computer Society},
	Title = {Globus Online: Accelerating and Democratizing Science through Cloud-Based Services},
	Volume = {15},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64}}

@misc{webhdfs,
	Howpublished = {\url{http://hadoop.apache.org/common/docs/r1.0.0/webhdfs.html}},
	Key = {webhdfs},
	Title = {{WebHDFS REST API}},
	Year = 2012}

@inproceedings{Moscicki:908910,
	Abstract = { Distributed analysis environment (DIANE) is the result of R D in CERN IT Division focused on interfacing semi-interactive parallel applications with distributed GRID technology. DIANE provides a master-worker workflow management layer above low-level GRID services. DIANE is application and language-neutral. Component-container architecture and component adapters provide flexibility necessary to fulfill the diverse requirements of distributed applications. Physical transport layer assures interoperability with existing middleware frameworks based on Web services. Several distributed simulations based on Geant 4 were deployed and tested in real-life scenarios with DIANE.},
	Author = {Moscicki, J.T.},
	Booktitle = {Nuclear Science Symposium Conference Record, 2003 IEEE},
	Doi = {10.1109/NSSMIC.2003.1352187},
	Issn = {1082-3654},
	Keywords = {CERN IT Division; GRID-enabled physics data simulation; Geant 4; Web services; component adapters; component-container architecture; distributed analysis environment; interoperability; master-worker workflow management layer; middleware frameworks; physical transport layer; physics data analysis; semi-interactive parallel applications; Internet; data analysis; grid computing; middleware; physics computing; workflow management software;},
	Pages = {1617 - 1620},
	Title = {DIANE - distributed analysis environment for GRID-enabled simulation and analysis of physics data},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/NSSMIC.2003.1352187}}

@misc{bigjob_web,
	Howpublished = {\url{http://saga-project.github.com/BigJob/}},
	Key = {SAGA BigJob},
	Title = {{SAGA BigJob}},
	Year = {2012}}

@misc{pilot_api,
	Author = {{Pilot API}},
	Howpublished = {\url{http://saga-project.github.com/BigJob/apidoc/}},
	Key = {Pilot},
	Year = 2012}

@article{condor-g,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Volume = {5},
	Year = {2002}
	}

@article{condor-g-short,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Volume = {5},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1015617019423}}

@inproceedings{1362680,
	Address = {New York, NY, USA},
	Author = {Ioan Raicu and Yong Zhao and Catalin Dumitrescu and Ian Foster and Mike Wilde},
	Booktitle = {SC '07: Proceedings of the 2007 ACM/IEEE conference on Supercomputing},
	Date-Added = {2008-08-09 21:04:33 +0200},
	Date-Modified = {2008-08-09 21:04:53 +0200},
	Doi = {http://doi.acm.org/10.1145/1362622.1362680},
	Isbn = {978-1-59593-764-3},
	Location = {Reno, Nevada},
	Pages = {1--12},
	Publisher = {ACM},
	Title = {{Falkon: A Fast and Light-Weight TasK ExecutiON Framework}},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1362622.1362680}}

@article{Wilde2011,
	Abstract = {Scientists, engineers, and statisticians must execute domain-specific application programs many times on large collections of file-based data. This activity requires complex orchestration and data management as data is passed to, from, and among application invocations. Distributed and parallel computing resources can accelerate such processing, but their use further increases programming complexity. The Swift parallel scripting language reduces these complexities by making file system structures accessible via language constructs and by allowing ordinary application programs to be composed into powerful parallel scripts that can efficiently utilize parallel and distributed resources. We present Swift's implicitly parallel and deterministic programming model, which applies external applications to file collections using a functional style that abstracts and simplifies distributed parallel execution.},
	Author = {Michael Wilde and Mihael Hategan and Justin M. Wozniak and Ben Clifford and Daniel S. Katz and Ian Foster},
	Doi = {10.1016/j.parco.2011.05.005},
	Issn = {0167-8191},
	Journal = {Parallel Computing},
	Keywords = {Dataflow},
	Number = {9},
	Pages = {633--652},
	Title = {Swift: A language for distributed parallel scripting},
	Volume = {37},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167819111000524},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.parco.2011.05.005}}

@misc{coasters,
	Howpublished = {\url{http://wiki.cogkit.org/wiki/Coasters}},
	Key = {coasters},
	Title = {Coasters},
	Year = 2009}

@misc{topos,
	Howpublished = {\url{https://grid.sara.nl/wiki/index.php/Using_the_Grid/ToPoS}},
	Key = {Topos},
	Title = {ToPoS - A Token Pool Server for Pilot Jobs},
	Year = 2011}

@inproceedings{1652061,
	Author = {Walker, E. and Gardner, J.P. and Litvin, V. and Turner, E.L.},
	Booktitle = {Challenges of Large Applications in Distributed Environments, 2006 IEEE},
	Doi = {10.1109/CLADE.2006.1652061},
	Keywords = {Condor;NSF TeraGrid;Sun Grid Engine cluster;cooperative system;distributed computing environment;personal adaptive cluster;resource management;scientific job management;grid computing;resource allocation;workstation clusters;},
	Month = {0-0},
	Pages = {95-103},
	Title = {Creating personal adaptive clusters for managing scientific jobs in a distributed computing environment},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CLADE.2006.1652061}}

@article{10.1109/HPC.2000.846563,
	Address = {Los Alamitos, CA, USA},
	Author = {Rajkumar Buyya and David Abramson and Jonathan Giddy},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563},
	Isbn = {0-7695-0589-2},
	Journal = {International Conference on High-Performance Computing in the Asia-Pacific Region},
	Pages = {283-289},
	Publisher = {IEEE Computer Society},
	Title = {Nimrod/G: An Architecture for a Resource Management and Scheduling System in a Global Computational Grid},
	Volume = {1},
	Year = {2000},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563}}

@article{1742-6596-219-6-062041,
	Abstract = {The Panda Workload Management System is designed around the concept of the Pilot Job -- a "smart wrapper" for the payload executable that can probe the environment on the remote worker node before pulling down the payload from the server and executing it. Such design allows for improved logging and monitoring capabilities as well as flexibility in Workload Management. In the Grid environment (such as the Open Science Grid), Panda Pilot Jobs are submitted to remote sites via mechanisms that ultimately rely on Condor-G. As our experience has shown, in cases where a large number of Panda jobs are simultaneously routed to a particular remote site, the increased load on the head node of the cluster, which is caused by the Pilot Job submission, may lead to overall lack of scalability. We have developed a Condor-inspired solution to this problem, which is using the schedd-based glidein, whose mission is to redirect pilots to the native batch system. Once a glidein schedd is installed and running, it can be utilized exactly the same way as local schedds and therefore, from the user's perspective, Pilots thus submitted are quite similar to jobs submitted to the local Condor pool.},
	Author = {Po-Hsiang Chiu and Maxim Potekhin},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062041},
	Title = {Pilot factory -- a Condor-based system for scalable Pilot Job generation in the Panda WMS framework},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062041}}

@misc{dpa_surveypaper,
	Note = {S. Jha et al., {\em Programming Abstractions for Large-scale Distributed Application s}, Submitted to ACM Computing Surveys; draft at \url{http://www.cct.lsu.edu/~sjha/publications/dpa_surveypaper.pdf}}}

@misc{egi,
	Author = {{EGI}},
	Howpublished = {\url{http://www.egi.eu/}},
	Key = {egi},
	Year = {2012}}

@article{bfast2009,
	Author = {Homer, N. and Merriman, B. and Nelson, S. F.},
	Journal = {PLoS One},
	Number = {11},
	Pages = {e7767},
	Title = {{BFAST : An alignment tool for large scale genome resequencing}},
	Volume = {4},
	Year = {2009}}

@misc{saga_rm,
	Author = {Andre Merzky},
	Booktitle = {OGF Draft},
	Howpublished = {\url{https://svn.cct.lsu.edu/repos/saga-ogf/trunk/documents/saga-package-resource/}},
	Title = {{SAGA Resource Management API (DRAFT)}},
	Year = 2011}

@misc{saga_advert,
	Author = {Andre Merzky},
	Howpublished = {OGF Document Series 177, \url{http://www.gridforum.org/documents/GFD.177.pdf}},
	Title = {{SAGA API Extension: Advert API}},
	Year = 2011}

@misc{redis,
	Author = {{Redis}},
	Howpublished = {\url{http://redis.io/}},
	Key = {Redis},
	Year = 2012}

@misc{zmq,
	Author = {{ZeroMQ}},
	Howpublished = {\url{http://www.zeromq.org/}},
	Key = {zmq},
	Year = 2012}

@misc{fg,
	Howpublished = {\url{https://portal.futuregrid.org/}},
	Key = {FutureGrid},
	Title = {{FutureGrid: An Experimental, High-Performance Grid Test-bed}},
	Year = 2012}

@misc{xsede,
	Howpublished = {\url{https://www.xsede.org/}},
	Key = {XSEDE},
	Title = {{XSEDE: Extreme Science and Engineering Discovery Environment}},
	Year = 2012}

@article{Gelernter:1985:GCL:2363.2433,
	Acmid = {2433},
	Address = {New York, NY, USA},
	Author = {Gelernter, David},
	Doi = {http://doi.acm.org/10.1145/2363.2433},
	Issn = {0164-0925},
	Issue = {1},
	Journal = {ACM Trans. Program. Lang. Syst.},
	Month = {January},
	Numpages = {33},
	Pages = {80--112},
	Publisher = {ACM},
	Title = {Generative communication in Linda},
	Url = {http://doi.acm.org/10.1145/2363.2433},
	Volume = {7},
	Year = {1985},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2363.2433}}

@misc{loni,
	Author = {{LONI}},
	Howpublished = {\url{http://www.loni.org}},
	Key = {loni},
	Year = {2011}}

@article{Moscicki20092303,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224â€‰590

No. of bytes in distributed program, including test data, etc.: 14â€‰365â€‰315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki {\it et al}},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@article{Moscicki20092303-long,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224â€‰590

No. of bytes in distributed program, including test data, etc.: 14â€‰365â€‰315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki and F. Brochu and J. Ebke and U. Egede and J. Elmsheuser and K. Harrison and R.W.L. Jones and H.C. Lee and D. Liko and A. Maier and A. Muraru and G.N. Patrick and K. Pajchel and W. Reece and B.H. Samset and M.W. Slater and A. Soroko and C.L. Tan and D.C. van der Ster and M. Williams},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@misc{leaky_abstractions,
	Author = {{Joel Spolsky}},
	Note = {\url{http://www.joelonsoftware.com/articles/LeakyAbstractions.html}},
	Title = {{The Law of Leaky Abstractions}}}

%http://iopscience.iop.org/1742-6596/396/3/032116?fromSearchPage=true
@article{bosco,
  author={Derek Weitzel and Dan Fraser and Brian Bockelman and David Swanson},
  title={Campus Grids: Bringing Additional Computational Resources to HEP Researchers},
  journal={Journal of Physics: Conference Series},
  volume={396},
  number={3},
  pages={032116},
  url={http://stacks.iop.org/1742-6596/396/i=3/a=032116},
  year={2012},
  abstract={It is common at research institutions to maintain multiple clusters that represent different owners or generations of hardware, or that fulfill different needs and policies. Many of these clusters are consistently under utilized while researchers on campus could greatly benefit from these unused capabilities. By leveraging principles from the Open Science Grid it is now possible to utilize these resources by forming a lightweight campus grid. The campus grids framework enables jobs that are submitted to one cluster to overflow, when necessary, to other clusters within the campus using whatever authentication mechanisms are available on campus. This framework is currently being used on several campuses to run HEP and other science jobs. Further, the framework has in some cases been expanded beyond the campus boundary by bridging campus grids into a regional grid, and can even be used to integrate resources from a national cyberinfrastructure such as the Open Science Grid. This paper will highlight 18 months of operational experiences creating campus grids in the US, and the different campus configurations that have successfully utilized the campus grid infrastructure.}
}

% IEEE http://ieeexplore.ieee.org.proxy.libraries.rutgers.edu/xpl/articleDetails.jsp?tp=&arnumber=6266981&contentType=Conference+Publications&searchField%3DSearch_All%26queryText%3DPerformance+improvements+for+the+neoclassical+transport+calculation+on+Grid+by+means+of+pilot+jobs
@INPROCEEDINGS{gwpilot,
author={Rubio-Montero, A.J. and Castejon, F. and Huedo, E. and Rodriguez-Pascual, M. and Mayo-Garcia, R.},
booktitle={High Performance Computing and Simulation (HPCS), 2012 International Conference on}, title={Performance improvements for the neoclassical transport calculation on Grid by means of pilot jobs},
year={2012},
month={july},
volume={},
number={},
pages={609 -615},
keywords={Databases;Dispatching;Europe;Middleware;Plasmas;Standards;Suspensions;Tokamak devices;grid computing;middleware;physics computing;plasma confinement;plasma transport processes;DRMAA-enabled DKEsG version;drift kinetic equation solver;fusion device;generic pilot-job platform;grid infrastructure;monoenergetic coefficient;neoclassical transport;plasmas;standard grid middleware;stellarator;tokamaks;transport coefficient;Drift Kinetic Equation solver;TJ-II;late-binding;neoclassical transport;pilot jobs;},
doi={10.1109/HPCSim.2012.6266981},
ISSN={},}

@inproceedings{masterworker,
 author = {Heymann, Elisa and Senar, Miquel A. and Luque, Emilio and Livny, Miron},
 title = {Adaptive Scheduling for Master-Worker Applications on the Computational Grid},
 booktitle = {Proceedings of the First IEEE/ACM International Workshop on Grid Computing},
 series = {GRID '00},
 year = {2000},
 isbn = {3-540-41403-7},
 pages = {214--227},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=645440.652833},
 acmid = {652833},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
} 

@INPROCEEDINGS{Goux00anenabling,
    author = {Jean-pierre Goux and Sanjeev Kulkarni and Jeff Linderoth and Michael Yoder},
    title = {An Enabling Framework for Master-Worker Applications on the Computational Grid},
    booktitle = {Cluster Computing},
    year = {2000},
    pages = {43--50},
    publisher = {Society Press}
}
