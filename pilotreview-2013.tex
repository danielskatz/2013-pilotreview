\documentclass{sig-alternate}
%\documentclass[conference]{IEEEtran}
%\documentclass[conference,final]{IEEEtran}

\input{head}
\input{include}
\usepackage{lscape}

\usepackage{listings}

\lstnewenvironment{code}[1][]%
{
\noindent
%\minipage{0.98 \linewidth}
\minipage{1.0 \linewidth}
\vspace{0.5\baselineskip}
\lstset{
    language=Python,
%    numbers=left,
%    numbersep=4pt,
    frame=single,
    captionpos=b,
    stringstyle=\ttfamily,
    basicstyle=\scriptsize\ttfamily,
    showstringspaces=false,#1}
}
{\endminipage}

\begin{document}
%\conferenceinfo{HPDC'13}{2013, New York, USA}
% \conferenceinfo{ECMLS'11,} {June 8, 2011, San Jose, California, USA.}
\CopyrightYear{2015}
% \crdata{978-1-4503-0702-4/11/06}
% \clubpenalty=10000
% \widowpenalty = 10000

\title{A Comprehensive Perspective on Pilot-Abstraction}

% \alignauthor
% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{1932 Wallamaloo Lane}\\
%        \affaddr{Wallamaloo, New Zealand}\\
%        \email{trovato@corporation.com}
% % 2nd. author
% \alignauthor
% G.K.M. Tobin\titlenote{The secretary disavows
% any knowledge of this author's actions.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{P.O. Box 1212}\\
%        \affaddr{Dublin, Ohio 43017-6221}\\
%        \email{webmaster@marysville-ohio.com}
% % 3rd. author
% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
% one who did all the really hard work.}\\
%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%        \affaddr{Hekla, Iceland}\\
%        \email{larst@affiliation.org}
% \and  % use '\and' if you need 'another row' of author names
% % 4th. author
% \alignauthor Lawrence P. Leipuner\\
%        \affaddr{Brookhaven Laboratories}\\
%        \affaddr{Brookhaven National Lab}\\
%        \affaddr{P.O. Box 5000}\\
%        \email{lleipuner@researchlabs.org}
% % 5th. author
% \alignauthor Sean Fogarty\\
%        \affaddr{NASA Ames Research Center}\\
%        \affaddr{Moffett Field}\\
%        \affaddr{California 94035}\\
%        \email{fogartys@amesres.org}
% % 6th. author
% \alignauthor Charles Palmer\\
%        \affaddr{Palmer Research Laboratories}\\
%        \affaddr{8600 Datapoint Drive}\\
%        \affaddr{San Antonio, Texas 78229}\\
%        \email{cpalmer@prl.com}
% }

\date{}
\maketitle

\begin{abstract}
  There is no agreed upon definition of \pilotjobs; however a
  functional attribute of \pilotjobs that is generally agreed upon is
  they are tools/services that support multi-level and/or
  application-level scheduling by providing a scheduling overlay on
  top of the system-provided schedulers.
  Nearly everything else is either specific to an
  implementation, open to interpretation or not agreed upon. For
  example, are \pilotjobs part of the application space, or part of
  the services provided by an infrastructure? We will see that
  close-formed answers to questions such as whether \pilotjobs are
  system-level or application-level capabilities are likely to be
  elusive. Hence, this paper does not make an attempt to provide
  close-formed answers, but aims to provide appropriate context,
  insight and analysis of a large number of \pilotjobs, and thereby
  bring about a hitherto missing consilience in the community's
  appreciation of \pilotjobs.  Specifically this paper aims to provide
  a comprehensive survey of \pilotjobs, or more generically of
  \pilotjob like capabilities.  A primary motivation for this work stems
  from our experience when looking for an interoperable, extensible
  and general-purpose \pilotjob; in the process, we realized that
  such a capability did not exist. The situation was however even more
  unsatisfactory: in fact there was no agreed upon definition or
  conceptual framework of \pilotjobs.  To substantiate these points of
  view, we begin by sampling (as opposed to a comprehensive survey)
  ~\onote{a few lines above we say that we're doing a comprehensive
    survey!} some existing \pilotjobs and the different aspects of
  these \pilotjobs, such as the applications scenarios that they have
  been used and how they have been used. The limited but sufficient
  sampling highlights the variation, and also provides both a
  motivation and the basis for developing an implementation agnostic
  terminology and vocabulary to understand \pilotjobs; Section \S3
  attempts to survey the landscape/eco-system of \pilotjobs.  With an
  agreed common framework/vocabulary to discuss and describe
  \pilotjobs, we proceed to analyze the most commonly utilized
  \pilotjobs and in the process provide a comprehensive survey of
  \pilotjobs, insight into their implementations, the infrastructure
  that they work on, the applications and application execution modes
  they support, and a frank assessment of their strengths and
  limitations.  An inconvenient but important question -- both
  technically and from a sustainability perspective that must be
  asked: why are there so many similar seeming, but partial and
  slightly differing implementations of \pilotjobs, yet with very
  limited interoperability amongst them?  Examining the reasons for
  this state-of-affairs provides a simple yet illustrative case-study
  to understand the state of the art and science of tools, services
  and middleware development.  Beyond the motivation to understand the
  current landscape of \pilotjobs from both a technical and a
  historical perspective, we believe a survey of \pilotjobs is a
  useful and timely undertaking as it provides interesting insight
  into understanding issues of software sustainability.
  % believe that a survey of \pilotjobs provides and appreciation for
  % the richness of the \pilotjobs landscape.  is
  % not to discuss the \pstar conceptual framework, but That led to
  % the \pstar model.
\end{abstract}

\section{Introduction}
\label{sec:intro}

% \jhanote{Generally not good style to begin new subsection immediately
%   after section starting}
% \jhanote{Now develop the following paragraph along the lines of: Why
%   have \pilotjobs been successful?}

% \jhanote{Although pilotjobs have solved/addressed many problems, now
%     develop the problem with \pilotjobs themselves..}

The seamless uptake of distributed infrastructures by scientific applications
has been limited by the lack of pervasive and simple-to-use abstractions at the
development, deployment, and execution level. Of all the abstractions proposed
to support effective distributed resource utilization, a survey of actual usage
suggested that \pilotjob \mtnote{Should we use just `pilot'} is arguably one of
the most widely-used distributed computing abstractions - as measured by the
number and types of applications that use them, as well as the number of
production distributed cyberinfrastructures that support them. \msnote{ref?}

The fundamental reason for the success of the \pilotjob abstraction is that
\pilotjobs facilitate the oterwise challenging mapping of specific tasks
onto explicit heterogeneous and dynamic resource pools. \pilotjobs decouple the
workload specification from the task management improving the efficiency of
task assignment while shielding applications from having to manage tasks across
such resources.\onote{not sure if 'load-balance' is appropriate
here}\mtnote{Changed into the more generic `manage'} Another concern often
addressed by \pilotjobs is fault tolerance which commonly refers to the ability
of the \pilotjob system to verify the execution environment before executing
jobs. The \pilotjob abstraction is also a promising route to address specific
requirements of distributed scientific applications, such as coupled-execution
and application-level scheduling~\cite{ko-efficient,DBLP:conf/hpdc/KimHMAJ10}.

%   \onote{I think the most important reasons why Pilot Jobs being so
%     popular (and re-invented over and over again) is that they allow
%     the execution of small (i.e., singe / few-core) tasks efficiently
%     on HPC infrastrucutre by massively reducing queueing time. HPC
%     sites (from schedulers to policies) have always been (and still
%     are) discrimatory against this type of workload in favor of the
%     large, tightly-coupled ones. Pilot-Jobs try to counteract. While
%     this is certainly not the main story that we want to tell, this
%     should IMHO still be mentioned. } \jhanote{This is definitely one
%     of the main reasons, but as Melissa pointed out it during RADICAL
%     call, it is by no means the only reason. Need to get the different
%     reasons down here.. then find a nice balance and description}

A variety of PJ frameworks have emerged: Condor-G/ Glide-in~\cite{condor-g},
Swift~\cite{Wilde2011}, DIANE~\cite{Moscicki:908910},
DIRAC~\cite{1742-6596-219-6-062049}, \panda~\cite{1742-6596-219-6-062041},
ToPoS~\cite{topos}, Nimrod/G~\cite{10.1109/HPC.2000.846563},
Falkon~\cite{1362680} and MyCluster~\cite{1652061} to name a few. Although they
are all, for the most part, functionally equivalent -- they support the
decoupling of workload submission from resource assignment -- it is often
impossible to use them interoperably or even just to compare them functionally
or qualitatively. The situation is reminiscent of the proliferation of
functionally similar yet incompatible workflow systems, where in spite of
significant {\it a posteriori} effort on workflow system extensibility and
interoperability, %(thus providing {\it post-facto} justification of its needs)
these objectives remain difficult if not infeasible.

\mtnote{Should we have a paragraph explaining the core contribution offered by
this paper?}

The remainder of this paper is divided into four Sections. \S\ref{sec:history}
offers a critical review of how the concept of \pilotjobs has evolved by
analyzing existing \pilot systems and systems with pilot-like characteristics.
In~\S\ref{sec:understanding}, the hetoregeneity described
in~\S\ref{sec:history} is addressed by deriving the minimal set of capabilities
and properties that has to characterize the design of a \pilot system. A
vocabulary is then defined so that it can be used consistently across different
designs and implmentations of a \pilot system. In~\S\ref{sec:analysis}, the
focus shifts from analysing the design of a \pilot system to  leveraging the
terminology defined in~\S\ref{sec:understanding} to critically reviewing the
characteritics and functionalities of a relevant set of \pilot system
implementations. Finally,~\S\ref{sec:5} closes the paper by outlining the
\pilot paradigm and elaborating on how it impacts and relates to other
middleware and the application layer. The generality and breath of the
\pilot paradigm is underlined by showing its adoption by the
Enterprise, outside the boundaries of scientific research.

% -----------------------------------------------------------------------------
% SECTION 2
%
\section{Functional Evolution of Pilot-Jobs}
\label{sec:history}

% -----------------------------------------------------------------------------
% Version 0.1
% -----------------------------------------------------------------------------

% \subsection{A Functional Approach to Pilot-Jobs}
% Many scientific communities began running into the same issues:

% As distributed systems grew in capacity and capability, they also grew
% in complexity and heterogeneity. For example, many machines
% implemented their own batch queuing systems, and oftentimes these
% systems varied from machine to machine.\msnote{The part after the
% comma is kind of implicit by the part before} The wide use of
% heterogenous resources, resulted in the need for workload management
% across these resources.  In order to harness the power of these
% heterogeneous resources to run jobs, one particular solution proposed
% is that of
% \pilotjobs

% , which have historically been used as a means of solving these
% issues. This gave rise to the the need for job submission management
% via batch queuing systems and middleware access also grew. We briefly
% discuss some specific uses of \pilotjobs below.

% \pilotjobs are most commonly used for the execution of many tasks
% through the use of a container job. They are often measured by their
% throughput, that is, the number of tasks that they can complete per
% second (tps)\msnote{I dont think we use tps further in the paper}, or
% alternatively, by the total number of tasks executed. As such,
% \pilotjobs are used to achieve high-throughput, for example, when
% using genome sequencing techniques \msnote{Arguably genome
% applications are not the most illustrative example of high throughput
% tasks getting benefit out of pilot-jobs} or ensemble-based
% applications. \pilotjobs have also been used for parameter sweeps,
% chained tasks, and loosely-coupled but distinct tasks. %note to self:
% cite these with papers

% Multi-scale simulations have also benefited from the use of
% \pilotjobs. A framework for load balancing via dynamic resource
% allocation for coupled multi-physics (MPI-based) simulations using
% \pilotjobs was demonstrated in Ref.~\cite{ko-efficient}. This was
% achieved by dynamically assigning more processors to jobs with longer
% runtimes, so that these jobs could accomplish their workload in the
% same amount of wall-clock time as those with shorter runtimes. This
% led to an overall reduction of jobs that were waiting to communicate
% via MPI, and an overall reduction of the total simulation runtime.

% \pilotjobs can be used for  simulations with varying numbers of tasks
%  to complete, for example, molecular dynamics simulations requiring
%  task restart. These types of simulations may start with a fixed
%  number of tasks but spawn more tasks in order to continue simulating.
%  \pilotjobs can be utilized for these types of dynamic simulations,
%  because new tasks can be fed to the \pilot at any time within a given
%  runtime. Without \pilotjobs, these simulations would have to be
%  resubmitted to the batch queue and wait for their time to become
%  active again~\cite{luckow2009adaptive}.

% \pilotjobs have also been used to avoid queue wait times for many jobs
% \msnote{I think we just said this above in the high-throughput case}
% as well as harness and utilize different resources (with different
% batch queueing systems) to do \textit{scale-across} simulations. As a
% fault tolerant mechanism, many \pilotjob systems monitor failed jobs
% and have the ability to restart them within the given time frame of
% the \pilotjob's total
% runtime~\cite{1742-6596-219-6-062049,condor-g,nilsson2011atlas}.

% In order to appreciate \pilotjobs, we outline the evolution of
% \pilot-like capabilities ultimately leading to the creation of the
% first actual \pilotjob. We present a brief chronological order of
% \pilotjob-like systems, beginning with simple Master-Worker-based
% applications through advanced workload management systems.

% \subsubsection*{The Evolution of \pilotjobs}\label{sssec:evolution}

% \pilotjobs provide the ability to distribute workload across multiple
% systems and offer an easy way to schedule many jobs at one time. This
% in turn improves the utilization of resources\msnote{why?}, reduces
% the net wait time of a collection of tasks, and also prevents
% saturation of resource batch queuing systems from high-throughput
% simulations where many jobs need to be run at one time\msnote{I dont
% understand the last claim}. While early \pilot-systems solely provided
% this placeholder job mechanism, many of these system evolved to more
% complex workload management systems. As applications began to utilize
% distributed cyberinfrastructure, the workloads grew from small sets of
% short running jobs to many jobs with either short or potentially long
% runtimes. There was a need for more complex management of these
% workloads and additional capabilities for user-level control of the
% tasks that would be executed within the placeholder job. This drove
% the creation of the modern idea of \pilots \msnote{Kind of an ambigous
% statement, like all statements that include the word "modern" :-)}

% \pilotjob systems differ in their focus and architecture. Our
% preliminary survey of existing \pilotjobs helped to identify three
% major layers that these systems exhibit: (i) core \pilotjob
% functionality - this provides the minimally complete set of
% capabilities for a simple \pilotjob, (ii) advanced \pilotjob
% functionality - a system that offers all of (i) plus a more
% sophisticated resource management mechanism, and (iii) higher-level
% \pilot-based frameworks - frameworks utilize \pilots for a specific
% use case, e.\,g.\ workflows or data analytics.

% \onote{These are not necessarily 'layers'. The more I think about it,
% the whole idea of 'layers' doesn't make so much sense if we want to
% distinguish between core and advanced systems / frameworks. I think
% discussing these 'functionality' along \textbf{orthogonal components}
% (that doesn't necessarily build upon each other) would make more
% sense. My (and ALs) comment w.r.t Figure 1 is related to this.}
% \msnote{I'm tempted to at this stage in the paper use a very high
% level figure to simply support the explanation of the pilotjob
% concept.}

% As one can intuit from the above descriptions, existing \pilotjob
% systems maybe overlap and overflow into these different layers, and
% each layer builds upon the previous one. Therefore, we use these
% classification layers only as a means to explain the basic progression
% of a \pilotjob system from simple scheduling reservation mechanisms to
% more complete job management systems. A more semantically-rich
% terminology and classification scheme will be presented in Sections
% \ref{sec:vocab} and \ref{sec:analysis}.\msnote{I think the whole layering
% discussion can go.}


% Figure~\ref{fig:figures_classification} categorized \pilotjob systems
% into three layers: (i) core \pilotjob systems that solely provide a
% simple \pilot capability, (ii) advanced \pilotjob systems that offer
% sophisticated resource management capabilities based on \pilots, and
% (iii) higher level \pilot-based frameworks that utilize \pilots for a
% specific use case, e.\,g.\ workflows or data analytics. One of the
% important aspects of these layers is that they often overlap or have
% evolved from one another. Therefore, it is hard to classify

% \begin{figure}[t]
%	\centering
%		\includegraphics[width=0.45\textwidth]{figures/classification}
%	\caption{Pilot-Job Classification: Different PJ systems focus
%         on different parts of the distributed computing stack: (i)
%          PJ systems that solely provide the \pilot capability, (ii)
%          systems that offer resource management capabilities based on
%          \pilots and (iii) applications, tools and services that
%          utilize \pilots for resource management. \jhanote{we should
%            make the three levels of the diagram consistent with the
%            three categories, ``core PJ'' , ``advanced PJ'' and
%            ``higher-level PJs'' . Also earlier comment about adding
%            ``higher-level pilot-based frameworks'' to ``higher-level
%            frameworks that can use pilot-jobs''.}}  \alnote{mention
%          that these layers are not cleanly separated, add
%          capabilities (outside)/properties (internal) of
%          each layer, what is the overlap between the layers (how they
%          interrelated?, what is the overlap?)?}
%          \onote{IMHO this figure doesn't really help to explain things
%          and is also wrong (see AL's comment above). I think we should
%          get rid of it.  } \msnote{+1}
%	\label{fig:figures_classification}
%\end{figure}

% In this section, we give a brief overview of different \pilotjob
% systems. As grid computing advanced in its size and capabilities, the
% need for job scheduling and time-sharing became more prevalent. Batch
% queuing systems were installed to solve this problem, wherein a login
% node accepted all job submissions and then the queuing system divided
% the work to the worker nodes. The requirements of distributed
% applications, such as efficient load balancing and resource
% utilization across multiple resources, drove the need for user-level
% control of tasks and ease of use of job descriptions for data driven
% applications~\cite{ko-efficient}~\cite{DBLP:conf/hpdc/KimHMAJ10}, but
% the concept of a \pilot was not the first type of application-level
% scheduling introduced.

% -----------------------------------------------------------------------------
% Version 0.2
% -----------------------------------------------------------------------------

% , it was an early mentioning of large scale parallel computing. In
% determining the necessary processing power for weather forecasting, he
% estimated that 64000 human \textit{computers} would be required for solving
% the equations. These computers would all be assigned a part of the globe by a
% central \textit{senior clerk}. The computers would perform their calculations
% and the results would be collected by the clerk. This was in effect a
% Master-Worker pattern.

% \jhanote{Mark: I propose the following: One common use for the M-W scheme is
% to serve as the coordination substrate for PJs. I know its a bit nebulous,
% but it connects the two concepts directly, which is the goal here.}
% \msnote{Interesting contrast. Do we see M/W as a communication pattern to
% implement PJs or do PJs enable the M/W pattern to applicatons?}
% \mtnote{Please see~\S\ref{sec:compsandfuncs}, 10th paragraph: "As see in..."}

% As established in the introduction, \pilotjobs have proven to be an effective
% tool for task-level parallelism. \mtnote{Do we introduce task-level
% parallelism explicitly?} (Note that the current term \textit{\pilotjob} is
% not as old as the concept itself, and the word \pilot for this concept was
% likely introduced around 2004 in the context of the LHC data challenge.
% \alnote{the concept or the name? figure 2 goes back way further than 2004} It's
% first written appearance is in a 2005 LHCb report\cite{lhcb2005}).
% \msnote{find a nice place for this line} One common use for the \pilotjob
% paradigm is to enable the Master-Worker (M-W) scheme \mtnote{We use pattern
% istead of schema in the next paragraph.} and its associated frameworks for
% applications~\cite{Shao:2000:masterslave}.

% When Lewis Fry Richardson in 1922 devised his \textit{Forecast Factory}
% (Figure~\ref{fig:figures_forecast-factory}), it was an early mentioning of
% large scale parallel computing. In determining the necessary processing power
% for weather forecasting, he estimated that 64000 human \textit{computers}
% would be required for solving the equations. These computers would all be
% assigned a part of the globe by a central \textit{senior clerk}. The
% computers would perform their calculations and the results would be collected
% by the clerk. This was in effect a Master-Worker pattern.

% Figure~\ref{fig:timeline} shows the introduction of the discussed systems and
% terms. When available, the date of first mention in a publication or
% otherwise the release date of software implementation is used.

% In the context of present distributed systems, the M-W scheme was initially
% used for farming tasks from a master to a various number of workers, and
% could easily be adapted to run in a platform-independent way across
% potentially heterogeneous resources~\cite{masterworker, Goux00anenabling}.
% M-W based frameworks could respond to dynamically changing resources by
% adapting the number of workers to match the resource
% availability.\mtnote{Should this paragraph be expanded so to include some
% description/example of the referenced distributed systems and capabilities?}

% As distributed computing infrastructures became more popular and available,
% user demand drove the need for efficient shared allocation of heterogeneous
% resources. Leveraging the batch processing concept, first used in the time of
% punchcards [ref], job schedulers were created to accommodate these needs,
% often called ``batch queuing systems''. The adoption of batch queuing meant
% users were expected to submit their tasks to the job scheduler (queue) of
% each cluster and grid system.
% \msnote{From here its an obvious (side)-step to meta scheduling?}

% Often, the type of scheduler on a one machine was different than that of
% another machine. Clearly, there was a need for managing these heterogeneous,
% dynamic grid environments, especially in terms of dynamic scheduling.

% \ldots

% This pattern was implemented with manageable overheads and could easily be
% adapted to run in a platform-independent way across potentially heterogeneous
% resources~\cite{masterworker, Goux00anenabling}. M-W based frameworks could
% respond to dynamically changing resources by adapting the number of workers to
% match the resource availability.\mtnote{To be aggregated in the new version}

% \ldots

% \msnote{\cite{Gehring:1996:mars} looks just as relevant and predates apples:
% "... Note: AppLeS more active than MARS now MARS uses accumulated statistical
% data on previous execution runs of the same application to derive an improved
% task-to-process mapping"}.

% Commented out GHS for now.
%
% The rise of application-level scheduling, as in AppLeS, opened new
% possibilities to Grid environments. The concept of application-level
% scheduling was extended to include long-term performance prediction in
% heterogenous Grid environments via the Grid Harvest Service (GHS)
% system~\cite{ghs}. GHS provides a prediction model that was derived by
% probability analysis and simulation and useful for large-scale applications
% in shared environments. Its prediction models and task scheduling algorithms
% are utilized in the placement of tasks across Grid resources. GHS supports
% three classes of task scheduling: (i) single task, (ii) parallel processing,
% and (iii) meta-task. The performance evaluation and modeling in conjunction
% with task-specific management (such as placement, scheduling, and execution)
% allows the utilization of many heterogenous resources in an efficient manner.

% Put real dates in the comment here.
% Boinc: X
% BigJob: 200X
% etc.

% complexities of task management and coordination to the application layer.
% In order to isolate the application itself from the scheduling of resource
% placeholders,

% , due to its design it required changes to the application itself. For
% obvious reasons this is not always a desired situation and there was a need
% to have user-level control of scheduling without altering the original
% application. This brought about the idea of placeholder scheduling.


%  in that it was an abstraction layer above the various batch
% queuing systems available on different resources. It held a \textit{place} in
% the regular batch queue, and when it became active, it could pull tasks to
% execute.

%Core \pilotjob systems focus on the basic \pilot capabilities, i.\,e.\ the
%provisioning of the placeholder job capability. Various Master-Worker systems
%that provide such a mechanism (e.\,g.\
%Nimrod-G~\cite{10.1109/HPC.2000.846563}). Condor-G/Glide-In is the most
%well-known \pilotjob system.  \msnote{Im tempted to not make claims like this,
%especially not without backing up.}

%Further examples for lightweight \pilotjob systems are: ToPoS~\cite{topos},
%MyCluster~\cite{Walker:2007:PAC:1285840.1285848}, MySGE~\cite{mysge},
%GridBot~\cite{Silberstein:2009:GEB:1654059.1654071} and LGI~\cite{lgi}.

%Nimrod-G~\cite{10.1109/HPC.2000.846563}, DIANE~\cite{diane-thesis} and Work
%Queue~\cite{workqueue-pyhpc2011} are examples of Master-Worker systems that
%utilize a placeholder agent that dispatches and manages tasks. For example,
%Nimrod-G utilizes a Job Wrapper that is responsible for pulling a task and its
%associated data and then manages the execution of this task. While modern
%\pilotjobs often acquire resources opportunistically and then distribute tasks
%to resources they were able to acquire, Nimrod-G utilizes a central,
%cost-based scheduler.\msnote{Is this really a distinction, arent most of the
%systems centrally controlled?}

% Applications can be built on top of BOINC for their own scientific endeavors.
% \mrnote{As just an end reader, it is really unclear how this relates to
% anything. Is this supposed to follow from the previous para?}
% \aznote{Yes -- perhaps combine this + last para?  maybe even next para
% too...}

% A Glide-in is submitted using the Condor-G grid universe \msnote{Is this a
% distinctive feature? Arent the universes just labels anyway?}. On the remote
% resource a set of Condor daemons is started, which then registers the
% available job slots with the central Condor pool. The resources added are
% available only for the user who added the resource to the pool, thus giving
% complete control over the resources for managing jobs without any queue
% waiting time. Glide-in installs and executes necessary Condor daemons and
% configuration on the remote resource, such that the resource reports to and
% joins the local Condor pool. Various systems that built on the \pilot
% capabilities of Condor-G/GlideIn have been developed, e.\,g.\

% Glide-in is limited in that the daemons must be running on a given resource,
% meaning that this process must be approved by resource owners or system
% administrators.

% Venus-C~\cite{venusc-generic-worker} provides a \pilotjob-like capability on
% Microsoft Azure clouds called a Generic Worker. The Generic Worker creates a
% layer of abstraction above the inner workings of the cloud.  The idea behind
% the Generic Worker is to allow scientists to do their science without
% requiring knowledge of backend HPC systems by offering e-Science as a
% service. Venus-C has not been shown to work with grids, because its main
% objective is to motivate scientists to use cloud infrastructures.  While the
% notion of moving to the cloud for data-driven science is an important one,
% many existing cyberinfrastructures still have powerful grid computers that
% can also be leveraged to assist with the data-driven computations.

% In addition to the \pilotjob systems developed around the LHC experiment,
% several other systems emerged. Commented out gwpilot. Not really
% published/downloadable, and no obvious new functionality.
% GWPilot~\cite{gwpilot} is a \pilot system that is developed as a component
% for the GridWay meta-scheduler. GWPilot emphasizes its easy deployment,
% multi-user support and the support of standards, such as DRMAA, OGSA-BES and
% JSDL.
% \jhanote{will need references to these obscure acronyms!}
% \jhanote{Furthermore, must check that the reference to GWPilot is kosher. No
% bacon allowed.}
% \mrnote{It seems a little awkward that we say several others have emerged,
% then we say gwpilot is one. but then we go into talking in next para about
% scientific workflows + pjs. Several makes it sound like we have a bunch more
% to discuss}
% \msnote{In what way are these systems intrinsically pilot based? I think for
% example with Pegasus, it can make use of pilots, but uses it as just yet
% another "job submission backend". In this way, any system that creates "task"
% could be adopted to submit to a pilot-based backend}

% Many higher-level tools and frameworks, such as workflow, visualization or
% data analytics systems, utilize \pilotjob systems to manage their
% computational workload. In general, two approaches exist: (i) the framework
% vertically integrates with a custom \pilotjob implementation (e.\,g.\
% Swift/Coaster) or (ii) it re-uses a general purpose PJ system (e.\,g\
% Pegasus/Condor-G). In case (i), the PJ system is also often exposed as
% stand-alone, multi-purpose \pilotjob systems.

% In contrast to GlideinWMS, Corral-Glide-Ins are run using the credential of
% the user and not a VO credential. Workflow task clustering with
% Pegasus~\cite{Singh:2008:WTC:1341811.1341822}.

% For this purpose, Swift formalizes the way that applications can define
% data-dependencies. Using so called mappers, these dependencies can be easily
% extended to files or groups of files. The runtime environment handles the
% allocation of resources and the spawning of the compute tasks. Both data- and
% execution management capabilities are provided via abstract interfaces.

% Using the Coaster service, one executes a Coaster Master on a head node, and
% the Coaster workers run on compute nodes to execute jobs. In the case of
% cloud computing, Coasters offers a zero-install feature in which it deploys
% itself and installs itself from the head node and onto the virtual machines
% without needing any prior installation on the machine.
% \msnote{Where does the virtual machine suddenly come from?} Coaster relies on
% a master/worker coordination model. communication is implemented using
% GSI-secured TCP sockets. Swift supports various scheduling mechanisms on top
% of Coaster, e.\,g.\ a FIFO and a load-aware scheduler.
% \jhanote{I think the previous paragraph can be highly reduced. The next
% paragraph should be modified to highlight that {\bf specialized} pilots have
% also emerged, namely falkon to support many short running jobs on HPC
% systems. This is an important point to make and speaks to the success of the
% pilot concept}

% Falkon refers to pilots as the so called provisioner, which are created using
% the Globus GRAM service. The provisioner spawns a set of executor processes
% on the allocated resources, which are then responsible for managing the
% execution of task. Tasks are submitted via a so called dispatcher service.
% Falkon also utilizes a master-work coordination model, i.\,e.\ the executors
% periodically query the dispatcher for new tasks.

% Web services are used for communication\msnote{between?}.

% WISDOM~\cite{Ahn:2008:ITR:1444448.1445115,wisdom} is an application-centric
% environment for supporting drug discovery. The architecture utilizes an agent
% run as a Grid job to pull tasks from a central metadata service referred to
% as AMGA.
% \jhanote{this paragraph/pilot can go. I don't see the new functional feature
% or increased complexiy that WISDOM introduces}

% A LRMS incorporating a pilot scheme:
% OAR~\cite{oar} is a batch scheduler system for clusters and other
% computing infrastructures. Besides its more traditional batch scheduler
% features, it also has the functionality of \textit{container jobs}. These type
% of jobs allow the execution of jobs within other jobs, effectively making it a
% sub-scheduling mechanism, and thereby making it a batch scheduler with
% \pilotjob capabilities.

% Maybe add netsolve later:
% NetSolve~\cite{Casanova:1995:NNS:898848}

% -----------------------------------------------------------------------------
% Version 0.3
% -----------------------------------------------------------------------------

The origin and motivations for devising the \pilot abstraction and developing
its many implementations can be traced back to five main notions: task-level
distribution and parallelism, Master-Worker pattern, multi-tenancy,
multi-level scheduling, and resource placeholder. This section offers an
overview of these five notions and an analysis of their relationship with the
\pilot abstraction. A chronological perspective is taken so to contextualize
the inception and evolution of the \pilot abstraction into its implementation
systems.

At the best of the authors' knowledge, the term `\pilotjob' was first
introduced around 2004 in the context of the Large Hadron Collider (LHC)
challenge [ref] and then used in a 2005 LHCb report\cite{lhcb2005}. Despite its
relatively recent explicit naming, the \pilot abstraction addresses a problem
already well-known at the beginning of the twentieth century: task-level
distribution and parallelism on diverse, multi-tenant resources.

Lewis Fry Richardson devised in 1922 a Forecast Factory
(Figure~\ref{fig:figures_forecast-factory}) to solve systems of differential
equations for weather forecasting. This factory required 64,000 human
\textit{computers} supervised by a senior clerk. The clerk would distribute
portions of the differential equations to the `computers' so that they could
forecast the weather of specific regions of the globe. The computers would
perform their calculations and then send the results back to the clerk. The
Forecast Factory was not only an early conceptualization of what is called
today a ``supercomputer'' but also of the coordination pattern for distributed
and parallel computation called ``Master-Worker'' (M-W).

\begin{figure}[t]
  \centering
    \includegraphics[width=.45\textwidth]{figures/forecast-factory.jpg}
  \caption{\textit{Forecast Factory} as envisioned by Lewis Fry Richardson.
    Drawing by Fran{\c c}ois Schuiten.}
  \label{fig:figures_forecast-factory}
\end{figure}

The clerk of the Forecast Factory is the `master' while the human computers are
her `workers'. Requests and responses go back and forth between the master and
all its workers. Each worker has no information about the overall computation
nor about the states of any other worker. The master is the only one possessing
a global view both of the overall problem to compute and of its progress
towards a solution. As such, the M-W is a coordination pattern allowing for the
structured distribution of tasks so to orchestrate their parallel execution.
This directly translates into a better time to completion (TTC) of the overall
computation when compared to a coordination pattern in which each equation is
sequentially solved by a single worker.

Modern silicon-based supercomputers brought at least three key differences when
compared to the carbon-based Forecast Factory devised by Richardson. Most of
modern supercomputers are meant to be used by multiple users, i.e. they support
multi-tenancy. Furthermore, diverse supercomputers were made available to the
scientific community, each with both distinctive and homogeneous properties in
terms of architecture, capacity, capabilities, and interfaces. Finally,
supercomputers supported different types of applications, depending on the
applications' communication and coordination models.

Multi-tenancy has defined the way in which high performance computing resources
are exposed to their users. Job schedulers, often called ``batch queuing
systems'' [ref] and first used in the time of punch cards [ref], leverage the
batch processing concept to promote efficient and fair resource sharing. Job
schedulers implement a usability model where users submit computational tasks
called ``jobs'' to a queue. The execution of these job is delayed waiting for
the required amount of resources to be available. The amount of delay mostly
depends on the size and duration of the submitted job, resource availability,
and fair usage policies.

Supercomputers are often characterized by several types of heterogeneity and
diversity. Users are faced with different job description languages, job
submission commands, and job configuration options. Furthermore,  the number of
queues exposed to the users and their properties like walltime, duration, and
compute-node sharing policies vary from resource to resource. Finally, each
supercomputer may be designed and configured to support only specific types of
application.

The resource provision of multi-tenant and heterogeneous supercomputers is
limited, irregular, and largely unpredictable [ref, ref]. By definition, the
resources accessible and available at any given time can be less than those
demanded by all the active users. Furthermore, the resource usage patterns are
not stable over time and alternating phases of resource availability and
starvation are common [ref]. This landscape led not only to a continuous
optimization of the management of each resource but also to the development of
alternative strategies to expose and serve resources to the users.

Multi-level or meta scheduling is one of the strategies devised to improve
resource access across multiple supercomputers. The idea is to hide the
scheduling point of each supercomputer behind a single (meta) scheduler. The
users or the applications submit their tasks to the single scheduler that
negotiates and orchestrates the distribution of the tasks via the scheduler of
each available supercomputer. While this approach promises an increase in both
scale and usability of applications, it also introduces diverse types of
complexity across resources, middleware, and applications.

Several approaches have been devised to manage the complexities associated with
multi-level scheduling. Some approaches, for example those developed under the
umbrellas of grid computing or cloud computing, targeted the resource layer,
others the application layer as, for example, with workflow frameworks. All
these approaches offered and still offer some degree of success for specific
applications and use cases but a general solution based on well-defined and
robust abstractions has still to be devised and implemented.

One of the persistent issues besetting resource management across multiple
supercomputers is the increase of the implementation complexity imposed on the
application layer. Even with solutions like grid computing aiming at
effectively and, to some extent, transparently integrating diverse resources,
most of the requirements involving the coordination of task execution still
lays with the application layer. This translates into single-point solutions,
extensive redesign and redevelopment of existing applications when they need to
be adapted to new use cases or new resources, and lack of portability and
interoperability.

Consider for example a simple distributed application implementing a M-W
pattern. With a single supercomputer, the application requires the capability
of concurrently submitting tasks to the queue of the supercomputer scheduler,
and retrieve and aggregate their outputs. When multiple supercomputers are
available, the application requires directly managing submissions to several
queues or the capability to leverage a third-party meta-scheduler and its
specific execution model. In both scenarios, the application requires a large
amount of development and capabilities that are not specific to the given
scientific problem but pertain instead to the coordination and management of
its computation.

The notion of resource placeholder was devised as a pragmatic and relatively
cheap to implement attempt to reduce or at least better manage the complexity
of executing distributed applications. A resource placeholder decouples the
acquisition of remote compute resource from their use to execute the tasks of a
distributed application. Resources are acquired by scheduling a job onto the
remote supercomputer. Once executed, the job runs an agent capable of
retrieving and executing application tasks.

Resource placeholders bring together multi-level scheduling and, when useful,
the M-W pattern to enable parallel execution of the tasks of distributed
applications. Multi-level scheduling is achieved by scheduling the agent and
then by enabling direct scheduling of application tasks to that agent. The M-W
pattern is often an effective choice to manage the coordination of tasks
execution on the available agent(s). Multi-level scheduling can be extended to
multiple resources by instantiating resource placeholders on diverse
supercomputing and then using a meta-scheduler to schedule tasks across all the
placeholders.

It should be noted that resource placeholders also mitigate the side-effects
introduced by multi-tenancy. Multi-tenancy affects only the scheduling of the
resource placeholder as it needs to be executed by the batch system of the
remote supercomputer. Once the placeholder is executed, the user -- or the
master process of the distributed application -- may hold total control of the
resource placeholder. In this way, tasks are directly scheduled on the
placeholder without competing with other users for the supercomputer scheduler.

As seen in Ref.~\cite{pstar-2012}, the \pilot abstraction has a rich set of
properties and its implementations offer a vast array of capabilities including
multiple scheduling algorithms, data and compute placeholders, and late or
early binding. Nonetheless, the capability of acquiring remote resources and
directly utilizing them, independently from the supercomputer resource
management, is a necessary property of the \pilot abstraction. As such,
resource placeholders and their
scheduling~\cite{Pinchak02practicalheterogeneous} should be seen as early
\pilot system implementations.

% The progressive definition and implementation of the \pilot abstraction can
% be seen as the process of evolving both the understanding and implementation
% complexity of the notion of resource placeholder.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/timeline}
    \caption{Introduction of systems over time. When available, the date of
    first mention in a publication or otherwise the release date of software
    implementation is used.}
    \label{fig:timeline}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[width=.45\textwidth]{figures/pilotjob-clustering.pdf}
  \caption{Pilot-Job Clustering}
  \label{fig:pilotjob_clustering}
\end{figure}

AppLeS~\cite{Berman:1996:apples} is a framework for application-level
scheduling and offers an example of an early implementation of resource
placeholder. AppLeS provides an agent that can be embedded into an application
thus enabling the application to acquire resources and to schedule tasks onto
these. Besides M-W, AppLeS also provides application templates, e.\,g.\ for
parameter sweep and moldable parallel
applications~\cite{Berman:2003:ACG:766629.766632}.

AppLeS offered user-level control of scheduling but did not isolate the
application layer from the management and coordination of task execution. Any
change in the coordination mechanisms directly translated into a change of the
application code. The next evolutionary step was to create a dedicated
abstraction layer between those of the application and of the various batch
queuing systems available at different remote systems.

Around the same time as AppLeS was introduced, volunteer computing projects
started using the M-W coordination pattern to achieve high-throughput
calculations for a wide range of scientific problems. The workers of these
systems could be downloaded and installed on the user laptop and workstation.
With an installation base potentially distributed across the globe, workers
pulled and executed computation tasks when CPU cycles were available.

The volunteer workers were essentially heterogeneous and dynamic as opposed to
the homogeneous and static AppLeS workers. The idea of farming out tasks in a
dynamic distributed environment including personal computers promised to lower
the complexity of distributed applications design and implementation. Each
volunteer worker can be seen as an opportunistic resource placeholder and, as
such, an implementation of the core functionality of the \pilot abstraction.

The first public volunteer computing projects were The Great Internet Mersenne
Prime Search effort\cite{woltman:2004:gimps}, shortly followed by
distributed.net~\cite{Lawton:2000:distributednet} in 1997 to compete in the
RC5-56 secret-key challenge, and the SETI@Home project, which set out to
analyze radio telescope data. The generic BOINC distributed master-worker
framework grew our of SETI@Home, becoming the {\it de facto} standard framework
for voluntary computing~\cite{Anderson:2004:BSP:1032646.1033223}.

It should be noted that process of resource acquisition is different in AppLes
and voluntary computing. The former has complete knowledge of the available
resources while the latter has none. As a consequence, AppLes can request and
orchestrate a set of resource, allocates tasks in advance to specific workers
(i.e. resources placeholders), and implement load balancing among resources. In
voluntary computing tasks are polled by the clients when they become active
and, as such, resource availability is unknown in advance. This potential
drawback is mitigated by the redundancy offered by the large scale that
voluntary computing can reach thanks to its simpler model of worker
distribution and installation.

The opportunistic use of geographically distributed resources championed by
voluntary computing offers several advantages. The resource landscape available
for scientific research is fragmented across multiple institutions, managed
with different policies and protocols, and heterogeneous both in quantity and
quality. Once aggregated, the sum of otherwise limited resources can support
very large distributed computations and an great amount of multi-tenancy. Note
that given the required capabilities, this model of resource provisioning can
still support the execution of parallel applications on the few resources that
offer low-latency network interconnect.

Condor is a high-throughput distributed batch computing system that leverages
diverse and possibly geographically distributed resources. Originally, Condor
was created for systems within one administrative domain but
Flocking~\cite{Epema:1996:flocking} made possible to group multiple Condor
resource pools in an aggregative manner. However, resource management could not
be done on application level by the user: Flocking required system level
software configurations that had to be made by the administrator of each
individual resource of each Condor resource pool.

This limitation was overcame by integrating a resource placeholder mechanism
within the Condor system. Condor-G/GlideIn~\cite{condor-g} allow users to add
remote grid resources to Condor resource pools. In this way, users can
uniformly execute jobs on resource pools composed by diverse resources. Thanks
to its use of resource placeholders, GladeIn has been one of the systems
pioneering the \pilot abstraction implementation, enabling \pilot capabilities
also for third parties systems like Bosco~\cite{bosco}.

The success of Condor-G/GlideIn shows the relevance of the pilot abstraction to
enable scientific computation at scale and on heterogeneous resources. The
implementation of GlideIn also highlighted at least two limitations:
user/system layer isolation, and application development model. While GlideIn
allows for the user to manage resource placeholders directly, daemons must
still be running on the remote resources. This means that GlideIn cannot be
deployed without involving the resource owners and system administrators.
Implemented as a service, GlideIn support integration with distributed
application frameworks but does not programmatically support the development of
distributed applications by means of dedicated APIs and libraries.

The BigJob \pilot system [ref] was designed to address these limitations, to
broaden the type of applications supported by the \pilot-based execution model,
and to extend the \pilot abstraction beyond the boundaries of compute tasks.
BigJob offers an application-level programmability to provide the end-user with
more flexibility and control over the design of distributed application and the
isolation of the management of their execution. BigJob is flexible and
extensible and uses the Simple API for Grid Applications (SAGA)
interoperability library [ref] to work on a variety of infrastructures.
Furthermore, BigJob supports both MPI and non-MPI jobs without adding
additional configuration requirements to the end-user. Finally, BigJob has also
been extended to work with data and, analogous to compute \pilots, to abstract
away direct user communication between different storage systems.

BigJob, recently redesigned as a pre-production system named RADICAL Pilot
[ref], represents one of the latest evolutionary stages of the \pilot
abstraction. From an initial phase in which \pilot were implemented as ad hoc
place holder machinery for a specific application, to the integration of \pilot
systems within the middleware of remote resources, PilotJob implements the
\pilot abstraction as an interoperable compute and data management system that
can be programmatically integrated into end-user applications.

Another ongoing evolutionary trend of the \pilot abstraction is to be
implemented into \pilot-based workload managers, thus moving away from
providing simple \pilot capabilities in application space. These higher-level
systems which are often centrally hosted, move critical functionality from the
client to the server (i.e. a service model). These systems usually deploy
\pilot factories that automatically start new \pilots on demand and integrate
security mechanisms to support multiple users simultaneously.

Several of these have been developed in the context of the LHC experiment at
CERN, which is associated with a major increase in the uptake and availability
of \pilots, e.\,g.\ GlideInWMS, DIRAC~\cite{1742-6596-219-6-062049},
PanDa~\cite{1742-6596-331-7-072069}, AliEn~\cite{1742-6596-119-6-062012} and
Co-Pilot~\cite{copilot-tr}. Each of these \pilots serves a particular user
community and experiment. Interestingly, we observe that these \pilots are
functionally very similar, work on almost the same underlying infrastructure,
and serve applications with very similar (if not identical) characteristics.

GlideinWMS~\cite{1742-6596-119-6-062044} is a higher-level workload management
system that is based on the \pilot capabilities of Condor GlideIn. The system
can, based on the current and expected number of jobs in the pool,
automatically increase or decrease the number of active Glide-ins (\pilots)
available to the pool. GlideinWMS is a multi-user \pilotjob system commonly
deployed as a hosted service. GlideinWMS attempts to hide the \pilot
capabilities rather than exposing them to the user. GlideinWMS is currently
deployed in production on the Open Science Grid (OSG)~\cite{url_osg} and is the
recommended mode for new users wanting to access OSG resources.

\panda (Production and Distributed Analysis)~\cite{1742-6596-331-7-072069} is
the workload management system of the ATLAS experiment, used to run managed
production and user analysis jobs on the grid. The ATLAS Computing Facility
operates the pilot submission systems. This is done using the \panda
`AutoPilot' scheduler component which submits pilot jobs via Condor-G. \panda
also provides the ability to manage data associated the jobs managed by the
\panda workload manager.

In addition to processing, AliEn~\cite{1742-6596-119-6-062012} also provides
the ability to tightly integrate storage and compute resources and is also able
to manage file replicas. While all data can be accessed from anywhere, the
scheduler is aware of data localities and attempts to schedule compute close to
the data. AliEn deploys a pull-based model~\cite{Saiz:2003:alien} assuming that
the resource pool is dynamic and that a pull model doesn't require the broker
to keep detailed track of all resources, which leads to a more simple
implementation.

DIRAC~\cite{1742-6596-219-6-062049} is another comprehensive workload
management system built on top of \pilots. It supports the management of data,
which can be placed in different kinds of storage elements (e.\,g.\ based on
SRM).

Another interesting \pilot that is used in the LHC context is
Co-Pilot~\cite{copilot-tr}. Co-Pilot serves as an integration point among
different grid \pilotjob systems (such as AliEn and \panda), clouds and
volunteer computing resources. Co-Pilot provides components for building a
framework for seamless and transparent integration of these resources into
existing Grid and batch computing infrastructures exploited by the High Energy
Physics community.

The \pilot abstraction has also been integrated into scientific workflow
systems. \pilot systems have proven an effective tool for managing the
workloads executed in the various stages of a workflow. For the Pegasus
project, the Corral system~\cite{Rynge:2011:EUG:2116259.2116599} was developed
to support the requirements of the Pegasus workflow system in particular to
optimize the placements of \pilots with respect to their workload. It did this
by serving as a front-end to Condor GlideIn. In contrast to GlideinWMS, Corral
provides more explicit control over the placement and start of \pilots to the
end-user. Corral was later extended to also serve as a possible front end to
GlideinWMS.

Swift~\cite{Wilde2011} is a scripting language designed for expressing abstract
workflows and computations. The language provides also capabilities for
executing external application as well as the implicit management of data flows
between application tasks. Swift uses a \pilot implementation called
``Coaster''~\cite{coasters}, developed to address workload management
requirements by supporting various types of infrastructure, including clouds
and grids.

% Swift has also been used in conjunction with Falkon~\cite{1362680}. Falkon
% was engineered for executing many small tasks on HPC systems and shows high
% performance compared to the native queuing systems. Falkon is a paradigmatic
% example of how the \pilot abstraction can be implemented to support very
% specific workload.

\mtnote{Needs a better closing}

Based on the descriptions in this section we can identify some distinctions in
terms of design, usage and operation of pilot(-based) systems.
Figure~\ref{fig:pilotjob_clustering} is a graphical representation of this
rough clustering.

The evolution of \pilotjobs attests to their usefulness across a wide range of
deployment environments and application scenarios, but the divergence in
specific functionality and inconsistent terminology calls for a standard
vocabulary to assist in understanding the varied approaches and their
commonalities and differences.


%------------------------------------------------------------------------------
% SECTION 3
%------------------------------------------------------------------------------

\section{Understanding the Landscape: Developing a Vocabulary}
\label{sec:understanding}

The overview presented in \S\ref{sec:history} shows a degree of
heterogeneity both in the functionalities and the vocabulary adopted by
different \pilot systems. Implementation details sometimes hide the
functional commonalities and differences among \pilot systems while
features and capabilities tend to be named inconsistently, often with
the same terms referring to multiple concepts or the same concept named
in different ways.

This section offers an analysis of the logical components and functionalities
shared by every \pilot system. The goal is to offer a paradigmatic description
of a \pilot system and a well-defined vocabulary to reason about such a
description and, eventually, about its multiple implementations.

%------------------------------------------------------------------------------
\subsection{Logical Components and Functionalities}
\label{sec:compsandfuncs}

All the \pilot systems introduced in~\S\ref{sec:history} are engineered to
allow for the execution of multiple types of workloads on Distributed Computing
Infrastructures (DCIs) such as Grids, Clouds, or HPC facilities. This is
achieved differently, depending on use cases, design and implementation
choices, but also on the constraints imposed by the middleware and policies of
the targeted DCIs. The common denominators among \pilot systems are defined
along multiple dimensions: purpose, logical components, and functionalities.

The purpose shared by every \pilot system is to improve the (performance of)
workload execution when compared to executing the same workload directly on one
or more DCI. Performance in \pilot systems is usually associated to throughput
and execution time to completion (TTC), but other metrics could be also
considered as, for example, energy efficiency, data transfer, scale of the
workload executed or a mix of them. In order to optimize the chosen set of
metrics, each \pilot system exhibits characteristics that are both common or
specific to one or more implementations. Discerning these characteristics
requires isolating and defining the minimal set of logical components that has
to characterize every \pilot system.

At some level, all \pilot systems leverage three separate but coordinated
logical components: a \textbf{Pilot Manager}, a \textbf{Workload Manager}, and
a \textbf{Task Executor} (see Table~\ref{table:terminology}). The Pilot Manager
handles the description, instantiation, and use of one or more resource
placeholders (i.e. `pilots') on single or multiple DCIs. The Workload Manager
handles the scheduling of one or more given workload on the available resource
placeholders. Finally, the Task Executor takes care of executing the tasks of
each workload by means of the resources held by the placeholders.

The implementation details of these three logical components significantly vary
across \pilot systems (see~\S\ref{sec:analysis}). One or more components may be
responsible for specific functionalities, both on application as well as
infrastructure level, two or more components may be implemented in a single
module, or additional components may be integrated into the managers and
executor. Nevertheless, the Pilot and Workload Managers and the Task Executor
can always be distinguished across different \pilot systems. For example,
looking at the systems mentioned in~\S\ref{sec:history}, \ldots
\mtnote{I will wait for S2 and S4 to be final before picking the right
examples/vocabulary.}

Each \pilot system support a minimal set of functionalities that allow for the
execution of workloads: \textbf{Pilot Provisioning}, \textbf{Task Dispatching},
and \textbf{Task Execution} (see Table~\ref{table:terminology}). \pilot systems
need to schedule resource placeholders on the targeted resources, schedule
tasks on the available placeholders, and then use these placeholders to execute
the tasks of the given workload.

More functionalities might be needed to implement a production-grade \pilot
system: authentication, authorization, accounting, data management,
fault-tolerance, or load-balancing. While these functionalities may be critical
implementation details, they depend on the specific characteristics of the
given use cases, workloads, or targeted resources. As such, these
functionalities should not be considered a necessary characteristic of every
\pilot system.

Among the core functionalities that characterize every \pilot system, Pilot
Provisioning is essential because it allows for the creation of resource
placeholders. As seen in \S\ref{sec:history}, this type of placeholder enables
tasks to utilize resources without directly depending on the capabilities
exposed by the targeted DCI. Resource placeholders are scheduled to the DCI
resources by means of the DCI capabilities but, once scheduled and then
executed, these placeholders make their resources directly available for the
execution of the tasks of a workload. Resource placeholders belong to the
user(s) that created them and they can be tailored to satisfy the specific use
cases. \mtnote{Currently, this is a bit of repetition of what we wrote already
in Section 2 but seeing the importance of the concept we might want to keep
it.}

% Furthermore, resource placeholders are logical partitions of resources that
% do not need to leverage trade-offs among competing user requirements as
% needed instead with large pools of resources adopting multi-tenancy.

The provision of resource placeholders depends on the capabilities exposed by
the targeted DCI and on the implementation of each \pilot system. Typically,
for DCIs adopting queues, batch systems, and schedulers, provisioning a
placeholder involves it being submitted as a job. A `job' on this kind of DCIs
is a type of logical container that includes configuration and execution
parameters alongside information on the executable that will be executed on the
DCIs compute nodes. Conversely, for infrastructures that do not adopt a
job-based middleware, a resource placeholder would be executed by means of
other types of logical container as, for example, a Virtual Machine (VM) or a
Docker Engine [cit, cit].

Once resource placeholders are bound to a DCI, tasks need to be dispatched to
those placeholders for execution. Task dispatching does not depend on the
functionalities of the DCI middleware so it can be implemented as part of the
\pilot system. In this way, the control over the execution of a workload is
shifted from the DCI to the \pilot system. This shift is a defining
characteristic of the \pilot abstraction \mtnote{I do not think it is too
disruptive to speak about abstraction but, in case, we can just use \pilot
system instead of \pilot abstraction} as it decouples the execution of a
workload from the need to submit its tasks via the DCI middleware. The tasks of
a workload will not wait on the DCI queues before being executed, and complex
execution patterns involving task and data interdependence can be implemented
outside the boundaries of the DCI capabilities. Ultimately, this is why \pilot
systems allow for the parametrization of workload execution and the
optimization, for example, of execution throughput.

Communication and coordination are two distinguishing characteristics of
distributed applications and \pilot systems make no exception. The Workload
Manager, Pilot Manager, and the Task Executor need to communicate to coordinate
the execution of the given workload on the instantiated resource placeholders.
Nonetheless, \pilot systems are not defined by any specific communication
pattern and coordination strategy. The logical components of a \pilot system
may communicate with every suitable pattern (e.g. one-to-one, many-to-one,
one-to-many (cit) with a push or pull model) and coordinate adopting any
suitable strategy (e.g. time synchronization, static or dynamic coordinator
election, local or global information sharing, or \MW). The same applies to
network architectures and protocols: different network architectures and
protocols may be leveraged to achieve effective communication and coordination.
\jhanote{maybe this is a good place to introduce pull vs push
models}\mtnote{Done.}

As seen in~\S\ref{ssec:history}, \MW is a very common coordination pattern
among \pilot systems. When the Master\mtnote{Should we use the capitalization?}
is identified with the Workload Manager, and the Worker with the Task Executor,
the functionalities related to task description, scheduling, and monitoring
will generally be implemented within the Workload Manager, while the
functionalities needed to execute each task will be implemented into the Task
Executor. Alternative coordination strategies, for example the one where a Task
Executor directly coordinates the task scheduling, might require a functionally
simpler Workload Manager but a comparatively feature-rich Task Executor. The
former would require capabilities for submitting tasks, while the latter would
require to coordinate with its neighbor executors leveraging, for example, a
dedicated overlay network. Both these systems, adopting different coordination
strategies, should be considered \pilot systems.\mtnote{I am happy with calling
\MW a pattern and then generalizing using the term strategies when speaking of
coordination in general. Do you agree?}

Data management may have an important role within \pilot systems. For example,
functionalities can be provided to support the local or remote data staging
required to execute the tasks of a workload, or data might be managed according
to the specific capabilities offered by the targeted DCI. \pilot systems can be
devised in which tasks do not require any data management because they (i) do
not necessitate input files, (ii) do not produce output files, (iii) data is
already locally available or (iv) data management is outsourced to third-party
systems. Being able to read and write files to a local filesystem should then
be considered the minimal capability related to data required by a \pilot
system. More advanced and specific data capabilities like, for example, data
replication, (concurrent) data transfers, data abstractions other than files
and folders, or data placeholders should be considered special-purpose
capabilities, not characteristic of every \pilot system.

In the following Subsection, a minimal set of terms related to the
logical components and capabilities just described is defined.

%------------------------------------------------------------------------------
\subsection{Terms and Definitions}
\label{subsec:3.2}

\begin{table*}
 \centering
 \begin{tabular}{|p{4cm}|p{3.2cm}|p{3.2cm}|}
  \hline
    \textbf{Term} & \textbf{Functionality} & \textbf{Logical Component} \\
  \hline
  \hline
    \textbf{Workload} & Task Dispatching & Workload Manager \\
  \hline
    \textbf{Task} & Task Dispatching \newline
    Task Execution & Workload Manager \newline
    Task Executor \\
  \hline
    \textbf{Resource} & Pilot Provisioning & Pilot Manager \\
  \hline
    \textbf{DCI} & Pilot Provisioning & Pilot Manager \\
  \hline
    \textbf{Job} & Pilot Provisioning & Pilot Manager \\
  \hline
    \textbf{Pilot} & Pilot Provisioning \newline
    Task Execution & Pilot Manager \newline
    Task Executor \\
  \hline
    \textbf{Multi-level scheduling} &
    Pilot Provisioning \newline Task Dispatching \newline (Task Execution?) &
    Pilot Manager \newline Workload Manager \newline (Task Executor?) \\
  \hline
    \textbf{Early binding} & Task Dispatching \newline
    Pilot Provisioning & Workload Manager \newline
    Pilot Manager\\
  \hline
    \textbf{Late binding} & Task Dispatching \newline
    Pilot Provisioning  & Workload Manager \newline
    Pilot Manager\\
  \hline
 \end{tabular}
 \caption{\textbf{Mapping of the core terminology of \pilot systems into
  the functionalities and logical components described in
  \S\ref{subsec:compsandfuncs}.}\msnote{What are the considerations for the
  mapping?}\mtnote{I am not sure I properly understand the comment but should
  we say common sense? We are defining the vocabulary and the logical model so
  I would assume we are fairly unbounded in our mapping choices.}}
 \label{table:terminology}
\end{table*}

The terms `pilot' and `job' are arguably among the most relevant when referring
to \pilot systems. It is the case that \pilot systems are commonly referred to
as `\pilotjob systems' [ref, ref], a clear indication of the primary role
played by the concepts of `pilot' and `job' in this type of system. The
definition of both concepts is context-dependent and several other terms need
to be clarified in order to offer a coherent terminology. Both `job' and
`pilot' need to be understood in the context of DCIs, the infrastructures used
by \pilot systems. DCIs offer compute, storage, and network resources and
\pilot allow for the users to utilize those resources to execute the tasks of
one or more workload.

\begin{description}

\item[Task.] A container for operations to be performed on a computing
platform, alongside a description of the properties of those operations, and
indications on how they should be executed. Implementations of a task may
include wrappers, scripts, or applications.

\item[Workload.] A set of tasks, possibly related by a set of arbitrary complex
relations.

\item[Resource.] Finite, typed, and physical quantity utilized when executing
the tasks of a workload. Compute cores, data storage space, or network
bandwidth between a source and a destination are all examples of resources
commonly utilized when executing workloads.

\item[Infrastructure or DCI.] Structured set of resources, possibly
geographically and institutionally separated from the users that utilize those
resources to execute the tasks of a workload [ref]. Infrastructures can be
logically partitioned, with a direct or indirect mapping onto individual pools
of hardware (i.e. clusters, systems, and supercomputers). In the context of
\pilot systems, an infrastructure is usually to be assumed to be a distributed
computed infrastructure, i.e., a DCI.

\end{description}

As seen in~\S\ref{sec:history}, most of the DCIs leveraged by \pilot systems
utilize `queues', `batch systems' and `schedulers'. In such DCIs, jobs are
scheduled and then executed by a batch system.

\begin{description}

\item[Job.] Functionally defined as a `task' from the perspective of the DCI,
but indicating the type of container required to acquire resources on a
specific infrastructure.

\end{description}

When considering \pilot systems, jobs and tasks are functionally analogous but
qualitatively different. Functionally, both jobs and tasks are containers --
i.e. metadata wrappers around one or more executable often called `kernel',
`application', or `script'. Qualitatively, the term `task' is used when
reasoning about workloads while `job' is used in relation to a specific type of
infrastructure where such a container can be executed. Accordingly, tasks are
considered as the functional units of a workload, while jobs as a way to
execute executables on a certain infrastructure. It should be noted that, given
their functional equivalence, the two terms can be adopted interchangeably when
considered outside the context of \pilot systems. Indeed, workloads are encoded
into jobs when they have to be directly executed on infrastructures that
support or require that type of container.

As described in~\S\ref{sec:compsandfuncs}, a resource placeholder needs to be
submitted to the target DCI wrapped in the type of container supported by that
specific DCI. For example, for a DCI exposing a HPC or Grid middleware [cit,
cit], a resource placeholder needs to be wrapped within a `job'. For other type
of DCIs, the same resource placeholder will need to be wrapped within a
different type of container as, for example, a VM or a Docker Engine. For this
reason, the capabilities exposed by the job submission system of the target DCI
determine the submission process of resource placeholders and how or for how
long they can be used. For example, when wrapped within a `job', placeholders
are provisioned by submitting a job to the DCI queuing system, become available
only once the job is scheduled on the resources out of the queue, and is
available only for the duration of the job walltime.

A `pilot' is a resource placeholder. As a resource placeholder, a pilot holds
portion of a DCI's resources for a user or a group of users, depending on
implementation details. A \pilot system is a software capable of creating
pilots so to gain exclusive control over a set of resources on one or more DCIs
and then to execute the tasks of one or more workload on those pilots.

\begin{description}

\item[Pilot.] The sum of a container (e.g., a job) and a set of executables.
Together, the container and executables function as a resource placeholder on a
given infrastructure and are capable of executing tasks of a workload.

\end{description}

It should be noted that the term `pilot' as defined here is named differently
across \pilot systems. Depending upon context, in addition to the term
`placeholder', pilot is also named `agent' and, in some cases, `\pilotjob'
[cit]. All these terms are, in practice, used as synonyms without properly
distinguishing between the type of container and the type of executable that
compose a \pilot. This is a clear indication of how necessary the minimal and
consistent vocabulary offered here is when reasoning analytically about
multiple implementations of a \pilotjob system.

The term `pilotjob' is often used to identify a \pilot system. This is an
unfortunate choice as the term `job' identifies just the way in which a pilot
is provisioned on a DCI exposing specific capabilities, not a general
properties of all the \pilot systems. The use of the term `\pilotjob system'
should therefore be regarded as a historical artifact, viz., the targeting of a
specific class of DCIs in which the term `job' was, and still is, meaningful.
With the development of new types of DCI middleware as, for example, cloud
infrastructures, the term `job' has become too restrictive, a situation that
can lead to terminological and conceptual confusion.

We have now defined resources, DCIs and \pilots. We have established that a
\pilot is a placeholder for a set of resources. When combined, the resources of
multiple \pilots form a resource overlay. The \pilots of a resource overlay can
potentially be distributed over multiple resources and/or DCIs.

% The details of this aggregation or federation is outside the scope of this
% paper.

\begin{description}
\item[Resource Overlay.] The aggregated set of resources of multiple \pilots.
\end{description}

Three more terms associated with \pilot systems (cit, cit, cit) need to be
explicitly defined: `Multi-level scheduling', `early binding', and `late
binding'.

Pilot systems are said to implement multi-level scheduling because they require
the scheduling of two types of entities: pilots and tasks. A portion of the
resources of a DCI is scheduled to one or more pilots in the form of containers
supported by that DCI, and the tasks of a workload are dispatched to those
pilots. This is a fundamental feature of \pilot systems because (i) a faster
and more flexible execution of workloads is achieved by avoiding the overhead
imposed by a centralized job management system shared among multiple users; and
(ii) the tasks of a workload can be bound to a set of pilots before of after it
becomes available on a remote resource. \msnote{There is another (sub)level
arguably, where the 'agent' makes scheduling decisions about task placement
within the pilot's resource pool. Although this is an implementation detail, it
might be worth mentioning?}\mtnote{I agree. I will write a small paragraph
connecting it also to resource overlays or I will add a sentence at the end of
the next paragraph. Would you agree?}

The simplification obtained by bypassing the job submission system of
the DCI is one of the main reasons for the success of the \pilot
systems. As mentioned in~\S\ref{sec:compsandfuncs}, the tasks of a workload can
be executed on a pilot without waiting in the queue system of the
infrastructure increasing, as a result, the throughput of the workload
execution. Moreover, pilots can be reused to execute multiple workloads
until the pilots' walltime expire. It should be noted that how tasks are
actually assigned to pilots is a matter of implementation. For example,
a dedicated scheduler could be adopted, or tasks might be directly
assigned to a pilot by the user.

The binding of tasks to pilots depends on the state of the pilot. A
pilot is inactive until it is executed on a DCI, active after that.
Early binding indicates the binding of a task to an inactive pilot; late
binding the binding of a task to an active pilot. Early binding is
useful to increase the flexibility with which pilots are deployed. By
knowing in advance the properties of the tasks that are bound to a
pilot, specific deployment decisions can be made for that pilot.
\msnote{This can also be done without pilots. We probably need a better (if
any) argument for early binding on pilots.}\mtnote{We may want to discuss this
comment briefly by voice at our next meeting.} Late binding is critical in
assuring the aforementioned high throughput by allowing task execution without
additional queuing time or container instantiation time.

\begin{description}

\item[Multi-level scheduling.] Scheduling pilots onto resources and tasks onto
active or inactive pilots.

\item[Early binding.] Binding one or more tasks to an inactive pilot.

\item[Late binding.] Binding one or more tasks to an active pilot.

\end{description}

Depending on the specific capabilities implemented in the Workload
Manager component, some \pilot systems allow for pilots to be
specified by taking into consideration the properties of (the tasks of) a
workload. This type of specification should not be confused with early
binding as the latter requires for a pilot to have been already
bound to a resource.

Figure~\ref{fig:core_vocabulary} offers a diagrammatic representation of
how the minimal and consistent vocabulary proposed in this Subsection
should be used to describe the logical components and core capabilities
of a \pilot system illustrated in~\S\ref{sec:compsandfuncs}.

\begin{figure}[t]
    \centering
        \includegraphics[width=.48\textwidth]{figures/core_vocabulary.png}
    \caption{Diagrammatic representation of the logical components,
             capabilities, and core vocabulary of a \pilot system.
             The terms of the core vocabulary are highlighted in red.}
    \label{fig:core_vocabulary}
\end{figure}

\mtnote{For S3, the TODO list is: 1. Argue explicitly that the offered `model'
  is sufficient and necessary in order to discriminate between \pilot and
  not-\pilot systems; [\ldots], and possibly another one relative to overlay
  enacting as distinguished from task dispatching; 3.  Clean up Table 1
removing 'Infrastructure'.}

%------------------------------------------------------------------------------
% SECTION 4
%------------------------------------------------------------------------------
\section{Pilot Systems: Analysis and Implementations}
\label{sec:analysis}

Section \S\ref{sec:understanding} offered two main contributions: a minimal description of
the logical components and the functionalities of \pilot systems \footnote{
From here on we will be internally consistent and use \pilot system instead of
\pilotjob system.}, and a well-defined core terminology to support reasoning
about such systems.
The former sets the necessary and sufficient requirements for a distributed
system to be a \pilot system, while the latter enables consistency when
referring to different \pilot systems. Both these contributions are leveraged
in this Section in order to review critically a set of relevant \pilot system
implementations.

The goal of this Section is twofold.  Initially, the set of
functionalities presented in \S\ref{sec:3} are used as the basis
to infer a set of core implementation properties. A set of auxiliary
properties is also defined when useful for a critical comparison among
different \pilot system implementations. Subsequently, several \pilot system
implementations are analyzed and then clustered around the properties
previously defined. In this way, insight is offered about how to
choose a \pilot system based on functional requirements, how
\pilot systems are designed and engineered, and the theoretical
properties that underly such systems.

\subsection{Core and Auxiliary Properties}
\label{sec:properties}

This Section discusses the properties of diverse implementations of a \pilot
system. Two sets of properties are introduced: Core and auxiliary.
\textit{Core} properties are common to all \pilot system implementations while
the auxiliaries characterize specific \pilot systems. Therefore, auxiliary
properties are not shared among all \pilot system implementations.

As shown in Table \ref{table:core_properties}, the set of Core Properties is
derived consistently with the set of functionalities presented in
\S\ref{sec:3} - Pilot Provisioning, Task Dispatching, and Task Execution.
\msnote{I dont see how it is derived or consistent ;-)}
As such, Core Properties are both necessary and sufficient for an
implementation of a distributed system to be classified as a \pilot system.
On the contrary, auxiliary properties are not defining of a \pilot system but
they are implementation details that further characterize a \pilot system.  The
set of auxillary properties we discuss is not closed, i.e., the complete set of
auxillary properties includes, but is not limited to the set of auxillary
properties we discuss.
\msnote{todo: Fix/merge the two non-compatible definitions of aux props}

\begin{table*}
\centering
\begin{tabular}{c|c|p{3cm}|p{5cm}|}
\cline{2-4}
& \textbf{Property} & \textbf{Component} & \textbf{Description} \\
\hline
% Core
\multirow{6}{*}{\textit{Core}}
  & Pilot Resource Capabilities & Pilot Manager & The resources that the \pilot
  represents. \\
  \cline{2-4}
  & Resource Interaction & Pilot Manager & The software and protocols of the
  interaction with the DCI. \\
  \cline{2-4}
  & Overlay Management & Workload Manager & The construction of the Overlay. \\
  \cline{2-4}
  & Workload Semantics & Workload Manager & The specification of the semantics
  (between tasks) captured in the workload description. \\
  \cline{2-4}
  & Task Binding Characteristics & Workload Manager & The techniques and
  policies of binding tasks to \pilots. \\
  \cline{2-4}
  & Task Execution Modes & Task Executor & The types of tasks and the
  mechanisms to execute tasks. \\
\hline
% Auxiliary
\multirow{10}{*}{\textit{Auxiliary}}
  & Architecture &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    Frameworks and archicture that the components and their whole are build
    with.\\
  \cline{2-4}
  & Coordination and Communicatuon &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    The interaction between the components of the system.\\
  \cline{2-4}
  & Interface &
    Pilot Manager\newline Workload Manager &
    Interface that the user can use to interact with the system.\\
  \cline{2-4}
  & Interoperability &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    Interopability between \pilots on multiple DCIs.\\
  \cline{2-4}
  & Multitenancy &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    The usage of components by multiple (simultaneous) users.\\
  \cline{2-4}
  & Robustness &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    The measures in place to increase the robustness of the components and the
    whole.\\
  \cline{2-4}
  & Security &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    AAA considerations for the components and the whole.\\
  \cline{2-4}
  & Files and Data &
    Pilot Manager\newline Workload Manager &
    The mechanisms that the system offers to explicitly deal with files and
    data.\\
  \cline{2-4}
  & Performance and Scalability &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    A description of scale and limitations and measures to reach that.\\
  \cline{2-4}
  & Development Model &
    Pilot Manager\newline Workload Manager\newline Task Executor &
    The development- and support model for the software.\\
\hline
\end{tabular}
\caption{\textbf{Mapping of the Properties of \pilot system implementations
onto the components described in \S\ref{sec:compsandfuncs}.}}
\label{table:property_component_mapping}
\end{table*}

Implementations of Pilot Provisioning are analyzed by focusing on two
specific properties: the requirements and dependencies between the
\pilot system and the underlying distributed resources; and the type
of resources the pilot system exposes in terms of computing, data and
networking. For example, in order to schedule a pilot onto a specific
resource, the \pilot system will need to know what type of container
to use (e.g. job, virtual machine), what type of scheduler the
resource exposes, but also what kind of functionalities will be
available on the nodes of the resource in terms of compute, data and
networking.

Two properties are also used to analyze the implementations of Task
Dispatching: The semantics of workloads, and how the tasks of a given workload
can be bound to single or multiple pilots. Semantically, a workload description
contains all the information necessary for it to be dispatched to the
appropriate resource. For example, information related to both space and time
should be available when deciding how many resources of a specific type should
be used to execute the given workload but also for how long such resources
should be available. Executing a workload requires for its tasks to be bound to
the resources. Both the temporal and spatial dimensions of the binding
operations are relevant for the implementation of Task Dispatching. Depending
on the concurrency of a given workload, tasks could be dispatched to one or
more pilots for an efficient execution. Furthermore, tasks could be bound to
pilots before or after its instantiation, depending on resource availability
and scheduling decisions.

Finally, implementations of Task Execution are analyzed by reviewing different
strategies for task scheduling and by describing how \pilot systems support
task execution. \pilot implementations may offer multiple scheduling
strategies depending on varying factors related to the nature of the workload,
the state of the resources, or the capabilities exposed by the underlying
middleware. Furthermore, \pilot systems often are responsible for setting up
the execution environment for the tasks of the given workload. While each task
can be seen as a self-contained and self-sufficient unit with a kernel ready to
be executed on the underlying architecture, often tasks require their
environment to be set up so that some libraries, data or accessory programs
are made available.

Several auxiliary properties play a fundamental role in distinguishing
\pilot systems implementations, as well as address, set and provide
constraints on their usability.  Programming and user interfaces;
interoperability across differing middleware and other \pilot systems;
multitenancy; strategies and abstractions for data management;
security including policies alongside authentication and
authorization; support for multiple usage modes like HPC or HTC; or
robustness in terms of fault-tolerance and high-availability; are all
examples of properties that might characterize a \pilot implementation
but in of themselves, would not distinguish a \pilot as a unique
system.

Both core and auxiliary properties have a direct impact on the multiple use
cases for which \pilot systems are currently engineered and deployed. For
example, while every \pilot system offers the opportunity to schedule the
tasks of a workload on a pilot, the degree of support of specific workloads
varies vastly across implementations. Furthermore, some \pilot systems
support Virtual Organizations and running tasks from multiple users on a single
pilot while others support jobs using a Message Passing Interface (MPI).
Analogously, all \pilot systems, support the execution of one or more type
of workload but they differ when considering execution modalities that maximize
throughput (HTC), computing (HPC) or container-based high scalability (Cloud).

\subsubsection{Core properties}
\label{sec:coreprops}

Following is the list of core properties. This list of properties is minimal
and complete. Note that these are the properties of \pilot implementations, and
not of (individual) instantiations of \pilots.

\begin{itemize}

%core because: it says something elementatary about the principle element
\item \textbf{Pilot Resource Capabilities}: In~\S\ref{sec:3} pilots have been
  defined as placeholders for resources. As such, the resource capabilities of
  each pilot depends upon the type and capabilities of the resource it exposes.
  Pilots usually expose computing resources but, depending on the capabilities
  offered by the infrastructure where the pilot is instantiated, they might
  also expose data and network capabilities. Within the domain of each type of
  resource and infrastructure, some of the typical characteristics of pilots
  are: Size (e.g. number of cores), lifespan, intercommunication (e.g.
  low-latency or inter-domain), computing platforms (e.g. x86, or GPU),
  file systems (e.g. local, shared, or distributed).
  Another consideration in discussing \pilot resource capabilities is the
  notion of granularity, e.g. is there one \pilot for a set of resources within
  on infrastructure or is there a more fine grained coupling between \pilot and
  resource and how does this effect the suitability for HTC and/or HPC?
  \msnote{This should not overlap with Task Execution Modes}
% \item \textit{Pilot Granularity (spatial)}: What unit of resource is
% controlled by a single pilot? A core, a node, multiple nodes, a cluster, ...?


% core because: its a must-have to do pilot provisiong
% (alternative view: aux because its an implementation detail \ldots)
\item \textbf{Resource Interaction}: The provisioning of pilots
  depends on how the \pilot system interfaces with one or more
  targeted infrastructure(s). In this context, the degree of coupling
  between the \pilot system and the infrastructure can vary,
  depending on how the \pilot system is deployed, how much it is
  integrated with the middleware used within the targeted
  infrastructure, whether the \pilot system is interoperable across
  multiple types of middleware, and how much information the \pilot
  system needs about the states and capabilities of the targeted
  infrastructure.
  \msnote{This should not overlap with Task Execution Modes}
% \item \textit{Supported DCI systems/services}: What are the type of systems
% (middlewares, services) that can be interfaced by the \pilotjob system to
% launch pilots?
% \item \textit{Supported Middleware:} What middleware does the system connect with? Is it tightly coupled or flexible/extensible?
% \item Supported Middleware (Auxiliary)

% core because: its fundamental to our model
\item \textbf{Overlay Management}: The total set of resources fostered by
  \pilots together form the overlay onto which a workload can be dispatched.
  Whether this overlay is left to chance or careful crafting by the \pilot
  system is a major implication.
  % resource acquisition/release policies,
% % (e.g. the automatic provisioning of \pilots).

% \item \textit{Pilot instantiation (spatial+temporal)}: When are pilots
% instantiated where and which entity (system/user) has control over it at
% which level?

% core because: both tasks and workload are core components
\item \textbf{Workload Semantics}: The tasks of a workload are dispatched to
  pilots depending on their semantics. Specifically, dispatching decisions
  depends on the relationship held by a task with the other tasks belonging to
  the workload, the affinity they require between data and compute resources,
  and the type of capabilities they require in order to be executed. \pilot
  implementations support a varying degree of semantic richness for the
  workload and its tasks.  Additionally of interest is the (standard) format or
  language in which the workloads are described.

% core because: this is of the heart of pilot systems
\item \textbf{Task Binding Characteristics}: One of the core functionalities
  implied by Task Dispatching is the binding of tasks to pilots. Without such
  capability, it would not be possible to know where to dispatch tasks, \pilots
  could not be used to execute tasks and, as such, the whole \pilot system
  would not be usable. As seen in \S\ref{sec:3}, \pilot systems may allow for
  two types of binding between tasks and \pilots: early binding and late
  binding. \pilot system implementations differ in whether and how they support
  these two types of binding. Specifically, while there might be
  implementations that only support a single type of binding behavior, they
  might also differ in whether they allow for the users to control directly
  what type of binding is performed, and in whether both types of binding are
  available on an heterogeneous pool of resources.
  Besides the binary decision between early and late binding, the \pilot system
  can expose more detailed application-level scheduling decisions, dispatch
  policies, etc., and might even include more levels of scheduling.
% % does the framework support advanced workload management capabilities
% \item \textit{Task-pilot relationship (temporal)}: What is the lifetime of a
% pilot in relation to the length of a task or workload? \msnote{What kind
% of answers to you expect here in section 4.3?}
% \item textit{Task-pilot relationship (spatial)}: Does a pilot execute one or
% more tasks concurrently?

% core because: its the only property of the task executor
% component/functionality
\item \textbf{Task Execution Modes}: Once the tasks are dispatched to
  a pilot, their execution may require for a specific environment to
  be set up.  \pilot system implementations differ in whether and how they
  offer such a capability. \pilot system implementations may adopt
  dedicated components for managing execution environments, or they
  may rely on ad hoc configuration of the pilots. Furthermore,
  execution environments can be of varying complexity, depending on
  whether the \pilot system implementation allows for data retrieval,
  dedicated software and library installations, communication and
  coordination among multiple execution environment and, in case,
  pilots.
% \item textit{Supported ``physical'' task types}: What types of (physical)
% tasks are supported, e.g., single-core, MPI, ...?

\end{itemize}


\subsubsection{Auxiliary properties}
\label{sec:auxprops}

\begin{itemize}

% aux because: its an "implementation" detail
\item \textbf{Architecture}: \pilot systems may be implemented by
  means of different type of architectures (e.g service-oriented,
  client-server, or peer- to-peer). For example, it is conceivable
  that architectural choices influence if not preclude certain
  deployment strategies. The analysis and comparison of architectural
  choices is limited to the trade-offs implied by such a choice,
  especially when considering how they affect the Core Properties.
% \item \textit{Transfer Protocols}: What network protocols that are used to
% connect the individual component of the \pilot system?

% (v) does the framework have dependencies to other third-party
% components/services?

% Further non-functional aspects describe the internals of a \pilotjob system,
% e.\,g.: (i) the architecture of the system (layers, sub-systems,
% communication \& coordination, central vs. decentral architectures; push vs.
% pull model, agent-based; number of supported resource types),

% aux because: its an implementation detail not directly coupled to one of the
% components or functions
\item \textbf{Coordination and Communication}: Earlier in
  \ref{sec:compsandfuncs} we discussed extensively the importance of
  Coordination and Communication, but also suggested it as a
  non-defining function of a \pilot system.  The details of
  Communication and Coordination between its components do distinguish
  various \pilot systems from each other.

% aux because: its an implementation detail
\item \textbf{Interface}: The implementations of \pilot systems may present
  several types of interfaces. For example, there are interface considerations
  between the main components of the \pilot system, between the application and
  \pilot system, between end users and the \pilot system, and for one or more
  programming languages. Some of this interface implications might be a
  consequence of the Architecture of the \pilot system.
  Here we will focus on the external interface.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Interoperability}: Two types of interoperability are
  relevant when analyzing different \pilot system implementations:
  Interoperability with multiple type of resources and
  interoperability across diverse \pilot systems. The former allows
  for the \pilot systems to provision pilots and execute workloads on
  different type of resources and systems (e.g. HTC, HPC, Cloud but
  also Condor, LSF, Slurm or Torque), while the latter becomes
  relevant when considering a landscape where multiple \pilot system
  implementations are available with diversified characteristics and
  capabilities.\mrnote{Note from meeting with SJ: May want to
  change to discuss one first (including descriptive text), then
  change tracks to discuss the second - they are two
  very different concepts to grasp in the first sentence (I know
  this detracts from the mathematical/proof language of it,
  but I think its a good idea). Especially since the second
  type of interoperability is very much less-common to a
  single PJ system (except in cases like \panda)}
% (ii) resource access layer: What low-level infrastructures/middleware are
% supported? How interoperable is the framework (vertically, horizontally)?,

% %Does the \pilotjob system provide interoperability support?

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Multitenancy}: A \pilot system may offer multitenancy
  at both system and local level. When offered at system level,
  multiple users are allow to utilize the same instance of a \pilot
  system, while when available at local level, multiple users may use
  the same pilot instance or any other component implemented within the
  \pilot system.
  \msnote{Alternative names: deployment, service model}
% \item \textit{Multi-user support:} Does the pilot job system allows \textbf{\pilots} to
% be used by multiple users?
% (iii) the % deployment model: hosted service versus tool/library, application
% vs.  system-level,

% \item \textit{Pilot ownership}: Who owns the pilot (job) -- the user or the
% \pilotjob system? Or in other words, in which security context does the pilot
% operate? \msnote{Dont think we have defined "user" by now, so thats ambiguous}
% \msnote{As there will be a wide spectrum of approaches in the various
% implementation, we might just want to call this as general as "security"}
% \aznote{I think this covers potentially multiple concepts: 1) security, as in
% the level of direct access users have to pilots (can they for instance force a
% kill directly?  Modify pilot configurations while it runs?) 2) Deployment, as
% in does the entire pilot-job system operate in user-space or is it part of the
% (admin/privileged) system stack? 3) I would argue that this could relate
% directly to pilot-instantiation... both from a security and deployment level.
% Who can instantiate pilots? What security level do they run on once
% instantiated?  So, maybe a ``security'' item as Mark suggested, but I'm not
% entirely sure if ``deployment'' should be part of this as well...  they do seem
% connected to me though...}
% Further,
% some \pilotjobs solely support single users, while more complex
% \pilot-based workload managers commonly support multiple users
% (e.\,g.\ using glexec).

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Robustness}: Used to identify those properties that
  contribute towards the resilience and the reliability of a \pilot system
  implementation. In this Section, the analysis focuses on
  fault-tolerance, high-availability and state persistence. These
  properties are considered indicators of both the maturity of the
  development stage of the \pilot system implementation, and the type of
  support offered to the paradigmatic use cases introduced in Section
  2.
% \item \textit{Fault-Tolerance}: What mechanisms are in place to shield the
% system from component failures?

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Security}: While the properties of \pilot system
  implementations related to security would require a dedicated
  analysis, we limit the discussion to authentication, authorization
  and policies. The scope of the analysis is further constrained by
  focusing the analysis only on those elements of these properties
  that impact the Core Functionalities as defined in
  \S\ref{sec:3}.
% \item \textit{Security}: How is a user identified in the system and how are
% identities and access credentials delegated to individual resources / pilots?
% Security describes the security mechanisms used by the
% framework, e.\,g.\ for authentication of the user, for securing the
% communication protocol and for sandboxing the application.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Files and Data:} Does the framework provide data
  management capabilities or should that be done out of band? If so, what does
  it offer?

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Performance and scalability:}
  Performance and scalability define the response times and size of the
  workload a user can expect to be supported.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Development Model:}
  Is this a community effort?

\end{itemize}


\begin{table*}[t]
 \up
 \centering
 \begin{tabular}{|p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
    \textbf{Pilot\newline System} &
    \textit{Pilot\newline Resource\newline Capabilities} &
    \textit{Resource\newline Interaction} &
    \textit{Overlay\newline Management} &
    \textit{Workload\newline Semantics} &
    \textit{Task\newline Binding\newline Characteristics} &
    \textit{Task\newline Execution\newline Modes} \\
  \hline
  \hline
    DIANE &
    HTC &
    GANGA &
    Out-of-Band / explicit &
    Programmable &
    Late &
    Serial \\
  \hline
    DIRAC &
    HTC &
    Custom &
    Community Service / implicit &
    None (Data dependencies?) &
    Late &
    Serial, some MPI \\
  \hline
    Falkon &
    HPC &
    Unspecified &
    Web Service &
    None &
    Late (mixed push/pull) &
    Serial \\
  \hline
    HTCondor &
    HTC (and to some degree HPC) &
    Condor-G &
    Explicit in Glidein case &
    Graph &
    Late &
    All \\
  \hline
    MyCluster &
    HPC &
    Custom (SGE / PBS / HTCondor) &
    CLI tools from respective LRMS &
    Workload semantics from respective LRMS &
    Agnostic &
    All \\
  \hline
    \panda &
    HTC &
    Custom, SAGA &
    Community Service / implicit &
    Task type, priority &
    Late &
    Serial, some MPI \\
  \hline
    RADICAL-Pilot &
    HPC &
    SAGA &
    Programmable / explicit &
    Programmable &
    Early \& Late &
    Serial \& MPI \\
 \hline
 \end{tabular}
 \caption{\textbf{Overview of \pilot systems and a summary their core properties.}}
 \label{table:implementations-properties}
\end{table*}

\subsection{Analysis of \pilot system Implementations}
\label{sec:implementations}

\newcommand{\vocab}[1]{\textbf{#1}\xspace}
\newcommand{\prop}[1]{\textit{#1}\xspace}
\newcommand{\impterm}[1]{\texttt{#1}\xspace}

In light of the common vocabulary discussion in \S\ref{sec:3}, a
representative set of \pilot systems has been chosen for further analysis.
Examining these \pilot systems using the common vocabulary exposes their core
similarities, and allows a detailed analysis of their differences.

The now following discussion of \pilot systems is ordered alphabetically.
To assist the reader, we make use of textual conventions: in \vocab{Bold} we
express the \vocab{Logical Components} and \vocab{Functionalities} of our model
from \S\ref{sec:compsandfuncs} and the \vocab{Terms and Definitions} from
\S\ref{subsec:3.2}, in \prop{Italic} we refer to the \prop{Properties} from
\S\ref{sec:properties} and in \impterm{Typewriter} we display terminology from
the respective \pilot system under discussion.

% \jhanote{this might go to the section where we begin describing
%   pilotjobs.. also will need an updated description of the methodology
%   employed to choose} ....such that ``exemplars'' were chosen; these
% are \pilotjobs which either laid foundational \pilotjob concept or
% incorporated strides in interoperability, usability, architecture or
% otherwise to advance the understanding and usage of \pilotjobs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Coasters
%
%\subsubsection{Coasters}
% AS PER SJ P* CALL: COASTERS SECTION IS CANCELED
% mrnote: Reason - insufficient documentation, no citations to actual science problems
% msnote: We probably want to revisit this decision, but not today :-)
%\subsubsection{Coasters}
%
%\msnote{Main coasters ref: \url{http://www.ci.uchicago.edu/swift/papers/UCC-coasters.pdf}}
%\mrnote{Coasters wiki: \url{http://wiki.cogkit.org/wiki/Coasters}}
%
%The Coaster System (or "Coasters") is a Java CoG based Pilot-Job system
%created for the needs of the Swift parallel scripting language.
%
%\paragraph{Design Goals}
%
%\begin{itemize}
%  \item Driven by the needs of Swift
%  \item Automatically-deployed to endpoint. Does not need user login to endpoint.
%  \item Supports file staging, on-demand opportunistic multi-node allocation,
%  remote log gin, and remote monitoring
%\end{itemize}
%
%\paragraph{Applications}
%
%\begin{itemize}
%  \item It has been used since 2009 for applications in fields
%that include biochemistry, earth systems science,
%energy modeling, and neuroscience.
%  \item Above claim made in Coasters paper - need to obtain citations
%\end{itemize}
%
% Swift~\cite{Wilde2011} is a scripting language designed for expressing abstract
% rest of this cut, but making a note of this in case we want
% to bring swift into the discussion later that i can find more info in 2011 paper
%% Swift~\cite{Wilde2011} is a scripting language designed for expressing
%% abstract workflows and computations. The language provides, amongst many other
%% things, capabilities for executing external applications, as well as the
%% implicit management of data flows between application tasks.
%% % For this
%% % purpose, Swift formalizes the way that applications can define
%% % data-dependencies. Using so called mappers, these dependencies can be
%% % easily extended to files or groups of files.
%% The runtime environment handles the allocation of resources and the spawning of
%% the compute tasks.
%% % Both data- and execution management capabilities are provided
%% % via abstract interfaces.
%% Swift supports e.\,g.\ Globus, Condor and PBS resources.
%% % The pool of resources
%% % that is used for an application is statically defined in a configuration file.
%% % While this configuration file can refer to highly dynamic resources (such as OSG
%% % resources), there is no possibility to manage this resource pool
%% % programmatically.
%% By default, Swift uses a 1:1 mapping for \cus and \sus. However,
%% Swift supports the grouping of SUs as well as PJs. For the PJ functionality, Swift uses the
%% Coaster~\cite{coasters} framework. Coaster relies on a master/worker
%% coordination model; communication is implemented using GSI-secured TCP sockets.
%% Swift and Coaster support various scheduling mechanisms, e.\,g.\ a FIFO and a
%% load-aware scheduler. Additionally, Swift can be used in conjunction with
%% Falkon~\cite{1362680}, which also provides \pilot-like functionality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIANE
%
\subsubsection{DIANE}
\label{sec:diane}

DIANE~\cite{Moscicki:908910} is a task coordination framework, which follows
the Master/Worker pattern.
It was developed at CERN for data analysis in the LHC experiment, but has since
been used in various other domains, though mainly in the Life Sciences.
As DIANE is a software framework, some of its semantics are not pre-determined,
as they can be implemented in various ways as the user sees them.
In that way it also provides \pilot functionality for job-style executions.

\paragraph{Resource Interaction}
The \pilot provisioning with GANGA~\cite{Moscicki20092303} provides a unified
interface for job submissions to various resource types.
In this way, DCI specifics are hidden from other parts of DIANE as well as the
from user.

\paragraph{Overlay Management}
The \prop{Overlay Management} of \pilots is done out-of-band by means of the
\impterm{diane-submitter} script which can launch a \pilot on any infrastructure
that is supported.

\paragraph{Workload Semantics}
As expected from the fact that DIANE is in essence a Master/Worker framework,
the \vocab{Tasks} have no relationships inside the framework, thereby
the \vocab{Workload Semantics} are that of a Bag-of-Tasks.
The \vocab{Workload} that is managed by the application specific part can
maintain any type of workload structure, as long as it can be translated to
individual tasks\cite{diane-dag, diane-etc}.
Plugins for other workloads, e.\,g.\ DAGs or for data-intensive application,
exist or are under development.
The framework is extensible: applications can implement a custom
application-level scheduler.

\paragraph{Coordination and Communication}
The \prop{Coordination \& Communication} in DIANE is based on
CORBA~\cite{OMG-CORBA303:2004} using TCP/IP.
The CORBA layer itself is invisible to the application.
Networking-wise, the workers are clients of the master server.
On TCP/IP level, communication is always uni-directionally from the
\impterm{WorkerAgent} to the \impterm{RunMaster}.
\prop{Security} is provided by a secret token that the \impterm{WorkerAgent}
needs to communicate back to the \impterm{RunMaster}.
This implies that the network requirements are such that the
\impterm{WorkerAgent} needs to be able to reach the \impterm{RunMaster} through
TCP/IP but not the other way around.
Bi-directional communication is achieved by periodic polling through
heartbeats by the \impterm{WorkerAgent}, where the \impterm{RunMaster} responds
with feedback.

\paragraph{Task Binding Characteristics}
DIANE is primarily designed with respect to HTC environments (such as
EGI~\cite{egi}), i.\,e.\ one \pilot consists of a single worker agent with the
size of 1 core.
Although the semantics of the binding are ultimately controllable by the
user-programmable scheduler, the general architecture is that of a pull model.
The pull model naturally implements the late-binding paradigm as the worker
will only pull a new task once it is available and has free resources.

\paragraph{Task Execution Modes}
DIANE is primarily designed with respect to HTC environments (such as
EGI~\cite{egi}), i.\,e.\ one \pilot consists of a single worker agent with the
typical size of 1 core/node, and thus, by default is not able to run (widely)
parallel applications (e.\,g.\ based on MPI).

\paragraph{Pilot Resource Capabilities}
One of the DIANE \prop{Pilot Resource Capabilities} is that it has the notition
of "capacity" and applications can make use of that property do diverge from
the "1 worker = 1 core" mode.

\paragraph{Architecture}
The central component of DIANE is the \impterm{RunMaster}.
It consists of a \impterm{TaskScheduler} and an \impterm{ApplicationManager}.
Both are abstract classes that need to be implemented for the specific purpose
at hand. Example and/or default implementations are provided for the user to
use or build upon.
Together the \impterm{TaskScheduler} and \impterm{ApplicationManager} implement
the \vocab{Workload Manager} component.

The \impterm{TaskScheduler} keeps track of the task entries and is responsible
for \vocab{Binding Tasks} to the \impterm{ApplicationWorkers}.
The implementation of the \impterm{ApplicationManager} is as its name suggest,
responsible for defining the application \vocab{Workload} and the creating
\vocab{Tasks} that are passed to the \vocab{TaskScheduler}.
As the \impterm{ApplicationWorker} asks for \vocab{Tasks} from the
\impterm{TaskScheduler} the natural way of implementing the scheduler is in a
\vocab{Late Binding} approach.

The \vocab{\pilot} in DIANE is the \impterm{WorkerAgent}.
The core component of the \impterm{WorkerAgent} is the
\impterm{ApplicationWorker}, which is an abstract class that defines three
methods that every implementation needs to implement.
Two of these methods are for initialization and cleanup and the last,
\impterm{do\_work()}, is the method that actually receives the task description
and executes the work, thereby being a close resemblance of the \vocab{Task
Executor}.

\paragraph{Interface}
DIANE's \prop{Architecture} is based on the \impterm{Inversion of Control}
design pattern.
It is a Python framework, that formulates certain hooks that an
\vocab{Application} can be programmed against. The aforementioned abstract
classes need to be implement to provide the application-specific semantics.
The \prop{Interface} to these "DIANE-\vocab{Applications}" is to start them
through the \impterm{diane-run} command.
As discussed in the \prop{Overlay Management} section, the creation of \pilots
is done out-of-band.

\paragraph{Performance and Scalability}
The authors report that in first instance they had implemented full
bi-directional communication, but that turned out to be difficult to correctly
implement and created scalability limitations.

\paragraph{Robustness}
The \prop{Robustness} in DIANE comes from the mature CORBA communication
layer, and from custom task-failure policies in the \impterm{TaskScheduler}.
Dealing with \prop{Robustness} in DIANE is twofold.
At one hand DIANE offers the mechanisms to supports fault tolerance: basic
error detection and propagation mechanisms are in place.
Further, an automatic re-execution of tasks is possible.
On the other hand, DIANE also leaves room for application specific strategies.

\paragraph{Files and Data}
The CORBA communiation channel between the Master and the Worker also allows
for file transfers, meaning that the \pilot not only exposes cores, but also
the file system on the worker node.

\paragraph{Interoperability}
\prop{Interoperability} to various middleware security mechanisms
(e.\,g.\ GSI, X509 authentication) and backends is achieved through its
integration with GANGA.

\paragraph{Security}
Access to various middleware security mechanisms (e.\,g.\ GSI, X509
authentication) is achieved through its integration with GANGA.

\paragraph{Multitenancy}
DIANE is in principle a single-user \pilot system, i.\,e.\ each \pilot is
executed with the privileges of the respective user.
Also, only workload of this respective user can be executed by DIANE.
However, \prop{Multitenancy} with DIANE does happen when it is used as a
backend for Science Gateways, where it serves multiple users on the same
installation, but with a single credential.

\paragraph{Development Model}
Used, maintained and developed by CERN with external contributions and users.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIRAC
%
\subsubsection{DIRAC}
\label{sec:dirac}

DIRAC (Distributed Infrastructure with Remote Agent Control) is a software
product that grew out of the CERN LHCb project\cite{diracgrid2004}.
DIRAC implements a community-wide \impterm{Workload Management System}(WMS) to
manage the computational payload of its community members.
The complete DIRAC ecosystem consists of the DIRAC Pilot framework and the WMS.

DIRAC's claim to fame is that ``... the LHCb data production run in summer 2004
was the first successful demonstration of the massive usage of the LCG grid
resources. This was largely due to the use of the DIRAC Workload Management
System, which boosted significantly the efficiency of the production
jobs.``~\cite{Tsaregorodtsev:2010cj}

\paragraph{Pilot Resource Capabilities}

The bootstrapping of the \impterm{DIRAC Agent} performs a full DIRAC
installation including a download of the most current version of the
configuration.
After completion of the DIRAC installation, the job checks for the working
conditions (e.g. where execution takes place, available resources, etc.) a
\impterm{DIRAC Agent} is started.

\paragraph{Resource Interaction}

\pilots are send to the resource by the \impterm{Pilot Director} and all
resource interaction is hidden behind that abstraction.
It is reported to be able to interact with different batch systems like
PBS/Torque, LSF, SGE and BQS.

\paragraph{Overlay Management}

\pilots are being launched by the \impterm{Pilot Directors} to the supported
DCIs.
The DIRAC \pilots create an overlay that presents a homogeneous layer to the
other components.
The user has no control over where \pilots are started.

\paragraph{Workload Semantics}

The user \vocab{Workload} (in DIRAC terminology: \impterm{payloads}),
consisting of independent \vocab{Tasks} have varying levels of priorities for
LHCb as a whole, as well as a variety of requirements for them to execute.

Additionally, users can specify requirements like needed input data for their
tasks and this will be taken into account when placing the task on a respource.

\paragraph{Task Binding Characteristics}

Pilots allow an effective implementation of the pull scheduling paradigm. Once
they are safely running in the final computing resource, they contact central
WMS servers for a late binding of the resource to the payload.

When \vocab{Tasks} come into the realm of the \impterm{WMS}, a consistency
check is performed and optional input data requirements are translated to
resource candidates.

Once this is done, \vocab{Tasks} are then organized into \impterm{TaskQueues}.
\impterm{TaskQueues} are sorted groups of \impterm{Payloads} with identical
requirements (e.g., user identity, number of cores, etc.) that are ready to run.

\impterm{TasqQueue Directors} direct the workload from the \impterm{TaskQueues}
for execution to the Job Agents.

\paragraph{Task Execution Modes}

Once the \vocab{Task} has been pulled, it is executed through means of a
\impterm{Job Wrapper}.
Given the nature of the resources DIRAC is aimed at, these are generally single
or few-core tasks.
Work on supporting MPI is ongoing\cite{}.

\paragraph{Coordination and Communication}

The \impterm{DIRAC Agent} is responsible for sending the \impterm{payload}
request to the central DIRAC WMS server and for the subsequent execution of the
received \impterm{payload}.
The client/service communication is performed with a light-weight protocol
(XML-RPC) with additional standards based authentication mechanisms.

\paragraph{Architecture}

The design of DIRAC relies on three main components: the \impterm{DIRAC Agent}
\impterm{TaskQueue}, and \impterm{TaskQueue Director}.

\paragraph{Interface}

The DIRAC project provides many command line tools as well as a comprehensive
Python API to be used in a scripting environment.
In DIRAC3 a full-featured graphical Web Portal was developed.

\paragraph{Interoperability}

DIRAC is not inter-operable with other \pilot systems.
Through the \impterm{Pilot Director} abstraction it does support execution of
\pilots on a variety of computing resources.

\paragraph{Multitenancy}

The community nature of the \impterm{DIRAC WMS} makes it an intrinsically
multi-user system.
The WMS holds the workload for multiple users and the pilot agents are shared
amongst the workload of multuple user.

\paragraph{Robustness}

The \impterm{Job Wrapper} creates a uniform environment to execute
\vocab{Tasks} indepent of the \vocab{DCI} where they run.
Furthermore, it retrieves the input sandbox, checks availability of required
input data and software, executes the payload, reports success or failure of
the execution, and finally uploads output sandbox and output data if required.

At the same time it also instantiates a \impterm{Watchdog} to monitor the
proper behaviour of the \impterm{Job Wrapper}.
The watchdog checks periodically the situation of the \impterm{Job Wrapper},
takes actions in case the disk or available cpu is about to be exhausted or the
payload stalls, and reports to the central \impterm{WMS}.
It can also execute management commands received from the central service, e.g.
to kill the \impterm{Payload}.

\paragraph{Security}

The DIRAC Secure client/service framework called \impterm{DISET} (DIRAC
SEcure Transport) is full-featured to implement efficient distributed systems
in the grid environment\cite{}.
It provides X509 based authentication and a fine-grained authorization scheme.
Addionally, the security framework includes logging service that provides
history to 90 days to investigate security incidents.

\paragraph{Files and Data}

Tasks within DIRAC that specify their input data are threated in a special way.
The WMS ensures that tasks are only started on resources where that data is
available or it makes sure that that data becomes available.

\paragraph{Performance and Scalability}

DIRAC has well documented numbers about scaling in supporting many users, on
many resources, for a variety of applications over a long period of time.

\paragraph{Development Model}

DIRAC is in active development and use by the LHCb community\cite{}.
More recently they have been reaching out to other communities too\cite{}.
From a development perspective there is ongoing work on enriching the data
capabilities\cite{} and to be able to execute MPI applications\cite{}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Falkon
%
\subsubsection{Falkon}
\label{sec:falkon}
%% % Falkon
%% % refers to \pilots as the so called provisioner, which are created using the
%% % Globus GRAM service. The provisioner spawns a set of executor processes on the
%% % allocated resources, which are then responsible for managing the execution of
%% % SUs. \cus are submitted via a so called dispatcher service. Similar to Coaster,
%% % Falkon utilizes a M/W coordination model, i.\,e.\ the executors periodically
%% % query the dispatcher for new SUs. Web services are used for communication.

%\mrnote{Main Falkon ref: \url{http://dev.globus.org/images/7/78/Falkon_SC07_v42.pdf}}
%\mrnote{Best Falkon ref: \url{http://dev.globus.org/wiki/Incubator/Falkon}}

The Fast and Light-weight tasK executiON framework (Falkon) ~\cite{1362680} was
created with the primary objective of enabling many independent tasks to run on
large computer clusters (an objective shared by most \pilotjob systems).
A particular focus on performance and time-to-completion for jobs on such
clusters drove Falkon development.
In addition to being \textit{fast}, Falkon, as its name suggests, also focused
on lightweight deployment schemes.

\paragraph{Pilot Resource Capabilities}

Falkon exposes resource as a set of cores as tasks are by definition single
core only.

\paragraph{Resource Interaction}

Falkon was originally developed for use on large computer clusters in a grid
environment, but has since been expanded to work on clouds and other resources.

Falkon has been shown to run on TeraGrid (now XSEDE), TeraPort, Amazon EC2, IBM
Blue Gene/L, SiCortex, and Workspace Service~\cite{1362680}.

\paragraph{Overlay Management}

The Dispatcher service in Falkon is implemented by means of a web service.
This Dispatcher implements a factory/instance deployment scenario.
When a new client sends task submission information, a new instance
of a Dispatcher is created.

\paragraph{Workload Semantics}

Falkon has been integrated with the Karajan workflow language and execution
engine, meaning that applications that utilize Karajan to describe their
workflow will be able to be executed by Falkon.

The Swift parallel programming system~\cite{Wilde2011} was integrated
with Falkon for the purpose of task dispatch.

\paragraph{Task Binding Characteristics}

Cores and tasks are considered homogeneous. Tasks are pulled in to the nodes
and are thereby of late-binding nature.

\paragraph{Task Execution Modes}

Falkon does not support MPI jobs, however - a limiting factor in its adoption
to certain scientific applications.

\paragraph{Coordination and Communication}

Interaction between the components is by use of the Globus Web Services model.

\paragraph{Architecture}

Falkon's architecture relies on the use of multi-level scheduling
as well as efficient dispatching of tasks to heterogeneous DCIs.
As mentioned above, there are two main components of Falkon:
(i) the Dispatcher for farming out tasks and
(ii) the Provisioner for acquiring resources.

The overall task submission mechanism can be considered
a 2-tier architecture; the Dispatcher (using the above terminology,
this is the \pilot-Manager) and the Executor (the \pilot-Agent).

The Dispatcher is a GRAM4 web service whose primary
function is to take task submission as input and farm
out these tasks to the executors. The Executor runs on
each local resource and is responsible for the actual
task execution. Falkon also utilizes \textit{provisioning}
capabilities with its Provisioner.

The Falkon Provisioner is the closest analogous entity to a \pilot: it is the
creator and destroyer of Executors, and is capable of providing both static and
dynamic resource acquisition and release.

Further, in order to process more complex workflows, Falkon has been integrated
with the Karajan workflow execution engine~\cite{karajan}.

This integration allows Falkon to accept more complex workflow-based scientific
applications as input to its \pilot-like job execution mechanism.

\paragraph{Interface}

Simpler task execution can be achieved without modifying the existing
executables - sufficient task description in the web service is all that is
required to utilize the Falkon system.

\paragraph{Interoperability}

For launching to resources Falkon relies on the availability of GRAM4.
This abstracts the resource details but also limits the use of other resources.

\paragraph{Multitenancy}

Each instantiation of the Dispatcher maintains its own task queue and state -
in this way, Falkon can be considered a single-user deployment scheme, wherein
the ``user'' in this case refers to an individual client request.

\paragraph{Robustness}

Falkon supports a fault tolerance mechanism which suspends and dynamically
readjusts for host failures.

\paragraph{Security}

In its use of Globus transport channels it allowed for both encrypted and
non-encrypted operation.
The non-encrypted verion was used to gain most throughput, but obviously doing
concessions on security for real-world use.

\paragraph{Files and Data}

The data management capabilities of Falkon also extend beyond the core
\pilotjob functionalities as described above.

Falkon has extended its capabilities
to encompass advanced data-scheduling and caching.

Work has also been done on Falkon to expand its data capabilities.
In addition to data caching and efficient data scheduling techniques, Falkon
has adopted a data diffusion approach.
Using this approach, resources for both compute and data are acquire
dynamically and compute is scheduled as close as possible to the data it
requires.
If necessary, the data diffusion approach replicates data in response to
changing demands~\cite{raicu2008accelerating}.

\paragraph{Performance and Scalability}

As previously stated, the design of Falkon was centered around the goal of
providing support to run large numbers of jobs efficiently on large clusters
and grids.
Falkon realizes this goal through the use of (i) a dispatcher to reduce the
time to actually place tasks as jobs onto specific resources (such a feature
was built to account for different issues amongst distributed
cyberinfrastructure - such as multiple queues, different task priorities,
allocations, etc), (ii) a provisioner which is responsible for resource
management, and (iii) data caching in a remote environment~\cite{1362680}.

Falkon has been tested for throughput and performance in such as applications
as fMRI (medical imaging), Montage (astronomy workflows), and MolDyn (molecular
dynamics simulation) and has shown favorable results in terms of overall
execution time when compared to GRAM and GRAM/Clustering
methods~\cite{1362680}.

The per task overhead of Falkon execution has been shown to be in the
millisecond range.

Falkon has been demonstrated to achieve throughput in the range
of hundreds to thousands of tasks per second for very fine-grained
tasks.

\paragraph{Development Model}

The Falkon project ran from 2006 to 2011, source code is not available.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% GWPilot
%
%\subsubsection{GWPilot}
%\label{sec:gwpilot}
%% \aznote{Considering the new direction of the paper, I am not sure whether
%% GWPilot~\cite{gwpilot} requires its own section for analysis...}
% msnote: To be reconsidered: but not now
%% %\begin{lstlisting}[breaklines]
%% %\url{https://indico.egi.eu/indico/materialDisplay.py?contribId=18&sessionId=46&materialId=slides&confId=1019}
%% %\url{http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266981}
%% \begin{itemize}
%% \item Integrates with GridWay metascheduler
%% \item Pilots advertise to GridWay, GridWay scheduler schedules pilots
%% \item Pilots pull tasks from scheduler
%% \item Installation as a GridWay driver -- written in Python
%% \item Interoperability managed by GridWay drivers (DRMAA, JDSL, BES, more?)
%% \item Using GWPilot requires only adding a single line to their GridWay task
%% \item ``Lightweight and scalable''
%% \end{itemize}
%% %\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% HTCondor
%
\subsubsection{HTCondor}
\label{sec:htcondor}

HTCondor can be considered one of the most prevalent distributed computing
projects of all time in terms of its pervasiveness and size of its user
community.
Is often cited as the project that introduced the \pilotjob concept.\cite{}
Similar in many respects to other batch queuing systems, HTCondor puts special
emhphasis on high-throughput computing (HTC) and opportunistic computing.
HTC is defined as providing large scale computational power on a long term
scale.
Opportunistic computing is about making pragmatic use of computing resources
whenever they are available, without requiring full availability.
HTC can be achieved by applying opportunistic computing.

% But despite this fact, describing HTCondor along the lines of a \pilotjob
% system has turned out to be difficult, if not partly impossible for several
% reasons.

% The \pilot concept is in its heart centered around Condor Glideins.
% Many Workload Management Systems make use of the core functionality, ranging
% from many specific WMS's to the ``generic`` GlideinWMS.


\paragraph{Architecture}

At the heart of it, HTCondor is a high-throughput distributed batch computing
system.
It accepts user tasks and executes them on one or more resources gathered in an
HTCondor \impterm{pool}.

In a pool, user tasks are represented through job description files and
submitted to a (user-)\impterm{agent} via command-line tools.
Agents are deployed as (\impterm{schedd}) system services on so-called
\impterm{gateway machines} (user front-ends of an HTCondor pool) and accept
tasks from multiple users.
Agents implement three different aspects of the overall architecture:
(1) they provide persistent storage for user jobs, (2) they find resource for
a task to execute by contacting the \impterm{matchmaker} and (3) they marshal
task execution via a so-called \impterm{shadow}.

The matchmaker, also called the \impterm{central manager}, is another system
service that realizes the concept of late binding by matching user tasks with
one or more of the resources available to an HTCondor pool. The matchmaking
process is based on \impterm{ClassAds} a description language that can capture
both, resource capabilities as well as task requirements.

\vocab{Resources} are tied into an HTCondor pool by \impterm{startd} system
services.
The \impterm{startd} services are deployed on the pool's compute resources.
They report resource state and  capabilities back to the matchmaker and start
tasks submitted by agents on the resource in encapsulated \impterm{sandboxes}.

Resources in a pool can span a wide spectrum of system.
While some pools are comprised of regular desktop PCs (sometimes called a
campus grid), other pools incorporate large HPC clusters and cloud resources.
Hybrid pools with heterogeneous sets of resources are also common.

It is possible for two or more HTCondor pools to ``collaborate'' so that one
pool has access to the resources of another pool and vice versa.
In HTCondor, this concept is called
\impterm{flocking}~\cite{Epema:1996:flocking} and allows agents to query
matchmakers outside their own pool for compatible resources.
Flocking is used to implement load-balancing between multiple pools but also to
provide a broader set of heterogeneous resources to user communities.

The components described above, jobs, agent, matchmaker, resource, shadow and
sandboxes are sometimes collectively referred to as the \impterm{HTCondor
Kernel} and satisfy the requirements for a \pilot system as outlined in
Section \ref{subsec:vocab_core_functionalities}.
However, the provisioning, allocation and usage of resources within a pool can
differentiate between different pools and multiple different approaches and
software systems have emerged over time, all under the umbrella of the wider
HTCondor project.

%\paragraph{Pilot Resource Capabilities}
\paragraph{Resource Interaction}

In its native mode, HTCondor is the middleware, and therefore the discussion of
resource interaction doesn't apply.


%\paragraph{Condor-G -- Condor via Globus}
Condor-G is an alternative (user-)agent for HTCondor that can ``speak'' the
Globus GRAM (Grid Resource Access and Management) protocol. GRAM services are
often deployed as remote job submission endpoints on top of HPC cluster
queuing systems. Condor-G allows users to incorporate those HPC resources
temporarily to an HTCondor pool.

Condor-G agents use the GRAM protocol to launch HTCondor \impterm{startd}
\pilots ad hoc via a GRAM endpoint service on a remote system. Tasks submitted
through the Condor-G (user-)agent are then assigned by a local matchmaker to
these ad-hoc provisioned \pilots. This concept is called \impterm{gliding-in}
or \impterm{GlideIn}. It implements \vocab{late-binding} on top of GRAM/HPC
systems: the (user-)agent can assign tasks to \impterm{startd}s after
they have been scheduled and started through the HPC queueing system. This
effectively decouples resource allocation (\impterm{startd} scheduling through
GRAM) and task assignment.

\paragraph{Overlay Management}

% \paragraph{glideinWMS -- Automated \pilot Provisioning}
glideinWMS, a workload management system (WMS)~\cite{1742-6596-119-6-062044}
based on Condor GlideIns introduces advanced \pilotjob capabilities to HTCondor
by providing automated \pilot (\impterm{startd}) provisioning based on the state
of an HTCondor pool.

%\paragraph{Workload Semantics}
%\paragraph{Task Binding Characteristics}
%\paragraph{Task Execution Modes}
%\paragraph{Coordination and Communication}

\paragraph{Interface}

The main HTCondor distribution provided command line utils to setup these
Glideins. These are now replaced by Bosco.
% -----------------------------------------------------------------------------
%
%\subsubsection{Corral}
%\onote{Corral seems to be a component in the glidein-WMS landscape and
%not an independent pilot-job implementation. I don't think that we should
%dedicate an extra section to it.}
%\aznote{SJ asked for an investigation of Corral -- not sure that this
%deserves a full analysis at this point on a technical level despite
%being commonly used, open to suggestions}

%Corral is designed to allow hybrid HTC/HPC execution, in which
%many small jobs may be executed in conjunction with larger runs.
%\cite{Rynge:2011:EUG:2116259.2116599}
%\aznote{Back this up w/ paper refs -- paper is somewhat dated,
%verify this is still true today}.  Corral operates as a Glidein
%WMS frontend\aznote{main reason that I think we shouldn't include
%Corral in its own section...}, where GlideinWMS manages the size
%of Condor glide-in pools.
%\begin{itemize}
%\item Workflow - handled by Pegasus workflow management system
%  \aznote{True in the paper I am using, but not handled by Corral itself, so should we include this?}
%\item Placeholder - handled by multislot requests
%  \aznote{multislot request requests a single large
%    GRAM job, and then starts glideins within this container}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% -----------------------------------------------------------------------------
%
%\paragraph{BoSCO}

%\onote{It seems that BOSCO is some sort of a user-space version of Condor.
%It can interface with single clusters as well as complex HTC grids (GlideinWMS).
%Multiple resource-scanrios are possible. There is a bosco-submit node which
%holds the user jobs. Bosco uses Condor glidein (-agents) internally as a
%resource overlay. The glideins pull data from the bosco-submit node. The
%main difference between BOSCO and Condor (even though both expose the same
%user API) is that BOSCO allows ad-hoc usage, while Condor requires a rather
%complex setup. In that regard, BOSCO is somehwat similar to BigJob.
%This page is somehwat insightful: http://bosco.opensciencegrid.org/about/}

%\subsubsection{Bosco}
%% \\
%% \begin{itemize}
%% \item Condor used as batch system/user interface
%% \item Single submit model for different cluster types (LSF/PBS/SGE/etc) via SSH with \texttt{BLAHPD}
%% \item Campus Factory (condor overlay generator) creates glideins, checks users queue for idle jobs,
%%   enforces submission policies
%% \item Bosco = ``BLAHPD Over SSH Condor Overlay''
%% \item Workstation-based (run Bosco client locally)
%% \item Multi-user (Bosco workstation install can be used by multiple researchers)
%% \item Supports multiple cluster submission (but what about coordination...)

BoSCO is a user-space job submission system based on HTCondor. BoSCO was
designed to allow individual users to utilize heterogeneous HPC and grid
computing resources through a uniform interface. Supported backends include
PBS, LSF and GridEngine clusters as well as other  grid resource pools managed
by HTCondor. BoSCO supports both, an agent-based (\textit{glidein} / worker)
and a native job execution mode through a single user-interface.

BoSCO exposes the same \impterm{ClassAd}-based user-interface as HTCondor,
however, the backend implementation for job management and resource
provisioning is significantly more lightweight than in HTCondor and explicitly
allows for ad hoc user-space deployment. BoSCO provides a \pilot-based
system that does not require the user to have access to a centrally-
administered HTCondor campus grid or  resource pool. The user has direct
control over \pilot agent provisioning (via the \impterm{bosco\_cluster}
command) and job-to-resource binding via \impterm{ClassAd} requirements.

The overall architecture of BoSCO is very similar to that of HTCondor. The
\impterm{BoSCO submit-node} (analogous to Condor \impterm{schedd}) provides the
central job submission service and manages the job queue as well as the worker
agent pool. Worker agents communicate with the \impterm{BoSCO submit-node} via
pull-requests (TCP). They can be dynamically added and removed to a
\impterm{BoSCO submit-node} by the user. BoSCO can be installed in user-space
as well as in system space. In the former case, worker agents are exclusively
available to a single user, while in the latter case, worker agents can be
shared among multiple users. The client-side tools to submit, control and
monitor BoSCO jobs are the same as in Condor (\impterm{condor\_submit},
\impterm{condor\_q}, etc).

%\impterm{CorralWMS:}
CorralWMS is an alternative front-end for GlideinWMS-based
infrastructures. It replaces or complements the regular GlideinWMS front-end
with an alternative API which is targeted towards workflow execution. Corral was
initially designed as a standalone pilot (glidein) provisioning system for
the Pegasus workflow system where user  workflows often produced workloads
consisting of many short-running jobs as well as mixed workloads consisting of
HTC and HPC jobs.

Over time, Corral has been integrated into the GlideinWMS stack as CorralWMS.
While CorralWMS still provides the same user-interface as the initial, stand-
alone version of Corral, the underlying pilot (glidein) provisioning is
now handled by the GlideinWMS factory.

%\paragraph{Interoperability}
\paragraph{Multitenancy}

The main differences between the GlideinWMS and the CorralWMS front-ends lie in
identity management and resource sharing.
While GlideinWMS pilots (glidins) are provisioned on a per-VO base and shared /
re-used amongst members of that VO, CorralWMS pilots (glideins) are bound to
one specific user via personal X.509 certificates.
This enables explicit resource provisioning in non-VO centric environments,
which includes many of the HPC clusters that are part of U.S. national
cyberinfrastructure (e.g., XSEDE).

%\paragraph{Robustness}
%\paragraph{Security}
%\paragraph{Files and Data}
%\paragraph{Performance and Scalability}
\paragraph{Development Model}

HTCondor is used in multiple different contexts: the HTCondor project,
the HTCondor software and HTCondor grids.
But even if we only look at the software parts of the landscape, we are faced
with a plethora of concepts, components and services that have been
grown and curated for the past 20 years.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MyCluster
%
\subsubsection{MyCluster}
\label{sec:mycluster}

MyCluster~\cite{1652061} was developed to allow users to submit and manage jobs
across heterogeneous NSF TeraGrid resources in a uniform, on-demand manner.
TeraGrid, the predecessor of XSEDE, existed as a group of compute clusters
connected by high-bandwidth links facilitating the movement of data, but with
many different job deployment middlewares requiring cluster-specific submission
scripts.
MyCluster allowed all cluster resources to be aggregated into one personal
cluster with a unified interface, being either SGE, OpenPBS or Condor.
This enhancement to user control was envisioned as a means of allowing users to
submit and manage thousands of jobs at once across heterogeneous distributed
computing infrastructures, while providing a familiar and homogene interface
for submission.

Applications are launched via MyCluster in a ``traditional'' HPC manner, via a
\impterm{virtual login session} which contains usual queuing commands to submit
and manage jobs.
This means that applications do not need to be explicitly rewritten to make use
of MyCluster functionality; rather, MyCluster provides user-level \pilot
capabilities which users can then use to schedule their applications with.

MyCluster was designed for and successfully executed on NSF TeraGrid
resources, enabling large-scale cross-site submission of ensemble
job submissions via its virtualized cluster interface.
This approach makes the \vocab{multi-level scheduling} abilities
of \pilot implicit; rather than directly \prop{binding}
to individual TeraGrid resources, users allow the virtual grid
overlay to \vocab{schedule} tasks to multiple allocated TeraGrid sites
presented as a single cohesive, unified resource.

\paragraph{Task Execution Modes}
MyCluster's \prop{Task Execution Modes} are explicitly limited to
``serial`` tasks although it is unspecified whether this also confines
it to single core \vocab{tasks} only.

\paragraph{Pilot Resource Capabilities}
As per the earlier description, MyCluster was specifically targetted to
TeraGrid resources and thereby the \prop{Pilot Resource Capabilities} are
those of the TeraGrid, being HPC resources.
As the \vocab{Task Execution Modes} are limited to serial tasks, the only
relevant resource that is exposed is the ``CPU`` or ``core``.

\paragraph{Resource Interaction}
The \prop{Resource Interaction} is exclusively by the \impterm{Agent Manager}
through Globus GRAM to start a \impterm{Proxy Manager} at each site.

\paragraph{Workload Semantics}
All \vocab{tasks} are considered independent and no other \prop{Workload
Semantics} are described.

\paragraph{Task Binding Characteristics}
As the \pilots are not exposed (and the \vocab{tasks} are considered
homogeneous) there are no explicitly controllable \vocab{Task Binding
Characteristics}. This in effect makes it a \vocab{Late Binding} mechanism.

\paragraph{Overlay Management}
With respect to \vocab{Overlay Management} MyCluster allows the user to
configure the number of \impterm{Proxies} per site, the number of CPUs per
\impterm{Proxy} and the list of sites to submit to.
All resources that are acquired through the \impterm{Proxy Managers} are pooled
together.
Of course, all of these resource specifications are still requests, and it
depends on the behavior of the queue whether (and when) these resources are
actually acquired.
In addition to these static resource specifications, MyCluster includes a mode
of operation where \impterm{Proxies} can be \impterm{Migrated} to other sites.
The rationale for \impterm{migration} is that the weight of the resource
requests can be moved to the site with the shortest queing times.

\paragraph{Architecture}
When the user starts a session via the \impterm{vo-login} command a
\impterm{Agent Manager} is instantiated.
The \impterm{Agent Manager} on its turn starts a \impterm{Proxy Manager} at
every resource site gateway host through Globus GRAM and a \impterm{Master Node
Manager} at the local client machine.
The \impterm{Proxy Manager} has an accompanying \impterm{Submission Agent} that
also runs on the resource gateway.
The \impterm{Submission Agent} interacts with the local queuing systems and
submits the \impterm{Job Proxy} that will launch a \impterm{Task Manager}
process on a compute host.
The \impterm{Task Manager} will start one or more \impterm{Slave Node Manager}
processes on all the allocates compute hosts.
Ultimately, the \impterm{Slave Node Manager} is now in control of the worker
node.
Depending on the configuration, the \impterm{Slave Node Manager} will now start
the Condor, SGE or OpenPBS job-starter daemons, which on their turn will
connect back to their master daemon started earlier on the local client machine
by the \impterm{Master Node Manager}.

\paragraph{Coordination and Communication}
The communication between \impterm{Proxy Manager} and \impterm{Agent Manager}
is over TCP.

\paragraph{Security}
The TCP communication channels are not encrypted but do use GSI-based
authentication and has meaures in place to prevent replay attacks.

\paragraph{Robustness}
The \impterm{Agent Manager} maintains little state. It is assumed that ``lost``
\impterm{Proxy Managers} will re-connect back when they recover.
The \impterm{Proxy Manager} is restartable and will try to re-establish the
\impterm{Proxies} based on a last-known state stored on disk when they got
lost due to exceeding wallclock limitations or node reboots;

\paragraph{Interoperability}
The primary goal of MyCluster was \prop{Interopability} over multiple TeraGrid
resources.

\paragraph{Interface}
The \prop{Interface} of MyCluster depends on the choice of the underlying
system as that one is exposed through the \impterm{Virtual Login Session}.
The interface also allows users to interactively monitor the status of their
\vocab{\pilots} and \vocab{tasks}.

\paragraph{Multitenancy}
The created cluster resides in userspace, and may be used to marshal multiple
resources ranging from small local resource pools (e.g.  departmental Condor or
SGE systems) to large HPC installations.
The clusters can be created per user, per experiment or to contribute to
the resources of a local cluster.

\paragraph{Files and Data}
MyCluster doest not offer and file staging capabilities other than those of the
underlying systems provide. This means that it exposes the file capabilities of
Condor, but doesn't offer any file staging capabilities when using SGE.

\paragraph{Development Model}
MyCluster is no longer developed or supported after the TeraGrid project came
to a conclusion in 2011.

\paragraph{Conclusion}
MyCluster illustrates how an approach aimed at \vocab{multi-level scheduling}
by marshalling multiple heterogeneous resources lends itself perfectly to a
\pilot-based approach.
The fact that the researchers behind it developed a complete \pilot system
while working toward interoperability/uniform access is a testament to the
usefulness of \pilots in attacking these problems.
The end result is a complete \pilot system, despite the authors of the system
being constructed not having used the word ``pilot'' once in their main
publication.

While not an official product, a recent and similar approach is conducted at
NERSC. The operators of the Hopper cluster provide a tool called MySGE which
allows the user to provision a personal SGE cluster on Hopper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% PanDA
%
\subsubsection{\panda}
\label{sec:panda}

%\paragraph{Pilot Resource Capabilities}
%\paragraph{Resource Interaction}
%\paragraph{Overlay Management}
%\paragraph{Workload Semantics}
%\paragraph{Task Binding Characteristics}
%\paragraph{Task Execution Modes}
%\paragraph{Coordination and Communication}
%\paragraph{Architecture}
%\paragraph{Interface}
%\paragraph{Interoperability}
%\paragraph{Multitenancy}
%\paragraph{Robustness}
%\paragraph{Security}
%\paragraph{Files and Data}
%\paragraph{Performance and Scalability}
%\paragraph{Development Model}

\panda was developed to provide a multi-user workload management system for
ATLAS~\cite{aad2008atlas}, which is a particle detector at the Large Hadron
Collider at CERN which could handle large numbers of jobs for data-driven
processing workloads.
In addition to the logistics of handling large-scale job submission, there was
a need for integrated monitoring for analysis of system state and a high degree
of automation to reduce the need for user/administrative intervention.
As \panda is a multi-stakeholder project that is active for many years, it
naturally went through many iterations.
We try to refer to the current state as much as possible.

The core component of \panda responsible for the \pilot aspects is
AutoPyFactory\cite{Caballero:2012ka}.
It is handles \pilot submission, management and monitoring system.
(AutoPyFactory supersedes the initial generation PandaJobScheduler, as well we
the second generation system, the AutoPilot.)

\paragraph{Design Goals}
\panda was designed as an advanced workload management system,
satisfying requirements such as the ability
to manage data, monitor job/disk space, and recover from failures.
It hereto adopted the \pilot paradigm.
A modular approach allowing the use of plug-ins was chosen in order
to incorporate additional features in the future.
\panda was designed from the ground-up to meet these requirements
while scaling to handle the large scale of jobs and data produced
by the ATLAS experiment.
%\aznote{Discussion on PanDA's design for LHC data processing and why \pilotjobs
%work for this, or is this out-of scope?}

\paragraph{Applications}
\panda has been used to process data and jobs relating to ATLAS.  Approximately
a million jobs a day are managed~\cite{pandapresentation2013-06}
which handle simulation, analysis, and other work~\cite{maeno_pd2p:_2012}.
The ATLAS experiment itself produces several petabytes of data a year
which must be processed and analyzed.
Current funding for \panda enables the project to reach out to new user
communities.

\paragraph{Workload Semantics}
job type, priority, input data/locality,

\paragraph{Deployment Scenarios (VO/multiuser)}
\panda has been initially deployed as an HTC-oriented multi-user WMS system for
ATLAS, consisting of ~100 heterogeneous computing
sites~\cite{maeno_pd2p:_2012}.
Recent improvements to \panda have been designed to extend the range of
deployment scenarios to non-ATLAS infrastructures making \panda a general-use
\pilotjob~\cite{nilsson2012recent}.
%% \aznote{Per-user/per-working group reference made in slides, follow-up
%% with actual papers}
When the pilot is launched, it collects information about the worker node and
sends it to the job dispatcher.
If a matching job does not exist, the pilot will end. If a job does exist, the
pilot will fork a separate thread for the job and start monitoring its
execution.
The DDM(?) handles data transfer, must have database access, file stage-in/out
and takes care of cleanup.

\paragraph{Resource Landscape (e.g. grid/cloud/hpc/etc)}
\panda began as a specialized \pilotjob for the ATLAS grid, and has been
extended into a generalized \pilotjob which is capable of working across other
grids as well as HPC and cloud resources.
Cloud capabilities have been used as part of Helix Nebula (CloudSigma,
T-Systems, ATOS), the FutureGrid and Synnefo clouds, and the commercial Google
and EC2 cloud offerings as well~\cite{pandapresentation2013-06}, extending the
reach of \panda to cloud resources as well.
Future \panda developments seek to interoperate with HPC
resources~\cite{pandapresentation2013-06}.

\paragraph{Interoperability}
\panda traditionally relies on Condor (and SGE?) for its interaction with DCIs.
For recent endeavours in HPC they are using SAGA to interface with the queuing
systems.

\paragraph{Architecture \& Interface}
\panda's \textit{manager} is called a \texttt{\panda server}, and matches
jobs with \pilots in addition to handling data management.
\panda's \textit{\pilot}
is called, appropriately enough, a \texttt{pilot}, and handles the execution
environment.  These pilots are generated via \panda's \texttt{PilotFactory},
which also monitors the status of pilots.
Pilot-resource \textit{provisioning} is handled by \panda itself and is not
user-controllable, whereas job-to-resource \textit{binding} is handled
manually by users.
A central job queue allows users to submit jobs
to distributed resources in a uniform manner.
%%  \aznote{Not required for \pilotjob -- put later or remove?}
This basic functionality (pilot creation and management) provides \panda with
all of the baseline capabilities required for a \pilotjob.

As \panda is an \textit{Advanced \pilotjob System}, it enables functionality
beyond that of a \textit{Basic \pilotjob}.
\panda contains some additional backend features such as \texttt{AutoPilot},
which tracks site statuses via a database, \texttt{Bamboo} which adds ATLAS
database interfacing, and automatic error handling and recovery.
\panda contains support for queues in clouds, including EC2, Helix Nebula, and
FutureGrid~\cite{pandapresentation2013-06}.
%% \aznote{Have to find a good cite for this}
Enhancements to the userspace include \texttt{Monitor}, for web-based
monitoring, and the \texttt{\panda client}.  \texttt{\panda Dynamic Data
Placement} \cite{maeno_pd2p:_2012} allows for additional, automatic data
management by replicating popular or backlogged input data to underutilized
resources for later computations and enables jobs to be placed where the data
already exists.

\paragraph{Robustness}
The wrapper of the \pilot takes care of disk space monitoring, and enables job
recovery of failed jobs and restarting from crash whenever possible.

\paragraph{Security}
\panda on HTC grids mainly depends on GSI Certificate-based security and glExec
to run as given user.

%% \textbf{job, task, resource, infrastructure, scheduling, pilot, pool, manager,
%%   agent, Pilot-Job, placeholder, multi-level scheduling, binding, early/late
%%   binding}

%% \begin{itemize}
%% \item Based upon Condor-G/Glidein, uses multiple queues
%% \item Jobs submitted via Condor-G
%% \item Jobs run with wrappers, on worker nodes they download/execute PanDA pilot code
%% \item Pilot then asks for job that worker node can handle
%% \item Stages in input files; executes and monitors progress of job; stages out output;
%%   cleans up environment
%% \item ``Special Features''
%%   \begin{itemize}
%%   \item Job/disk monitoring
%%   \item Job recovery (leaves output files resident for another
%%     process to resume from)
%%   \item Multi-job processing (submit batch of jobs to a single pilot)
%%   \item Certificate/token-based security
%%   \item gLExec security (user's identity used instead of Pilot credentials)
%%   \end{itemize}

%% \item PanDA server (task buffer -- job queue manager keeps track of
%%   active jobs, brokerage -- matches jobs  with sites/pilots and manages
%%   data placement, job dispatcher -- receives requests from pilots and sends
%%   jobs payloads, data service -- dispatch and retrieval from sites)
%% \item PanDA DB (general DB = advert?)
%% \item PanDA client (user client for submission/etc)
%% \item Pilot (execution environment)
%% \item AutoPilot (pilot submission, management, monitoring), submits
%%   pilots to remote sites and tracks site status via database
%% \item SchedConfig (configuration database)
%% \item Monitor (Web-based monitoring)
%% \item Logger (logs incidents w/ Python logging)
%% \item Bamboo (interface w/ ATLAS database)
%% \item PilotFactory -- generates pilots (sends schedd glideins to remote sites)
%%   and monitors pilot status, basically enables use of glideins with PanDA (?)
%% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% RADICAL-Pilot
%
\subsubsection{RADICAL-Pilot}
\label{sec:radicalpilot}

The authors of this paper (the RADICAL group) have been engaged in theoretical
and practical aspects of \pilot systems for the past several years.
In addition to formulating the P* Model\cite{} which by most accounts is the
first complete conceptual model of \pilots, the RADICAL group is responsible
for the development and maintainance of RADICAL-Pilot\cite{}.
RADICAL-Pilot is the groups long-term effort for creating a production level
\pilot system.
The effort is build upon the experience gained from developing, maintaining and
using BigJob\cite{}, a more prototype-style \pilot system.

\paragraph{Pilot Resource Capabilities}

RADICAL-Pilot is mainly tailored towards HPC environments such as the resources
of XSEDE and NERSC\cite{}.
The primary \vocab{resource} that is exposed is the compute node, or more
specically a a core within such a node.

\paragraph{Resource Interaction}

RADICAL-Pilot uses the Simple API for Grid Applications
(SAGA)~\cite{ogf-gfd-90, sagastuff} in order to interface with different DCIs.
Through means of SAGA, RADICAL-Pilot submits the Pilot Agent as a \vocab{job}
through the reservation or queuing system of the DCI.
Once the \impterm{Agent} is started there is no direct \prop{resource
interaction} with the DCI anymore except for monitoring of the state of the
\impterm{Agent} process.
Using SAGA has enabled RADICAL-Pilot to expand to many of the changing and
evolving architectures and middlewares.

\paragraph{Overlay Management}

The programming interface of RADICAL-Pilot enables (and requires) the explicit
configuration of the \vocab{overlay}.
The user is expected to specify the size, type and destination of the pilot(s)
he wants to add to the \vocab{overlay}.
Through the concept of \impterm{UnitManagers} on the client side, the user can
group \pilots together and foster them under a single scheduler or have
multiple independent \impterm{UnitManagers} and \pilots, all with their own
\impterm{scheduler}.
On HPC style systems there is typically one \pilot that manages all the
resources that are allocated to that specific run, but there can be as many
\pilots per resource as the policies allows concurrently running jobs.

\paragraph{Workload Semantics}

The \vocab{workload} within RADICAL-Pilot consists of \impterm{ComputeUnits}
that represent \vocab{tasks}.
From the perspective of RADICAL-Pilot there are no dependencies between these
\impterm{ComputeUnits} and once a \impterm{ComputeUnit} is given to the control
of RADICAL-Pilot it is assumed to be ready to be executed.
RADICAL-Pilot includes the concept of \impterm{kernel abstractions}.
These are generic application descriptions that can be configured on a
per-source basis.
Once an \impterm{application kernel} is configured for a specific resource and
available in the repository, the user only needs to refer to the \impterm{application kernel}.
RADICAL-Pilot will then take care of setting up the right environment and
executing the right executable.
This facility is especially useful in the case of \vocab{late-binding}, when it
is at the time of submission unknown on which \pilot (and thereby resource) a
task will run.

\paragraph{Task Binding Characteristics}

\vocab{Task} to \vocab{resource} \vocab{binding} can be done either implicitly
or explicitly.
A user can bind a task explicitly to a pilot, and thereby to the resource the
pilot is scheduled to, making this a form of early binding.
The user can also submit a task to a unit manager that schedules units to
multiple pilots.
In this case it is the semantics of the scheduler that decides on the task
binding.
RADICAL-Pilot supports multiple schedulers that can be selected at runtime.
There currently exist two schedulers:
1) a round-robin scheduler that binds incoming tasks to associated pilots in a
rotating fashion, irrespecive of their state, and thereby performing
\vocab{early binding}.
2) a BackFilling scheduler that binds tasks to pilots once the pilot is active
and has available resources, thereby making it a \vocab{late binding}
scheduler.

\paragraph{Task Execution Modes}

RADICAL-Pilot supports two type of \vocab{tasks}.
One are the generic single node tasks, that can be single core, or multi-core
threaded/OpenMP applications.
In addition RADICAL-Pilot has extensive support for MPI applications, where it
supports a large variaty of launch methods that are required to succesfully run
MPI applications on a wide range of HPC systems.
The \impterm{launch methods} are a modular system and new \impterm{launch
methods} can be added once they are required.

\paragraph{Architecture}

From a high level perspective RADICAL-Pilot consist of two parts.
A client side python module that is programmable through an API and is used by
scripts / applications and the agent (or multiple agents) that runs on a
resource and executes tasks.

One of the primary features of RADICAL-Pilot is that it lives completely in
user-space and thereby requires no collaboration from the resource owner /
administrator.
Other than the an online database that maintains state during the lifetime of a
session, there are no other (persistent) service components that RADICAL-Pilot
relies on.

\paragraph{Coordination and Communication}

The bridge between these the client side and agent is MongoDB, a
document-oriented database that needs to run in a location that is accessible
for both type of components.
The client side publishes the \vocab{workload} in the MongoDB which is picked
up and executed by the agent.
The agent on its turn publishes \impterm{ComputeUnit} status and results back
to the MongoDB which can be retrieved by the client side.
The main advantage is that this model works in situations where there is no
direct communication channel between the client host and the compute resource,
which is a very common scenario.
The MongoDB database also offers (some) persistency which allows the client
side to re-connect to active sessions stored in the database.
The main drawback of the use of a database for communication is that all
communication is by definition indirect with potential performance bottlenecks
as a consequence.

\paragraph{Interface}

The client side RADICAL-Pilot is a Python library that is programmable through
the so called \impterm{Pilot-API}.
The application-level programmability that RADICAL-Pilot offers was
incorporated as a means of giving the end-user more flexibility and control
over their job management.
Users define the their pilots and their tasks and submit them through the
\impterm{Unit Manager}.
They can query the state of pilots and units or rely on a \impterm{callback}
menanism to get notified of any state change that they are interested in.
The users define their pilots and tasks in a declaritive way, but the flow of
activity is more of an imperative style.

\paragraph{Interoperability}

RADICAL-Pilot allows for \prop{interoperability} with multiples types of
resources (heterogenous schedulers and clouds/grids/clusters) at one time.
It does not have \prop{interoperability} across different \pilot systems.

\paragraph{Multitenancy}

RADICAL-Pilot is installable by a user onto the resource of his or her choice.
It is capable of running only in ``single user'' mode; that is, a \pilot
belongs to the user who spawned it and cannot be accessed by other users.

\paragraph{Robustness}

For fault-tolerance RADICAL-Pilot relies on the user to write the application
logic to achieve \prop{Robustness}, to assist in that process, it does report
task failures.

\paragraph{Security}
The compute resource \prop{Security} aspects of RADICAL-Pilot are that it
follows security measures of the respective scheduler systems (i.e. policies
like allocations, etc).

\paragraph{Files and Data}

RADICAL-Pilot supports many modes of \impterm{data staging} although all of
them are exclusively file based.
It supports the staging of files on both \pilot and \impterm{ComputeUnit} level.
Files can be transferred to and from the staging area of the pilot and the
compute unit \impterm{sandbox} from and to the client machine.
It also allows the transfer of files from third party location onto the compute
resource.
In addition the ComputeUnits can be instructed to use files from the pilot
sandbox turns the pilot sandbox into a shared file storage for all its compute
units.
The user has the option to Move, Copy or Link data based on the performance and
usage criteria.

\paragraph{Performance and Scalability}

RADICAL-Pilot is one of the largest pilot based consumers of resources on
XSEDE\cite{}.
Per Pilot Agent many thousands of concurrent ComputeUnits can be managed.
As multiple pilots can easily be aggregated over multiple distinct DCIs,
RADICAL-Pilot can achieve scalability in many dimensions.

\paragraph{Development Model}

RADICAL-Pilot is an Open Source project released under the MIT license.
Its development takes place on Github.
As of writing the project is under active funding and development and has a
number of external projects that use it as a foundation layer for job
execution.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section 4's concluding remarks
%
\subsection{Overall observations}

\msnote{Maybe this subsection should go and address it in the discussion and
conclusion}

Some of systems we have discussed, like DIRAC, X, are inherently service
oriented, or exposed as a service.
On the other side of the spectrum, systems like DIANE and RADICAL-Pilot take
the form of a library or a framework.
From a formal standpoint, services can be abstracted by a library and a library
can be abstracted by a service, so the main distinction between these
architectures if the default form they come in and how they are meant to be
deployed.
% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------
% SECTION 5
% -----------------------------------------------------------------------------
\section{Discussion and Conclusion}
\label{sec:5}

% \jhanote{the structure of section 5 is: (i) revisit the myths. (ii)
% state clearly how section 3 and 4 help us define what a pilot is,
% the necessary and sufficient conditions (if possible), discuss the
% classifiers and apply/discuss them to the set of pilot systems here.
% (iii) then we go on to say motivate P*/pilot for data, (iv) discuss
% implications for WF systems and conclude with a summary for
% tools/sustainability/etc. At some point, we discuss how/why pilots
% are more than just pilots, eg can be RM layer for middleware,
% runtime framework etc}\mtnote{I have organized the following items on
% the lines you have traced, with some changes in the order and number
% of the items.}

\mtnote{In Section 3 we use pilotjob. We will have to move to \pilot
or decide to use the less correct pilotjob here too.}\mtnote{Following
discussion, we decided to use \pilot. Section 3 has been updated, the
other Sections will have to follow.}

Sections~\ref{sec:3} and~\ref{sec:analysis} offered respectively a description
of the minimal capabilities and properties of a \pilot system; a
vocabulary defining `pilot' and its cognate concepts; a classification
of the core and auxiliary properties of a paradigmatic pilot system
implementation; and the analysis of an exemplar set of pilot system
implementations. Considered altogether, these contributions show how the
notion of \pilot system indicates a paradigm for the execution of tasks
on distributed resources by means of resource placeholders.

In this Section we critically assess the properties of the \pilot
paradigm. The goal is to show the generality of this paradigm and how
\pilot systems go beyond the implementation of a special purpose
trickery to speed up the execution of a certain type of workload. Once
understood the breath of the \pilot paradigm, we contextualize it
by describing its relation with relevant domains such as middleware,
applications, security, and enterprise. Finally, we close the Section
with a look into the future of \pilot systems moving from the current
state of the art and discussing the sociotechnical and engineering
challenges that are being faced both by developers and target users.

\subsection{The Pilot Paradigm}
\label{sec:5.1}

The \pilot paradigm identifies a type of software system with both
general and unique characteristics. This paradigm is general because, in
principle, it does not depend on a single type of workload, a specific
infrastructure, or a unique performance metric. Systems implementing the
\pilot paradigm can execute workloads composed by any number of
tasks with disparate requirements. For example, as seen
in~\S\ref{sec:analysis}, different \pilot systems can execute homogeneous or
heterogeneous bags of independent or intercommunicating tasks with
arbitrary duration, data or computation requirements.

The same generality applies to the type of resource on which a \pilot
system can execute given workloads. As seen in~\S\ref{sec:3}, the
\pilot paradigm demands resource placeholders but does not specify the
type of resource container that should be exposed by the target
resource. HPC, Grid, or Cloud resources all expose type of containers
that are currently leveraged by \pilot systems to create resource
placeholders.

Finally, while usually \pilot systems are designed to maximize the
throughput of task execution, the \pilot paradigm is independent from
any performance metric. Depending on the given workload and the target
resource, a \pilot system implementing the \pilot paradigm can be design
to maximize or minimize a specific element of task execution.

The \pilot paradigm is unique because of three distinctive
characteristics: resource placeholding, multi-level scheduling, and
early or late binding. A \pilot system does not offer just an
alternative implementation of a scheduler, of a workload manager, of a
batch system, or of any other special-purpose software component. A
\pilot system implements a specific patterns of workload execution based
on multi-level scheduling and distributed task execution. Every software
system with these three characteristics, built upon the logical
components and the functionalities described in~\S\ref{sec:compsandfuncs} is an
implementation of a \pilot system as seen in Section~\ref{sec:analysis}.

% \mtnote{we use resource overlay in 3.1 but we should probably use
% placeholding seeing how we use overlays these days in OWMS} \mtnote{This
% has been fixed in S3.}

\jhanote{Need thinking: Following on from discussion yesterday, is
early/late binding a logical consequence of multi-level scheduling? Can
we absorb the latter into the former?}\mtnote{Following the revision of
S3, early/late binding seems to be related to having resource
placeholder while multi-level scheduling seems to be the means to have
place-holders. Both early and late binding refers to the same entities:
tasks bound to pilots, i.e. place holders. Multi-level scheduling refers
to two pairs of entities: container (i.e. jobs) scheduled to DCIs, and
tasks scheduled to placeholders. This opens the issue of whether binding
is a form of scheduling, whether binding and scheduling are the same. We
may want to discuss about it.}

\mtnote{The following paragraphs address the following: PJs have well
defined semantics and model. PJs dont help with data intensive
applications.}

The scope of the \pilot paradigm encompasses multiple types of resource.
Usually, the tasks executed on pilots leverage mainly the cores of the
compute nodes for which the pilot is a placeholder. Tasks may also use
data or network resources but mostly as accessories to their execution
on computational resources. Local or shared filesystems may be used to
read input and write output data while network connectivity may be
required to download, for example, the code of the task executor. In
this context, it is important to notice that the focus on computational
resources is not mandated by the \pilot paradigm. In principle, pilots
can be placeholders also and exclusively for data or network resources
depending on the capabilities exposed by the middleware of the target
DCIs. For example, in Ref.~[pilot data], the notion of \pilotdata has
been conceived using the power of symmetry, i.e., \pilotdata taken to be
a notion as fundamental to dynamic data placement and scheduling as
\pilot is to computational tasks.

The generality of the \pilot paradigm has been modeled in
Ref.~\cite{Luckow:2008la}. This investigation was motivated by the
desire to provide a single conceptual framework -- the so called $P*$
Model -- that would be used to subsume the design characteristics of the
various \pilot system implementations, a sample of which have been
analyzed in detail in Section~\ref{sec:analysis}. \mtnote{Is this still the
case? I think this has become one of the goals of this paper. How do you
want to mention P* here? What do you want to say about it? Is the
paragraph above sufficient?} Interestingly, the \pstar model was
amenable and easily extensible to \pilotdata.  The consistent and
symmetrical treatment of data and compute in the model led to the
generalization of the model as the {\it P* Model of Pilot Abstractions}.

\mtnote{The following paragraphs address the following: (i) PJs are just
for HTC and they just circumvent job queuing delays; PJs unfairly game
HPC queuing system; (ii) PJs have to be tied to specific infrastructure
and infrastructures have to be tied to specific pjs; (iii) PJs are such
a simple concept, it doesn't need more attention: Conversely, everyone
should write their own PJ just because they can; Note: the second part
of this statement is better addressed when discussing the fragmentation
of the pilot landscape. (iv) PJs are stand-alone tools passive (system)
tools, as opposed to user-space, active and extensible components of a
CI;}

The generality of the \pilot paradigm may come as a surprise when
considering the most common requirement that motivates its
implementations. Leveraging multi-level scheduling so to increase the
execution throughput of large workloads made of short running tasks has
been achieved without devising a new paradigm. For example, as seen
in~\S\ref{sec:analysis} \panda, Falkon, or DIRAC were initially developed as
single-point solutions, focusing on either a type of workload, a
specific infrastructure, or the optimization of a single performance
metric. Appreciating the properties of the \pilot paradigm becomes
necessary once requirements of infrastructure interoperability, support
from multiple types of workloads, or flexibility in the optimization of
execution are introduced. Satisfying those requirements requires
abstracting the specificity of middlewares, infrastructure
architectures, and application patterns. As shown in this paper, this
process of abstraction means to develop an understanding of the \pilot
paradigm.

Appreciating the \pilot paradigm means also to understand that its
implementations are not circumventing the infrastructure middleware but
simply abstracting away some of its properties in order to optimize one
or more user-defined performance metric. Multi-level scheduling is not
replacing the infrastructure-level scheduler. As clearly illustrated
in~\S\ref{sec:3}, resource containers are still created on the target
infrastructure by means of that infrastructure capabilities and the
second level of scheduling is completely contained within the boundaries
of the resource container. Optimizing the usage of the resource
containers, for example by minimizing idling time, is an implementation
issue, not an intrinsic limitation of the \pilot paradigm. Furthermore,
it should be noted that shifting the control over tasks scheduling away
from the infrastructure middleware does not imply that the end-user will
become necessary responsible for an efficient resource utilization. A
\pilot system classify as a middleware component and, as seen with
HTCondor, it can be implemented as an integral part of the
infrastructure middlware stack.

% The \pilot paradigm... Why it is a independent distributed system
% abstraction? Independent = unique characteristics and properties when
% compared to other distributed system paradigms. What `other' paradigms
% should we consider? In what sense the pilot paradigm is distributed?
% Why should we define the \pilot paradigm an abstraction?

\subsection{Pilot Paradigm in Context}
\label{sec:context}

\paragraph*{HTC vs HPC} We have identified that the Pilot Paradigm in itself is
very common and on an abstract level applies to all \pilot systems we have
discussed. This is also illustrated by the mapping to the \pstar model.
While arguably not a fundamental difference in the application of \pilots,
there certainly are structural practical differences in using \pilots on HPC vs
HTC.
While in the \textit{temporal} dimension the ratio of \vocab{Tasks} and \pilots might
be similar for both type of infrastructures (i.e. \pilots run for 12 hours and
tasks for 1 hour), the \textit{spatial} dimension is a different story.
Let's define HPC as O(10) sites with a tightly coupled system.
These systems typically have a head/login/service node where the \pilot will
operate from.
For one site, the ratio of \pilots vs compute resources will typically be
O(1):O(Nodes) and therefore the ratio of \pilots vs \vocab{Tasks} will be
O(1):O(Tasks).
With the given O(10) sites this leaves us with a \pilot:\vocab{Tasks} ratio of at most
O(10):O(Tasks)
On the other hand, if we define HTC as O(100) sites of lousely coupled systems,
without a central node to foster these resources, the ratio of \pilots to
compute resources will typically be closer to O(1):O(Cores).
These differences have not only implications on the type of applications that
can be run, but also on the nature of the multi-level scheduling. In the HPC
case there is a large degree of freedom for the \pilot layer scheduling while
in HTC the scheduling on that level is much more pre-determined.

\paragraph*{MPI, OpenMP, etc.} In the \pstar model and in the discussion so far
we have mainly been concerned about the abstract notion of \vocab{Task}, i.e.
the application payload to be run on the DCI.
While this abstraction is very natural it does pose challenges in practice when
we are operating in a real-life heterogeneous environment, i.e. what does
``run'' mean \ldots. And if we are able to define what it means, how do we pass
that information down the stack?

\paragraph*{Generality or optimisation} The existence of many \pilot systems
is tempting to frown at from a perspective of Not-Invented-Here-Syndrome.
Workload management. [this has a clear link to the pilot and application layer
paragraph]

\paragraph*{Pilot and Middleware Layer} The definition of the term
`pilot' and the critical review of its implementations highlight the
relation between \pilot systems and middleware as commonly defined in
[cit, cit, cit]. The notion of pilot as a resource placeholder is
something more than a job as defined in the context of Grid- based
middleware. For example, cloud-based DCIs \mtnote{Use of DCIs for plural
needs to be consistent across the paper.} introduce notable exceptions
and differences in the way in which pilots can be provisioned.[cit, cit]
Within a IaaS [cit], Virtual Machines (VMs) and not jobs are used for
their provisioning. VMs can often be instantiated without waiting into a
queue  with a predictable instantiation time, usually minimal when
compared to the queuing time often experinced on Grid-based DCIs [ref
Vishal paper]. Furthermore, the limitations on the execution time of a
VM can be virtually absent, a limitation that on grid-based middleware
is imposed by the queue walltime. Clearly, overheads are introduced by
having to deal with VMs and not simple jobs and the model adopted within
a IaaS-based DCI to assign resources to each VM can affect the
flexibility of the whole \pilotjob system. [cit] A similar assessment
could be done for a DCI deploying a PaaS model of cloud computing.[cit]

\paragraph*{Pilot and Application Layer} The current state of workflow
(WF) systems~\cite{nsf-workflow,1196459} provides a motivating example
for the P* Model and the Pilot-API: even though many WF systems exist
(with significant duplicated effort), they provide limited means for
extensibility and interoperability.  We are not naive enough to suggest
a single reason, but assert that one important contributing fact is the
lack of the right interface abstractions upon which to construct
workflow systems; had those been available, many/most WF engines would
have likely utilized them (or parts thereof), instead of proprietary
solutions. Significant effort has been invested towards WF
interoperability at different levels -- if nothing else, providing
post-facto justification of its importance. The impact of missing
interface abstractions on the WF world can be seen through the
consequences of their absence: WF interoperability remains difficult if
not infeasible. The Pilot-API in conjunction with the P* Model aims to
prevent similar situation for \pilotjobs.\mtnote{To be abstracted from
workflow to application.}

\paragraph*{Pilot and AAA} Because the \pilot can run as a different ``user''
than the ``owner'' of the \vocab{Task} there are challenges for many aspects of
Authentication, Authorization and Accounting (AAA).
The issue mainly arrises because of the fact that multi-level scheduling
decouples the \vocab{DCI} user from the \vocab{Task} owner.
While technically one could make the choice not to be concerned about this
decoupling and only be concerned with the DCI credential of the Pilot, in
practice this does not hold true.
[ref glexec].

\paragraph*{Pilot and the Enterprise} Over the past years,
Hadoop~\cite{hadoop} emerged as distributed computing for data-intensive
tasks in the enterprise space. While early versions of Hadoop were very
monolithic tightly coupling the Map Reduce programming framework with
the underlying infrastructure resource management. With Hadoop~2,
YARN~\cite{yarn-paper} was introduced as a central resource manager
supporting heterogenous workloads (and not only Map Reduce). YARN
provides support multi-level scheduling enabling the application to
deploy their own application-level scheduling routines on top of
Hadoop-managed storage and compute resources. While YARN manages the
lower resources, the higher-level runtimes typically use an
application-level scheduler to optimize resource usage for the
application. Applications need to initialize their so-called
Application-Master via YARN; the Application Master is then responsible
for allocating resources in form of so-called containers for the
applications. It then can execute tasks in these containers.

As in the HPC environment, the support for different application
workloads and job types: long-lived vs. short-lived applications,
homogeneous vs. heterogeneous tasks is a challenge. \pilots address many
of these requirements in HPC and HTC environments. Similar frameworks
are emerging for YARN: Llama~\cite{llama} offers a long-running
application master for YARN application designed for the Impala SQL
engine. TEZ~\cite{tez} is a DAG processing engine primarily designed to
support the Hive SQL engine allowing the application to hold containers
across multiple phases of the DAG execution without the need to
de-/re-allocate resources. REEF~\cite{Chun:2013:RRE:2536274.2536318} is
a similar runtime environment that provides applications a higher-level
abstractions to YARN resources allowing it to retain memory and cores
supporting heterogeneous workloads.

\subsection{Future Directions and Challenges}
\label{sec:5.3}

\paragraph*{Sociotechnical} \jhanote{What is the future of PJ?};
\jhanote{\pilotjobs have potential, but it is not being realized due to
ad hoc nature of theory and practise,}; Fragmentation/balkanization of
the pilot landscape.
While in this paper we also talk about HTC and HPC and their differences and
commonalities, we have refered to an existing and growing demand for
convergence between the two from an application perspective.
[ref MTC?]
HPC users are no longer exclusively interested in running one large job, but
applications become more versatile and dynamic and are conflicting with the
conventional thougths and operational realities on how to ``use'' HPC
resources. [ref cray, cram, etc.]
If the HPC world is going to accept this mode of operation, then recognizing,
embracing and investing in the \pilot Paradigm is inevatable.

\paragraph*{Engineering} \jhanote{Why should we do to enhance the
usability?}; \aznote{I would argue that our work is important to
\pilotjobs because by expressing a common model, we enable researchers
to 1) understand the commonalities between existing \pilotjob approaches
in order to 2) motivate innovation + construction of ``next-gen''
\pilotjob systems. E.G., implement the basics (or work from an existing
system) + understand where the boundaries/unexplored territory is
without having to first completely understand all 15+ existing \pilotjob
systems and their unique vocabulary.}

While we are in no way to argue for a one-size-fits-all \pilot we do believe
that our work has shown that there is enough common ground that establishing a
new recognized layer in the stack would not only help conceptually, but would
also pay-off from a practical engineering standpoint.
There are many intricate details for example in the runtime environment, that
sharing efforts on that layer would allow the various systems to focus their
attention where there is less overlap and more tailored to their own
communities. [ref Open RunTime Environment] Standardisation?

\paragraph*{Contributions} \jhanote{(i) we provide first comprehensive
historical and technical analysis, (ii) set the stage for a common
conceptual model and implementation framework, and (iii) provide insight
and lessons for other tools and higher-level frameworks, such as
Workflow systems possibly}

%-------------------------------------------------------------------------------
% \item \textbf{Pilot abstraction} as a well-defined, independent
% distributed system abstraction. \jhanote{state clearly how section 3
% and 4 help us define what a pilot is, the necessary and sufficient
% conditions (if possible), discuss the classifiers and apply/discuss
% them to the set of pilot systems here.} ``pilot-abstractions works!''
% , p* as a model ok. Our initial investigation~\cite{Luckow:2008la}
% into \pilot- Abstractions was motivated by the desire to provide a
% single conceptual framework --- referred to as the P* Model, that
% would be used to understand and reason the plethora and myriad
% \pilotjob implementations that exist.

% \item \textbf{Generality of the pilot abstraction} when compared to
% different types of resources. Is pilot useful only for compute
% resources? What about pilot data? Pilot network? Other types but
% compute, data, network?

% \item \textbf{Revisit the myths about pilot abstraction}: how they
% evolve from being a simple hack to get around queuing/scheduling
% policies to become a first class citizen of the abstractions for
% distrubuted systems. What are the characteristics/properties that make
% the pilot abstract a first class citizen? Do we notice a progressive
% evolution in the multiple implementations we introduced and described
% in Section 4?

% \item \textbf{Relationship between pilot abstraction and different
% types of middleware}: grids, cloud, HPC.

% \item \textbf{Pilot abstraction and `lessons for the workflow
% systems'}.

% \mtnote{Do we want to put this at the end of S4, in the analysis that
% we will have to write in 4.3?}

% \section*{Notes - to be commented out by their authors}

% \jhanote{The question is what are the fundamental ``concepts''. It is
%   not necessary that the concepts have a specific implementation or
%   map to a component in \pilotjob system. As we know there are
%   different ways in which tasks get committed to the \pilotjob
%   system. One possible primary concept is that of logical grouping;
%   all tasks are committed to a logical grouping -- where the grouping
%   is such that all entities in this group will be executed
%   (disregrarding details of how this grouping will happen, or who will
%   perform the execution).  It appears that the concept of logical
%   grouping of tasks is a fundamental one, and avoids a requirement of
%   any further specification of of details of who/where/when; if so,
%   then the notion of a pool can be dispensed with, which will have the
%   advantage of liberating us from the requirement of imposing on the
%   the manager the need to push/pulling tasks from a pool.}

% % \mtnote{The paragraph relative to this comment is gone --- no more
% % pool concept. I do like the idea of grouping though so I would suggest
% % to wait for the entire Section to stabilize further and then see
% % whether we can add/extend a paragraph by introducing the concept of
% % grouping.}

% \jhanote{AppLeS is not strictly \pilotjob based?  but \pilotjob like
%   capabilities?} \alnote{The question is: when is a \pilot a \pilot?
%   When the use the term \pilot and when \pilot-like? Apples has a
%   component -- the Actuator (Quote from paper: "... handles task
%   launching, polling, and cancellation ..."), which is quite similar
%   to a \pilot. But maybe AppLeS is something for the history section
%   or a separate category for Master/Worker frameworks...  PJ evolved
%   from the need to map master/worker style computations to
%   heterogeneous, dynamic distributed grid environments. Added a
%   pre-\pilot category to the history sub-section.}

% \jhanote{I think all the functionality of PJs is predicated on the
%   following core capability: enable the decoupling between assigning a
%   workload from the spatial/temporal execution properties of its
%   execution. In condor, this is mentioned as ``separating between job
%   scheduling and resource''. This get mentioned somewhere in core
%   functionality?}

% \jhanote{deployment and provisioning are not the same in my
%   opinion: provisioning is about resources, i.e., provisioning is not
%   the same as scheduling, it is more like arranging. Deployment is
%   about ``setting up T=0 requirements, which could be software
%   environment, input/data dependencies, as well as resources.''}

% \aznote{Some additional thoughts for this section... 1) The early
% history of \pilotjobs seems to have a few ``independent'' \pilotjob
% approaches, where systems (eg MyCluster/AppLeS) incorporate \pilotjob
% techniques with sometimes wildly differing implementations/vocabulary
% but when boiled down, offering the same attempts at base/minimal
% functionality.  This helps push the value of the approach (multiple
% groups working on it independently helps imply its value), with
% increasingly complex \pilotjob systems coming out as time pushes on.
% Our review aids both in the construction of these increasingly complex
% systems by exposing their core functionality + allowing researchers/etc
% to push the field forward by reducing the apparent complexity to a set
% of common terms/comparisons/classifiers/etc.  I don't know if this is
% too prescriptive/hand-wavey -- feel free to ignore if so, but I feel
% that the essence of this is one potential contribution of our work.}

\section*{Acknowledgements}
{\footnotesize{This work is funded by the Department of Energy Award
    (ASCR) DE-FG02-12ER26115 and NSF CAREER ACI-1253644. This work has
    also been made possible thanks to computer resources provided by
    TeraGrid TRAC award TG-MCB090174 and BiG Grid.  This document was
    developed with support from the US NSF under Grant No. 0910812 to
    Indiana University for ``FutureGrid: An Experimental,
    High-Performance Grid Test-bed''.}}

% \bibliographystyle{IEEEtran}
\bibliographystyle{abbrv}
\bibliography{pilotjob,literatur,saga,saga-related,urls,hadoop}


\end{document}
