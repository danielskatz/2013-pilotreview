\documentclass{sig-alternate}
%\documentclass[conference]{IEEEtran}
%\documentclass[conference,final]{IEEEtran}

\input{head}
\input{include}
\usepackage{lscape}

\usepackage{listings}

\lstnewenvironment{code}[1][]%
{
\noindent
%\minipage{0.98 \linewidth}
\minipage{1.0 \linewidth}
\vspace{0.5\baselineskip}
\lstset{
    language=Python,
%    numbers=left,
%    numbersep=4pt,
    frame=single,
    captionpos=b,
    stringstyle=\ttfamily,
    basicstyle=\scriptsize\ttfamily,
    showstringspaces=false,#1}
}
{\endminipage}

\begin{document}
%\conferenceinfo{HPDC'13}{2013, New York, USA}
% \conferenceinfo{ECMLS'11,} {June 8, 2011, San Jose, California, USA.}
\CopyrightYear{2015}
% \crdata{978-1-4503-0702-4/11/06}
% \clubpenalty=10000
% \widowpenalty = 10000

\title{A Comprehensive Perspective on Pilot-Abstraction}

% \alignauthor
% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{1932 Wallamaloo Lane}\\
%        \affaddr{Wallamaloo, New Zealand}\\
%        \email{trovato@corporation.com}
% % 2nd. author
% \alignauthor
% G.K.M. Tobin\titlenote{The secretary disavows
% any knowledge of this author's actions.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{P.O. Box 1212}\\
%        \affaddr{Dublin, Ohio 43017-6221}\\
%        \email{webmaster@marysville-ohio.com}
% % 3rd. author
% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
% one who did all the really hard work.}\\
%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%        \affaddr{Hekla, Iceland}\\
%        \email{larst@affiliation.org}
% \and  % use '\and' if you need 'another row' of author names
% % 4th. author
% \alignauthor Lawrence P. Leipuner\\
%        \affaddr{Brookhaven Laboratories}\\
%        \affaddr{Brookhaven National Lab}\\
%        \affaddr{P.O. Box 5000}\\
%        \email{lleipuner@researchlabs.org}
% % 5th. author
% \alignauthor Sean Fogarty\\
%        \affaddr{NASA Ames Research Center}\\
%        \affaddr{Moffett Field}\\
%        \affaddr{California 94035}\\
%        \email{fogartys@amesres.org}
% % 6th. author
% \alignauthor Charles Palmer\\
%        \affaddr{Palmer Research Laboratories}\\
%        \affaddr{8600 Datapoint Drive}\\
%        \affaddr{San Antonio, Texas 78229}\\
%        \email{cpalmer@prl.com}
% }

\date{}
\maketitle

\begin{abstract}
  There is no agreed upon definition of \pilotjobs; however a functional
  attribute of \pilotjobs that is generally agreed upon is they are
  tools/services that support multi-level and/or application-level scheduling by
  providing a scheduling overlay on top of the system-provided schedulers.
  Nearly everything else is either specific to an implementation, open to
  interpretation or not agreed upon. For example, are \pilotjobs part of the
  application space, or part of the services provided by an infrastructure? We
  will see that close-formed answers to questions such as whether \pilotjobs are
  system-level or application-level capabilities are likely to be
  elusive. Hence, this paper does not make an attempt to provide close-formed
  answers, but aims to provide appropriate context, insight and analysis of a
  large number of \pilotjobs, and thereby bring about a hitherto missing
  consilience in the community's appreciation of \pilotjobs.  Specifically this
  paper aims to provide a comprehensive perspective of \pilotjobs. A primary
  motivation for this work stems from our experience when looking for an
  interoperable, extensible and general-purpose \pilotjob; in the process, we
  realized that such a capability did not exist. The situation was however even
  more unsatisfactory: in fact there was no agreed upon definition or conceptual
  framework of \pilotjobs.  To substantiate these points of view, we begin by
  discussing some existing \pilotjobs and the different aspects of these
  \pilotjobs, such as the applications scenarios that they have been used and
  how they have been used. The limited but sufficient sampling highlights the
  variation, and also provides both a motivation and the basis for developing an
  implementation agnostic terminology and vocabulary to understand \pilotjobs;
  Section \S3 attempts to survey the landscape/eco-system of \pilotjobs.  With
  an agreed common framework/vocabulary to discuss and describe \pilotjobs, we
  proceed to analyze the most commonly utilized \pilotjobs and in the process
  provide a comprehensive survey of \pilotjobs, insight into their
  implementations, the infrastructure that they work on, the applications and
  application execution modes they support, and a frank assessment of their
  strengths and limitations.  An inconvenient but important question -- both
  technically and from a sustainability perspective that must be asked: why are
  there so many similar seeming, but partial and slightly differing
  implementations of \pilotjobs, yet with very limited interoperability amongst
  them?  Examining the reasons for this state-of-affairs provides a simple yet
  illustrative case-study to understand the state of the art and science of
  tools, services and middleware development.  Beyond the motivation to
  understand the current landscape of \pilotjobs from both a technical and a
  historical perspective, we believe a survey of \pilotjobs is a useful and
  timely undertaking as it provides interesting insight into understanding
  issues of software sustainability.
  % believe that a survey of \pilotjobs provides and appreciation for
  % the richness of the \pilotjobs landscape.  is
  % not to discuss the \pstar conceptual framework, but That led to
  % the \pstar model.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\jhanote{Building tools and components that have well-defined and
well-characterized behavior, including performance. This leads to descriptive
models of pilot-jobs, which while pervasive in distributed computing, are
conspicuous by their absence in high-performance and data-intensive computing.
By providing a firm theoretical underpinning to pilot-jobs [3], one can provide
a more “programmable” and flexible yet common pilot-job for different types of
distributed infrastructure, and also extend the concept of pilot-jobs to
high-performance and data-intensive computing [4]}


% \jhanote{Generally not good style to begin new subsection immediately
%   after section starting}
% \jhanote{Now develop the following paragraph along the lines of: Why
%   have \pilotjobs been successful?}

% \jhanote{Although pilotjobs have solved/addressed many problems, now
%     develop the problem with \pilotjobs themselves..}

The seamless uptake of distributed infrastructures by scientific applications
has been limited by the lack of pervasive and simple-to-use abstractions at the
development, deployment, and execution level. Of all the abstractions proposed
to support effective distributed resource utilization, a survey of actual usage
suggested that \pilotjob \mtnote{Should we use just `pilot'}\jhanote{I think
you have proposed a graceful transition: from pilotjobs to pilotsystems? If so,
should we stick with pilotjobs here?}is arguably one of the most widely-used
distributed computing abstractions - as measured by the number and types of
applications that use them, as well as the number of production distributed
cyberinfrastructures that support them. \msnote{ref?}

The fundamental reason for the success of the \pilotjob abstraction is that
\pilotjobs facilitate the oterwise challenging mapping of specific tasks onto
explicit heterogeneous and dynamic resource pools. \pilotjobs decouple the
workload specification from the task management improving the efficiency of
task assignment while shielding applications from having to manage tasks across
such resources.  Another concern often addressed by \pilotjobs is fault
tolerance which commonly refers to the ability of the \pilotjob system to
verify the execution environment before executing jobs. The \pilotjob
abstraction is also a promising route to address specific requirements of
distributed scientific applications, such as coupled-execution and
application-level scheduling~\cite{ko-efficient,DBLP:conf/hpdc/KimHMAJ10}.

%   \onote{I think the most important reasons why Pilot Jobs being so
%     popular (and re-invented over and over again) is that they allow
%     the execution of small (i.e., singe / few-core) tasks efficiently
%     on HPC infrastrucutre by massively reducing queueing time. HPC
%     sites (from schedulers to policies) have always been (and still
%     are) discrimatory against this type of workload in favor of the
%     large, tightly-coupled ones. Pilot-Jobs try to counteract. While
%     this is certainly not the main story that we want to tell, this
%     should IMHO still be mentioned. } \jhanote{This is definitely one
%     of the main reasons, but as Melissa pointed out it during RADICAL
%     call, it is by no means the only reason. Need to get the different
%     reasons down here.. then find a nice balance and description}

A variety of PJ frameworks have emerged: Condor-G/ Glide-in~\cite{condor-g},
Swift~\cite{Wilde2011}, DIANE~\cite{Moscicki:908910},
DIRAC~\cite{1742-6596-219-6-062049}, \panda~\cite{1742-6596-219-6-062041},
ToPoS~\cite{topos}, Nimrod/G~\cite{10.1109/HPC.2000.846563},
Falkon~\cite{1362680} and MyCluster~\cite{1652061} to name a few. Although they
are all, for the most part, functionally equivalent -- they support the
decoupling of workload submission from resource assignment -- it is often
impossible to use them interoperably or even just to compare them functionally
or qualitatively. The situation is reminiscent of the proliferation of
functionally similar yet incompatible workflow systems, where in spite of
significant {\it a posteriori} effort on workflow system extensibility and
interoperability, %(thus providing {\it post-facto} justification of its needs)
these objectives remain difficult if not infeasible.

\mtnote{Should we have a paragraph explaining the core contribution offered by
this paper?}\jhanote{yes}

The remainder of this paper is divided into four Sections. \S\ref{sec:history}
offers a critical review of how the concept of \pilotjobs has evolved by
analyzing existing \pilot systems and systems with pilot-like characteristics.
In~\S\ref{sec:understanding}, the hetoregeneity described
in~\S\ref{sec:history} is addressed by deriving the minimal set of capabilities
and properties that has to characterize the design of a \pilot system. A
vocabulary is then defined so that it can be used consistently across different
designs and implmentations of a \pilot system. In~\S\ref{sec:analysis}, the
focus shifts from analysing the design of a \pilot system to  leveraging the
terminology defined in~\S\ref{sec:understanding} to critically reviewing the
characteritics and functionalities of a relevant set of \pilot system
implementations. Finally,~\S\ref{sec:discussion} closes the paper by outlining
the \pilot paradigm and elaborating on how it impacts and relates to other
middleware and the application layer. The generality and breath of the \pilot
paradigm is underlined by showing its adoption by the Enterprise, outside the
boundaries of scientific research.


% -----------------------------------------------------------------------------
% SECTION 2
%
\section{Functional Underpinnings and Evolution of Pilot Abstraction}
\label{sec:history}

% -----------------------------------------------------------------------------
% Version 0.1
% -----------------------------------------------------------------------------

% \subsection{A Functional Approach to Pilot-Jobs}
% Many scientific communities began running into the same issues:

% As distributed systems grew in capacity and capability, they also grew
% in complexity and heterogeneity. For example, many machines
% implemented their own batch queuing systems, and oftentimes these
% systems varied from machine to machine.\msnote{The part after the
% comma is kind of implicit by the part before} The wide use of
% heterogenous resources, resulted in the need for workload management
% across these resources.  In order to harness the power of these
% heterogeneous resources to run jobs, one particular solution proposed
% is that of
% \pilotjobs

% , which have historically been used as a means of solving these
% issues. This gave rise to the the need for job submission management
% via batch queuing systems and middleware access also grew. We briefly
% discuss some specific uses of \pilotjobs below.

% \pilotjobs are most commonly used for the execution of many tasks
% through the use of a container job. They are often measured by their
% throughput, that is, the number of tasks that they can complete per
% second (tps)\msnote{I dont think we use tps further in the paper}, or
% alternatively, by the total number of tasks executed. As such,
% \pilotjobs are used to achieve high-throughput, for example, when
% using genome sequencing techniques \msnote{Arguably genome
% applications are not the most illustrative example of high throughput
% tasks getting benefit out of pilot-jobs} or ensemble-based
% applications. \pilotjobs have also been used for parameter sweeps,
% chained tasks, and loosely-coupled but distinct tasks. %note to self:
% cite these with papers

% Multi-scale simulations have also benefited from the use of
% \pilotjobs. A framework for load balancing via dynamic resource
% allocation for coupled multi-physics (MPI-based) simulations using
% \pilotjobs was demonstrated in Ref.~\cite{ko-efficient}. This was
% achieved by dynamically assigning more processors to jobs with longer
% runtimes, so that these jobs could accomplish their workload in the
% same amount of wall-clock time as those with shorter runtimes. This
% led to an overall reduction of jobs that were waiting to communicate
% via MPI, and an overall reduction of the total simulation runtime.

% \pilotjobs can be used for  simulations with varying numbers of tasks
%  to complete, for example, molecular dynamics simulations requiring
%  task restart. These types of simulations may start with a fixed
%  number of tasks but spawn more tasks in order to continue simulating.
%  \pilotjobs can be utilized for these types of dynamic simulations,
%  because new tasks can be fed to the \pilot at any time within a given
%  runtime. Without \pilotjobs, these simulations would have to be
%  resubmitted to the batch queue and wait for their time to become
%  active again~\cite{luckow2009adaptive}.

% \pilotjobs have also been used to avoid queue wait times for many jobs
% \msnote{I think we just said this above in the high-throughput case}
% as well as harness and utilize different resources (with different
% batch queueing systems) to do \textit{scale-across} simulations. As a
% fault tolerant mechanism, many \pilotjob systems monitor failed jobs
% and have the ability to restart them within the given time frame of
% the \pilotjob's total
% runtime~\cite{1742-6596-219-6-062049,condor-g,nilsson2011atlas}.

% In order to appreciate \pilotjobs, we outline the evolution of
% \pilot-like capabilities ultimately leading to the creation of the
% first actual \pilotjob. We present a brief chronological order of
% \pilotjob-like systems, beginning with simple Master-Worker-based
% applications through advanced workload management systems.

% \subsubsection*{The Evolution of \pilotjobs}\label{sssec:evolution}

% \pilotjobs provide the ability to distribute workload across multiple
% systems and offer an easy way to schedule many jobs at one time. This
% in turn improves the utilization of resources\msnote{why?}, reduces
% the net wait time of a collection of tasks, and also prevents
% saturation of resource batch queuing systems from high-throughput
% simulations where many jobs need to be run at one time\msnote{I dont
% understand the last claim}. While early \pilot-systems solely provided
% this placeholder job mechanism, many of these system evolved to more
% complex workload management systems. As applications began to utilize
% distributed cyberinfrastructure, the workloads grew from small sets of
% short running jobs to many jobs with either short or potentially long
% runtimes. There was a need for more complex management of these
% workloads and additional capabilities for user-level control of the
% tasks that would be executed within the placeholder job. This drove
% the creation of the modern idea of \pilots \msnote{Kind of an ambigous
% statement, like all statements that include the word "modern" :-)}

% \pilotjob systems differ in their focus and architecture. Our
% preliminary survey of existing \pilotjobs helped to identify three
% major layers that these systems exhibit: (i) core \pilotjob
% functionality - this provides the minimally complete set of
% capabilities for a simple \pilotjob, (ii) advanced \pilotjob
% functionality - a system that offers all of (i) plus a more
% sophisticated resource management mechanism, and (iii) higher-level
% \pilot-based frameworks - frameworks utilize \pilots for a specific
% use case, e.\,g.\ workflows or data analytics.

% \onote{These are not necessarily 'layers'. The more I think about it,
% the whole idea of 'layers' doesn't make so much sense if we want to
% distinguish between core and advanced systems / frameworks. I think
% discussing these 'functionality' along \textbf{orthogonal components}
% (that doesn't necessarily build upon each other) would make more
% sense. My (and ALs) comment w.r.t Figure 1 is related to this.}
% \msnote{I'm tempted to at this stage in the paper use a very high
% level figure to simply support the explanation of the pilotjob
% concept.}

% As one can intuit from the above descriptions, existing \pilotjob
% systems maybe overlap and overflow into these different layers, and
% each layer builds upon the previous one. Therefore, we use these
% classification layers only as a means to explain the basic progression
% of a \pilotjob system from simple scheduling reservation mechanisms to
% more complete job management systems. A more semantically-rich
% terminology and classification scheme will be presented in Sections
% \ref{sec:vocab} and \ref{sec:analysis}.\msnote{I think the whole layering
% discussion can go.}


% Figure~\ref{fig:figures_classification} categorized \pilotjob systems
% into three layers: (i) core \pilotjob systems that solely provide a
% simple \pilot capability, (ii) advanced \pilotjob systems that offer
% sophisticated resource management capabilities based on \pilots, and
% (iii) higher level \pilot-based frameworks that utilize \pilots for a
% specific use case, e.\,g.\ workflows or data analytics. One of the
% important aspects of these layers is that they often overlap or have
% evolved from one another. Therefore, it is hard to classify

% \begin{figure}[t]
%	\centering
%		\includegraphics[width=0.45\textwidth]{figures/classification}
%	\caption{Pilot-Job Classification: Different PJ systems focus
%         on different parts of the distributed computing stack: (i)
%          PJ systems that solely provide the \pilot capability, (ii)
%          systems that offer resource management capabilities based on
%          \pilots and (iii) applications, tools and services that
%          utilize \pilots for resource management. \jhanote{we should
%            make the three levels of the diagram consistent with the
%            three categories, ``core PJ'' , ``advanced PJ'' and
%            ``higher-level PJs'' . Also earlier comment about adding
%            ``higher-level pilot-based frameworks'' to ``higher-level
%            frameworks that can use pilot-jobs''.}}  \alnote{mention
%          that these layers are not cleanly separated, add
%          capabilities (outside)/properties (internal) of
%          each layer, what is the overlap between the layers (how they
%          interrelated?, what is the overlap?)?}
%          \onote{IMHO this figure doesn't really help to explain things
%          and is also wrong (see AL's comment above). I think we should
%          get rid of it.  } \msnote{+1}
%	\label{fig:figures_classification}
%\end{figure}

% In this section, we give a brief overview of different \pilotjob
% systems. As grid computing advanced in its size and capabilities, the
% need for job scheduling and time-sharing became more prevalent. Batch
% queuing systems were installed to solve this problem, wherein a login
% node accepted all job submissions and then the queuing system divided
% the work to the worker nodes. The requirements of distributed
% applications, such as efficient load balancing and resource
% utilization across multiple resources, drove the need for user-level
% control of tasks and ease of use of job descriptions for data driven
% applications~\cite{ko-efficient}~\cite{DBLP:conf/hpdc/KimHMAJ10}, but
% the concept of a \pilot was not the first type of application-level
% scheduling introduced.

% -----------------------------------------------------------------------------
% Version 0.2
% -----------------------------------------------------------------------------

% , it was an early mentioning of large scale parallel computing. In
% determining the necessary processing power for weather forecasting, he
% estimated that 64000 human \textit{computers} would be required for solving
% the equations. These computers would all be assigned a part of the globe by a
% central \textit{senior clerk}. The computers would perform their calculations
% and the results would be collected by the clerk. This was in effect a
% Master-Worker pattern.

% \jhanote{Mark: I propose the following: One common use for the M-W scheme is
% to serve as the coordination substrate for PJs. I know its a bit nebulous,
% but it connects the two concepts directly, which is the goal here.}
% \msnote{Interesting contrast. Do we see M/W as a communication pattern to
% implement PJs or do PJs enable the M/W pattern to applicatons?}
% \mtnote{Please see~\S\ref{sec:compsandfuncs}, 10th paragraph: "As see in..."}

% As established in the introduction, \pilotjobs have proven to be an effective
% tool for task-level parallelism. \mtnote{Do we introduce task-level
% parallelism explicitly?} (Note that the current term \textit{\pilotjob} is
% not as old as the concept itself, and the word \pilot for this concept was
% likely introduced around 2004 in the context of the LHC data challenge.
% \alnote{the concept or the name? figure 2 goes back way further than 2004} It's
% first written appearance is in a 2005 LHCb report\cite{lhcb2005}).
% \msnote{find a nice place for this line} One common use for the \pilotjob
% paradigm is to enable the Master-Worker (M-W) scheme \mtnote{We use pattern
% istead of schema in the next paragraph.} and its associated frameworks for
% applications~\cite{Shao:2000:masterslave}.

% When Lewis Fry Richardson in 1922 devised his \textit{Forecast Factory}
% (Figure~\ref{fig:figures_forecast-factory}), it was an early mentioning of
% large scale parallel computing. In determining the necessary processing power
% for weather forecasting, he estimated that 64000 human \textit{computers}
% would be required for solving the equations. These computers would all be
% assigned a part of the globe by a central \textit{senior clerk}. The
% computers would perform their calculations and the results would be collected
% by the clerk. This was in effect a Master-Worker pattern.

% Figure~\ref{fig:timeline} shows the introduction of the discussed systems and
% terms. When available, the date of first mention in a publication or
% otherwise the release date of software implementation is used.

% In the context of present distributed systems, the M-W scheme was initially
% used for farming tasks from a master to a various number of workers, and
% could easily be adapted to run in a platform-independent way across
% potentially heterogeneous resources~\cite{masterworker, Goux00anenabling}.
% M-W based frameworks could respond to dynamically changing resources by
% adapting the number of workers to match the resource
% availability.\mtnote{Should this paragraph be expanded so to include some
% description/example of the referenced distributed systems and capabilities?}

% As distributed computing infrastructures became more popular and available,
% user demand drove the need for efficient shared allocation of heterogeneous
% resources. Leveraging the batch processing concept, first used in the time of
% punchcards [ref], job schedulers were created to accommodate these needs,
% often called ``batch queuing systems''. The adoption of batch queuing meant
% users were expected to submit their tasks to the job scheduler (queue) of
% each cluster and grid system.
% \msnote{From here its an obvious (side)-step to meta scheduling?}

% Often, the type of scheduler on a one machine was different than that of
% another machine. Clearly, there was a need for managing these heterogeneous,
% dynamic grid environments, especially in terms of dynamic scheduling.

% \ldots

% This pattern was implemented with manageable overheads and could easily be
% adapted to run in a platform-independent way across potentially heterogeneous
% resources~\cite{masterworker, Goux00anenabling}. M-W based frameworks could
% respond to dynamically changing resources by adapting the number of workers to
% match the resource availability.\mtnote{To be aggregated in the new version}

% \ldots

% \msnote{\cite{Gehring:1996:mars} looks just as relevant and predates apples:
% "... Note: AppLeS more active than MARS now MARS uses accumulated statistical
% data on previous execution runs of the same application to derive an improved
% task-to-process mapping"}.

% Commented out GHS for now.
%
% The rise of application-level scheduling, as in AppLeS, opened new
% possibilities to Grid environments. The concept of application-level
% scheduling was extended to include long-term performance prediction in
% heterogenous Grid environments via the Grid Harvest Service (GHS)
% system~\cite{ghs}. GHS provides a prediction model that was derived by
% probability analysis and simulation and useful for large-scale applications
% in shared environments. Its prediction models and task scheduling algorithms
% are utilized in the placement of tasks across Grid resources. GHS supports
% three classes of task scheduling: (i) single task, (ii) parallel processing,
% and (iii) meta-task. The performance evaluation and modeling in conjunction
% with task-specific management (such as placement, scheduling, and execution)
% allows the utilization of many heterogenous resources in an efficient manner.

% complexities of task management and coordination to the application layer.
% In order to isolate the application itself from the scheduling of resource
% placeholders,

% , due to its design it required changes to the application itself. For
% obvious reasons this is not always a desired situation and there was a need
% to have user-level control of scheduling without altering the original
% application. This brought about the idea of placeholder scheduling.


%  in that it was an abstraction layer above the various batch
% queuing systems available on different resources. It held a \textit{place} in
% the regular batch queue, and when it became active, it could pull tasks to
% execute.

%Core \pilotjob systems focus on the basic \pilot capabilities, i.\,e.\ the
%provisioning of the placeholder job capability. Various Master-Worker systems
%that provide such a mechanism (e.\,g.\
%Nimrod-G~\cite{10.1109/HPC.2000.846563}). Condor-G/Glide-In is the most
%well-known \pilotjob system.  \msnote{Im tempted to not make claims like this,
%especially not without backing up.}

%Further examples for lightweight \pilotjob systems are: ToPoS~\cite{topos},
%MyCluster~\cite{Walker:2007:PAC:1285840.1285848}, MySGE~\cite{mysge},
%GridBot~\cite{Silberstein:2009:GEB:1654059.1654071} and LGI~\cite{lgi}.

%Nimrod-G~\cite{10.1109/HPC.2000.846563}, DIANE~\cite{diane-thesis} and Work
%Queue~\cite{workqueue-pyhpc2011} are examples of Master-Worker systems that
%utilize a placeholder agent that dispatches and manages tasks. For example,
%Nimrod-G utilizes a Job Wrapper that is responsible for pulling a task and its
%associated data and then manages the execution of this task. While modern
%\pilotjobs often acquire resources opportunistically and then distribute tasks
%to resources they were able to acquire, Nimrod-G utilizes a central,
%cost-based scheduler.\msnote{Is this really a distinction, arent most of the
%systems centrally controlled?}

% Applications can be built on top of BOINC for their own scientific endeavors.
% \mrnote{As just an end reader, it is really unclear how this relates to
% anything. Is this supposed to follow from the previous para?}
% \aznote{Yes -- perhaps combine this + last para?  maybe even next para
% too...}

% A Glide-in is submitted using the Condor-G grid universe \msnote{Is this a
% distinctive feature? Arent the universes just labels anyway?}. On the remote
% resource a set of Condor daemons is started, which then registers the
% available job slots with the central Condor pool. The resources added are
% available only for the user who added the resource to the pool, thus giving
% complete control over the resources for managing jobs without any queue
% waiting time. Glide-in installs and executes necessary Condor daemons and
% configuration on the remote resource, such that the resource reports to and
% joins the local Condor pool. Various systems that built on the \pilot
% capabilities of Condor-G/GlideIn have been developed, e.\,g.\

% Glide-in is limited in that the daemons must be running on a given resource,
% meaning that this process must be approved by resource owners or system
% administrators.

% Venus-C~\cite{venusc-generic-worker} provides a \pilotjob-like capability on
% Microsoft Azure clouds called a Generic Worker. The Generic Worker creates a
% layer of abstraction above the inner workings of the cloud.  The idea behind
% the Generic Worker is to allow scientists to do their science without
% requiring knowledge of backend HPC systems by offering e-Science as a
% service. Venus-C has not been shown to work with grids, because its main
% objective is to motivate scientists to use cloud infrastructures.  While the
% notion of moving to the cloud for data-driven science is an important one,
% many existing cyberinfrastructures still have powerful grid computers that
% can also be leveraged to assist with the data-driven computations.

% In addition to the \pilotjob systems developed around the LHC experiment,
% several other systems emerged. Commented out gwpilot. Not really
% published/downloadable, and no obvious new functionality.
% GWPilot~\cite{gwpilot} is a \pilot system that is developed as a component
% for the GridWay meta-scheduler. GWPilot emphasizes its easy deployment,
% multi-user support and the support of standards, such as DRMAA, OGSA-BES and
% JSDL.
% \jhanote{will need references to these obscure acronyms!}
% \jhanote{Furthermore, must check that the reference to GWPilot is kosher. No
% bacon allowed.}
% \mrnote{It seems a little awkward that we say several others have emerged,
% then we say gwpilot is one. but then we go into talking in next para about
% scientific workflows + pjs. Several makes it sound like we have a bunch more
% to discuss}
% \msnote{In what way are these systems intrinsically pilot based? I think for
% example with Pegasus, it can make use of pilots, but uses it as just yet
% another "job submission backend". In this way, any system that creates "task"
% could be adopted to submit to a pilot-based backend}

% Many higher-level tools and frameworks, such as workflow, visualization or
% data analytics systems, utilize \pilotjob systems to manage their
% computational workload. In general, two approaches exist: (i) the framework
% vertically integrates with a custom \pilotjob implementation (e.\,g.\
% Swift/Coaster) or (ii) it re-uses a general purpose PJ system (e.\,g\
% Pegasus/Condor-G). In case (i), the PJ system is also often exposed as
% stand-alone, multi-purpose \pilotjob systems.

% In contrast to GlideinWMS, Corral-Glide-Ins are run using the credential of
% the user and not a VO credential. Workflow task clustering with
% Pegasus~\cite{Singh:2008:WTC:1341811.1341822}.

% For this purpose, Swift formalizes the way that applications can define
% data-dependencies. Using so called mappers, these dependencies can be easily
% extended to files or groups of files. The runtime environment handles the
% allocation of resources and the spawning of the compute tasks. Both data- and
% execution management capabilities are provided via abstract interfaces.

% Using the Coaster service, one executes a Coaster Master on a head node, and
% the Coaster workers run on compute nodes to execute jobs. In the case of
% cloud computing, Coasters offers a zero-install feature in which it deploys
% itself and installs itself from the head node and onto the virtual machines
% without needing any prior installation on the machine.
% \msnote{Where does the virtual machine suddenly come from?} Coaster relies on
% a master/worker coordination model. communication is implemented using
% GSI-secured TCP sockets. Swift supports various scheduling mechanisms on top
% of Coaster, e.\,g.\ a FIFO and a load-aware scheduler.
% \jhanote{I think the previous paragraph can be highly reduced. The next
% paragraph should be modified to highlight that {\bf specialized} pilots have
% also emerged, namely falkon to support many short running jobs on HPC
% systems. This is an important point to make and speaks to the success of the
% pilot concept}

% Falkon refers to pilots as the so called provisioner, which are created using
% the Globus GRAM service. The provisioner spawns a set of executor processes
% on the allocated resources, which are then responsible for managing the
% execution of task. Tasks are submitted via a so called dispatcher service.
% Falkon also utilizes a master-work coordination model, i.\,e.\ the executors
% periodically query the dispatcher for new tasks.

% Web services are used for communication\msnote{between?}.

% WISDOM~\cite{Ahn:2008:ITR:1444448.1445115,wisdom} is an application-centric
% environment for supporting drug discovery. The architecture utilizes an agent
% run as a Grid job to pull tasks from a central metadata service referred to
% as AMGA.
% \jhanote{this paragraph/pilot can go. I don't see the new functional feature
% or increased complexiy that WISDOM introduces}

% A LRMS incorporating a pilot scheme:
% OAR~\cite{oar} is a batch scheduler system for clusters and other
% computing infrastructures. Besides its more traditional batch scheduler
% features, it also has the functionality of \textit{container jobs}. These type
% of jobs allow the execution of jobs within other jobs, effectively making it a
% sub-scheduling mechanism, and thereby making it a batch scheduler with
% \pilotjob capabilities.

% Maybe add netsolve later:
% NetSolve~\cite{Casanova:1995:NNS:898848}

% -----------------------------------------------------------------------------
% Version 0.3
% -----------------------------------------------------------------------------

The origin and motivations for devising the \pilot abstraction and developing
its many implementations can be traced back to five main notions: task-level
distribution and parallelism, \MW pattern, multi-tenancy,
multi-level scheduling, and resource placeholder. This section offers an
overview of these five notions and an analysis of their relationship with the
\pilot abstraction. A chronological perspective is taken so to contextualize
the inception and evolution of the \pilot abstraction into its implementation
systems.

To the best of the authors' knowledge, the term `\pilotjob' was first
introduced around 2004 in the context of the Large Hadron Collider (LHC)
challenge [ref] and then used in a 2005 LHCb report\cite{lhcb2005}. Despite its
relatively recent explicit naming, the \pilot abstraction addresses a problem
already well-known at the beginning of the twentieth century: task-level
distribution and parallelism on diverse, multi-tenant resources.

Lewis Fry Richardson devised in 1922 a Forecast Factory
(Figure~\ref{fig:figures_forecast-factory}) to solve systems of differential
equations for weather forecasting. This factory required 64,000 human
\textit{computers} supervised by a senior clerk. The clerk would distribute
portions of the differential equations to the `computers' so that they could
forecast the weather of specific regions of the globe. The computers would
perform their calculations and then send the results back to the clerk. The
Forecast Factory was not only an early conceptualization of what is called
today a ``supercomputer'' but also of the coordination pattern for distributed
and parallel computation called ``\MW''.

\begin{figure}[t]
  \centering
    \includegraphics[width=.45\textwidth]{figures/forecast-factory.jpg}
  \caption{\textit{Forecast Factory} as envisioned by Lewis Fry Richardson.
    Drawing by Fran{\c c}ois Schuiten.}
  \label{fig:figures_forecast-factory}
\end{figure}

The clerk of the Forecast Factory is the `master' while the human computers are
her `workers'. Requests and responses go back and forth between the master and
all its workers. Each worker has no information about the overall computation
nor about the states of any other worker. The master is the only one possessing
a global view both of the overall problem to compute and of its progress
towards a solution. As such, the M-W is a coordination pattern allowing for the
structured distribution of tasks so to orchestrate their parallel execution.
This directly translates into a better time to completion (TTC) of the overall
computation when compared to a coordination pattern in which each equation is
sequentially solved by a single worker.

Modern silicon-based supercomputers brought at least three key differences when
compared to the carbon-based Forecast Factory devised by Richardson. Most of
modern supercomputers are meant to be used by multiple users, i.e. they support
multi-tenancy. Furthermore, diverse supercomputers were made available to the
scientific community, each with both distinctive and homogeneous properties in
terms of architecture, capacity, capabilities, and interfaces. Finally,
supercomputers supported different types of applications, depending on the
applications' communication and coordination models.

Multi-tenancy has defined the way in which high performance computing resources
are exposed to their users. Job schedulers, often called ``batch queuing
systems'' [ref] and first used in the time of punch cards [ref], leverage the
batch processing concept to promote efficient and fair resource sharing. Job
schedulers implement a usability model where users submit computational tasks
called ``jobs'' to a queue. The execution of these job is delayed waiting for
the required amount of resources to be available. The amount of delay mostly
depends on the size and duration of the submitted job, resource availability,
and fair usage policies.

Supercomputers are often characterized by several types of heterogeneity and
diversity. Users are faced with different job description languages, job
submission commands, and job configuration options. Furthermore,  the number of
queues exposed to the users and their properties like walltime, duration, and
compute-node sharing policies vary from resource to resource. Finally, each
supercomputer may be designed and configured to support only specific types of
application.

The resource provision of multi-tenant and heterogeneous supercomputers is
limited, irregular, and largely unpredictable [ref, ref]. By definition, the
resources accessible and available at any given time can be less than those
demanded by all the active users. Furthermore, the resource usage patterns are
not stable over time and alternating phases of resource availability and
starvation are common [ref]. This landscape led not only to a continuous
optimization of the management of each resource but also to the development of
alternative strategies to expose and serve resources to the users.

Multi-level or meta scheduling is one of the strategies devised to improve
resource access across multiple supercomputers. The idea is to hide the
scheduling point of each supercomputer behind a single (meta) scheduler. The
users or the applications submit their tasks to the single scheduler that
negotiates and orchestrates the distribution of the tasks via the scheduler of
each available supercomputer. While this approach promises an increase in both
scale and usability of applications, it also introduces diverse types of
complexity across resources, middleware, and applications.

Several approaches have been devised to manage the complexities associated with
multi-level scheduling. Some approaches, for example those developed under the
umbrellas of grid computing or cloud computing, targeted the resource layer,
others the application layer as, for example, with workflow frameworks. All
these approaches offered and still offer some degree of success for specific
applications and use cases but a general solution based on well-defined and
robust abstractions has still to be devised and implemented.

One of the persistent issues besetting resource management across multiple
supercomputers is the increase of the implementation complexity imposed on the
application layer. Even with solutions like grid computing aiming at
effectively and, to some extent, transparently integrating diverse resources,
most of the requirements involving the coordination of task execution still
lays with the application layer. This translates into single-point solutions,
extensive redesign and redevelopment of existing applications when they need to
be adapted to new use cases or new resources, and lack of portability and
interoperability.

Consider for example a simple distributed application implementing a M-W
pattern. With a single supercomputer, the application requires the capability
of concurrently submitting tasks to the queue of the supercomputer scheduler,
and retrieve and aggregate their outputs. When multiple supercomputers are
available, the application requires directly managing submissions to several
queues or the capability to leverage a third-party meta-scheduler and its
specific execution model. In both scenarios, the application requires a large
amount of development and capabilities that are not specific to the given
scientific problem but pertain instead to the coordination and management of
its computation.

The notion of resource placeholder was devised as a pragmatic and relatively
cheap to implement attempt to reduce or at least better manage the complexity
of executing distributed applications. A resource placeholder decouples the
acquisition of remote compute resource from their use to execute the tasks of a
distributed application. Resources are acquired by scheduling a job onto the
remote supercomputer. Once executed, the job runs an agent capable of
retrieving and executing application tasks.

Resource placeholders bring together multi-level scheduling and, when useful,
the M-W pattern to enable parallel execution of the tasks of distributed
applications. Multi-level scheduling is achieved by scheduling the agent and
then by enabling direct scheduling of application tasks to that agent. The M-W
pattern is often an effective choice to manage the coordination of tasks
execution on the available agent(s). Multi-level scheduling can be extended to
multiple resources by instantiating resource placeholders on diverse
supercomputing and then using a meta-scheduler to schedule tasks across all the
placeholders.

It should be noted that resource placeholders also mitigate the side-effects
introduced by multi-tenancy. Multi-tenancy affects only the scheduling of the
resource placeholder as it needs to be executed by the batch system of the
remote supercomputer. Once the placeholder is executed, the user -- or the
master process of the distributed application -- may hold total control of the
resource placeholder. In this way, tasks are directly scheduled on the
placeholder without competing with other users for the supercomputer scheduler.

As seen in Ref.~\cite{pstar-2012}, the \pilot abstraction has a rich set of
properties and its implementations offer a vast array of capabilities including
multiple scheduling algorithms, data and compute placeholders, and late or
early binding. Nonetheless, the capability of acquiring remote resources and
directly utilizing them, independently from the supercomputer resource
management, is a necessary property of the \pilot abstraction. As such,
resource placeholders and their
scheduling~\cite{Pinchak02practicalheterogeneous} should be seen as early
\pilot system implementations.

% The progressive definition and implementation of the \pilot abstraction can
% be seen as the process of evolving both the understanding and implementation
% complexity of the notion of resource placeholder.

\begin{figure}[t]
% Put real dates in the comment here.
% Boinc: X
% BigJob: 200X
% etc.
  \centering
    \includegraphics[width=0.45\textwidth]{figures/timeline}
    \caption{Introduction of systems over time. When available, the date of
    first mention in a publication or otherwise the release date of software
    implementation is used.}
    \label{fig:timeline}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[width=.45\textwidth]{figures/pilotjob-clustering.pdf}
  \caption{Pilot-Job Clustering}
  \label{fig:pilotjob_clustering}
\end{figure}

AppLeS~\cite{Berman:1996:apples} is a framework for application-level
scheduling and offers an example of an early implementation of resource
placeholder. AppLeS provides an agent that can be embedded into an application
thus enabling the application to acquire resources and to schedule tasks onto
these. Besides M-W, AppLeS also provides application templates, e.\,g.\ for
parameter sweep and moldable parallel
applications~\cite{Berman:2003:ACG:766629.766632}.

AppLeS offered user-level control of scheduling but did not isolate the
application layer from the management and coordination of task execution. Any
change in the coordination mechanisms directly translated into a change of the
application code. The next evolutionary step was to create a dedicated
abstraction layer between those of the application and of the various batch
queuing systems available at different remote systems.

Around the same time as AppLeS was introduced, volunteer computing projects
started using the M-W coordination pattern to achieve high-throughput
calculations for a wide range of scientific problems. The workers of these
systems could be downloaded and installed on the user laptop and workstation.
With an installation base potentially distributed across the globe, workers
pulled and executed computation tasks when CPU cycles were available.

The volunteer workers were essentially heterogeneous and dynamic as opposed to
the homogeneous and static AppLeS workers. The idea of farming out tasks in a
dynamic distributed environment including personal computers promised to lower
the complexity of distributed applications design and implementation. Each
volunteer worker can be seen as an opportunistic resource placeholder and, as
such, an implementation of the core functionality of the \pilot abstraction.

The first public volunteer computing projects were The Great Internet Mersenne
Prime Search effort\cite{woltman:2004:gimps}, shortly followed by
distributed.net~\cite{Lawton:2000:distributednet} in 1997 to compete in the
RC5-56 secret-key challenge, and the SETI@Home project, which set out to
analyze radio telescope data. The generic BOINC distributed master-worker
framework grew our of SETI@Home, becoming the {\it de facto} standard framework
for voluntary computing~\cite{Anderson:2004:BSP:1032646.1033223}.

It should be noted that process of resource acquisition is different in AppLes
and voluntary computing. The former has complete knowledge of the available
resources while the latter has none. As a consequence, AppLes can request and
orchestrate a set of resource, allocates tasks in advance to specific workers
(i.e. resources placeholders), and implement load balancing among resources. In
voluntary computing tasks are polled by the clients when they become active
and, as such, resource availability is unknown in advance. This potential
drawback is mitigated by the redundancy offered by the large scale that
voluntary computing can reach thanks to its simpler model of worker
distribution and installation.

The opportunistic use of geographically distributed resources championed by
voluntary computing offers several advantages. The resource landscape available
for scientific research is fragmented across multiple institutions, managed
with different policies and protocols, and heterogeneous both in quantity and
quality. Once aggregated, the sum of otherwise limited resources can support
very large distributed computations and an great amount of multi-tenancy. Note
that given the required capabilities, this model of resource provisioning can
still support the execution of parallel applications on the few resources that
offer low-latency network interconnect.

Condor is a high-throughput distributed batch computing system that leverages
diverse and possibly geographically distributed resources. Originally, Condor
was created for systems within one administrative domain but
Flocking~\cite{Epema:1996:flocking} made possible to group multiple Condor
resource pools in an aggregative manner. However, resource management could not
be done on application level by the user: Flocking required system level
software configurations that had to be made by the administrator of each
individual resource of each Condor resource pool.

This limitation was overcame by integrating a resource placeholder mechanism
within the Condor system. Condor-G/GlideIn~\cite{condor-g} allow users to add
remote grid resources to Condor resource pools. In this way, users can
uniformly execute jobs on resource pools composed by diverse resources. Thanks
to its use of resource placeholders, GladeIn has been one of the systems
pioneering the \pilot abstraction implementation, enabling \pilot capabilities
also for third parties systems like Bosco~\cite{bosco}.

The success of Condor-G/GlideIn shows the relevance of the pilot abstraction to
enable scientific computation at scale and on heterogeneous resources. The
implementation of GlideIn also highlighted at least two limitations:
user/system layer isolation, and application development model. While GlideIn
allows for the user to manage resource placeholders directly, daemons must
still be running on the remote resources. This means that GlideIn cannot be
deployed without involving the resource owners and system administrators.
Implemented as a service, GlideIn support integration with distributed
application frameworks but does not programmatically support the development of
distributed applications by means of dedicated APIs and libraries.

The BigJob \pilot system [ref] was designed to address these limitations, to
broaden the type of applications supported by the \pilot-based execution model,
and to extend the \pilot abstraction beyond the boundaries of compute tasks.
BigJob offers an application-level programmability to provide the end-user with
more flexibility and control over the design of distributed application and the
isolation of the management of their execution. BigJob is flexible and
extensible and uses the Simple API for Grid Applications (SAGA)
interoperability library [ref] to work on a variety of infrastructures.
Furthermore, BigJob supports both MPI and non-MPI jobs without adding
additional configuration requirements to the end-user. Finally, BigJob has also
been extended to work with data and, analogous to compute \pilots, to abstract
away direct user communication between different storage systems.

BigJob, recently redesigned as a pre-production system named RADICAL Pilot
[ref], represents one of the latest evolutionary stages of the \pilot
abstraction. From an initial phase in which \pilot were implemented as ad hoc
place holder machinery for a specific application, to the integration of \pilot
systems within the middleware of remote resources, PilotJob implements the
\pilot abstraction as an interoperable compute and data management system that
can be programmatically integrated into end-user applications.

Another ongoing evolutionary trend of the \pilot abstraction is to be
implemented into \pilot-based workload managers, thus moving away from
providing simple \pilot capabilities in application space. These higher-level
systems which are often centrally hosted, move critical functionality from the
client to the server (i.e. a service model). These systems usually deploy
\pilot factories that automatically start new \pilots on demand and integrate
security mechanisms to support multiple users simultaneously.

Several of these have been developed in the context of the LHC experiment at
CERN, which is associated with a major increase in the uptake and availability
of \pilots, e.\,g.\ GlideInWMS, DIRAC~\cite{1742-6596-219-6-062049},
PanDa~\cite{1742-6596-331-7-072069}, AliEn~\cite{1742-6596-119-6-062012} and
Co-Pilot~\cite{copilot-tr}. Each of these \pilots serves a particular user
community and experiment. Interestingly, we observe that these \pilots are
functionally very similar, work on almost the same underlying infrastructure,
and serve applications with very similar (if not identical) characteristics.

GlideinWMS~\cite{1742-6596-119-6-062044} is a higher-level workload management
system that is based on the \pilot capabilities of Condor GlideIn. The system
can, based on the current and expected number of jobs in the pool,
automatically increase or decrease the number of active Glide-ins (\pilots)
available to the pool. GlideinWMS is a multi-user \pilotjob system commonly
deployed as a hosted service. GlideinWMS attempts to hide the \pilot
capabilities rather than exposing them to the user. GlideinWMS is currently
deployed in production on the Open Science Grid (OSG)~\cite{url_osg} and is the
recommended mode for new users wanting to access OSG resources.

\panda (Production and Distributed Analysis)~\cite{1742-6596-331-7-072069} is
the workload management system of the ATLAS experiment, used to run managed
production and user analysis jobs on the grid. The ATLAS Computing Facility
operates the pilot submission systems. This is done using the \panda
`AutoPilot' scheduler component which submits pilot jobs via Condor-G. \panda
also provides the ability to manage data associated the jobs managed by the
\panda workload manager.

In addition to processing, AliEn~\cite{1742-6596-119-6-062012} also provides
the ability to tightly integrate storage and compute resources and is also able
to manage file replicas. While all data can be accessed from anywhere, the
scheduler is aware of data localities and attempts to schedule compute close to
the data. AliEn deploys a pull-based model~\cite{Saiz:2003:alien} assuming that
the resource pool is dynamic and that a pull model doesn't require the broker
to keep detailed track of all resources, which leads to a more simple
implementation.

DIRAC~\cite{1742-6596-219-6-062049} is another comprehensive workload
management system built on top of \pilots. It supports the management of data,
which can be placed in different kinds of storage elements (e.\,g.\ based on
SRM).

Another interesting \pilot that is used in the LHC context is
Co-Pilot~\cite{copilot-tr}. Co-Pilot serves as an integration point among
different grid \pilotjob systems (such as AliEn and \panda), clouds and
volunteer computing resources. Co-Pilot provides components for building a
framework for seamless and transparent integration of these resources into
existing Grid and batch computing infrastructures exploited by the High Energy
Physics community.

The \pilot abstraction has also been integrated into scientific workflow
systems. \pilot systems have proven an effective tool for managing the
workloads executed in the various stages of a workflow. For the Pegasus
project, the Corral system~\cite{Rynge:2011:EUG:2116259.2116599} was developed
to support the requirements of the Pegasus workflow system in particular to
optimize the placements of \pilots with respect to their workload. It did this
by serving as a front-end to Condor GlideIn. In contrast to GlideinWMS, Corral
provides more explicit control over the placement and start of \pilots to the
end-user. Corral was later extended to also serve as a possible front end to
GlideinWMS.

Swift~\cite{Wilde2011} is a scripting language designed for expressing abstract
workflows and computations. The language provides also capabilities for
executing external application as well as the implicit management of data flows
between application tasks. Swift uses a \pilot implementation called
``Coaster''~\cite{coasters}, developed to address workload management
requirements by supporting various types of infrastructure, including clouds
and grids.

% Swift has also been used in conjunction with Falkon~\cite{1362680}. Falkon
% was engineered for executing many small tasks on HPC systems and shows high
% performance compared to the native queuing systems. Falkon is a paradigmatic
% example of how the \pilot abstraction can be implemented to support very
% specific workload.

\mtnote{Needs a better closing}

Based on the descriptions in this section we can identify some distinctions in
terms of design, usage and operation of pilot(-based) systems.
Figure~\ref{fig:pilotjob_clustering} is a graphical representation of this
rough clustering.

The evolution of \pilotjobs attests to their usefulness across a wide range of
deployment environments and application scenarios, but the divergence in
specific functionality and inconsistent terminology calls for a standard
vocabulary to assist in understanding the varied approaches and their
commonalities and differences.


%------------------------------------------------------------------------------
% SECTION 3
%------------------------------------------------------------------------------

\newcommand{\vocab}[1]{\textbf{#1}\xspace}
\newcommand{\prop}[1]{\textit{#1}\xspace}
\newcommand{\impterm}[1]{\texttt{#1}\xspace}

\section{Understanding the Landscape: Developing a Vocabulary}
\label{sec:understanding}

The overview presented in \S\ref{sec:history} shows a degree of heterogeneity
both in the functionalities and the vocabulary adopted by different \pilot
systems. Implementation details sometimes hide the functional commonalities and
differences among \pilot systems while features and capabilities tend to be
named inconsistently, often with the same terms referring to multiple concepts
or the same concept named in different ways.

This section offers an analysis of the logical components and functionalities
shared by every \pilot system. The goal is to offer a paradigmatic description
of a \pilot system and a well-defined vocabulary to reason about such a
description and, eventually, about its multiple implementations.

%------------------------------------------------------------------------------
% 3.1
\subsection{Logical Components and Functionalities}
\label{sec:compsandfuncs}

All the \pilot systems introduced in~\S\ref{sec:history} are engineered to
allow for the execution of (multiple types of) workloads on Distributed
Computing Infrastructures (DCIs) such as grid, cloud, or HPC facilities. This
is achieved differently, depending on use cases, design and implementation
choices, but also on the constraints imposed by the middleware and policies of
the targeted DCIs. The common denominators among \pilot systems are defined
along multiple dimensions: purpose, logical components, and functionalities.

The purpose shared by every \pilot system is to improve the (performance of)
workload execution when compared to executing the same workload directly on one
or more DCI. Performance in \pilot systems is usually associated to throughput
and execution time to completion (TTC), but other metrics could also be
considered, for example, energy efficiency, data transfer minimization, scale
of the workload executed, or a mix of them. In order to optimize the chosen set
of metrics, each \pilot system exhibits characteristics that are both common or
specific to one or more implementations. Discerning these characteristics
requires isolating and defining the minimal set of logical components that has
to characterize every \pilot system.

At some level, all \pilot systems leverage three separate but coordinated
logical components: a \vocab{Pilot Manager}, a \vocab{Workload Manager}, and a
\vocab{Task Manager}. The Pilot Manager handles the description, instantiation,
and use of one or more resource placeholders (i.e. `\pilots`) on single or
multiple DCIs. The Workload Manager handles the scheduling of one or more given
workloads on the available resource placeholders. Finally, the Task Manager
takes care of executing the tasks of each workload by means of the resources
held by the placeholders.  \jhanote{does the task executor just execute, or
does it have some decision making capacity? if the latter, then purely for
naming consistency, should we consider calling the third component ``task
manager'' or ``Task execution manager'' ? Also we go on to discuss task
dispatching and execution as functionalities, which leaves current name a bit
limiting.} \mtnote{Replaced `Task Executor' with `Task Manager'.}

The implementation details of these three logical components significantly vary
across \pilot systems (see~\S\ref{sec:analysis}). One or more logical
components may be responsible for specific functionalities, both on application
as well as infrastructure level, two or more logical components may be
implemented in a single software module, or additional components may be
integrated into the Managers and Task Manager. Nevertheless, the Pilot and
Workload Managers and the Task Manager can always be distinguished across
different \pilot systems. For example, looking at the systems mentioned
in~\S\ref{sec:history}, \ldots \mtnote{I will wait for S2 and S4 to be final
before picking the right examples/vocabulary.} \msnote{How about not having an
example here? In a way it will cut short what we do in S4} \mtnote{I see what
you mean. Let's see how does it look like once we have a full draft.}

Each \pilot system supports a minimal set of functionalities that allow for the
execution of workloads: \vocab{Pilot Provisioning}, \vocab{Task Dispatching},
and \vocab{Task Execution}. \pilot systems need to schedule resource
placeholders on the targeted resources, schedule tasks on the available
placeholders, and then use these placeholders to execute the tasks of the given
workload.

More functionalities might be needed to implement a production-grade \pilot
system: authentication, authorization, accounting, data management,
fault-tolerance, or load-balancing. While these functionalities may be critical
implementation details, they depend on the specific characteristics of the
given use cases, workloads, or targeted resources. As such, these
functionalities should not be considered a necessary characteristic of every
\pilot system.

Among the core functionalities that characterize every \pilot system, Pilot
Provisioning is essential because it allows for the creation of resource
placeholders. As seen in \S\ref{sec:history}, this type of placeholder enables
tasks to utilize resources without directly depending on the capabilities
exposed by the targeted DCI. Resource placeholders are scheduled onto the DCI
resources by means of the DCI capabilities, but once scheduled and then
executed, these placeholders make their resources directly available for the
execution of the tasks of a workload.
% Resource placeholders belong to the user(s) that created them and they can be
% tailored to satisfy the specific use cases.
\msnote{Regarding the "belonging", this becomes fuzzy in multi-user
systems, I would suggest to drop the last line} \mtnote{Done.}
\mtnote{Currently, this is a bit of repetition of what we wrote already in
Section 2 but seeing the importance of the concept we might want to keep it.}

% MS: I would move this comment to section 5 I think, as multi-tenant pilot
% systems do have to make these trade-offs, and it would be good to point that
% out. (not doing it now because of MT lock in 5)
% Furthermore, resource placeholders are logical partitions of resources that
% do not need to leverage trade-offs among competing user requirements as
% needed instead with large pools of resources adopting multi-tenancy.

The provisioning of resource placeholders depends on the capabilities exposed
by the targeted DCI and on the implementation of each \pilot system. Typically,
for DCIs adopting queues, batch systems, and schedulers, provisioning a
placeholder involves it being submitted as a job. A `job' on this kind of DCIs
is a type of logical container that includes configuration and execution
parameters alongside information on the executable that will be executed on the
DCIs compute nodes. Conversely, for infrastructures that do not adopt a
job-based middleware, a resource placeholder would be executed by means of
other types of logical container as, for example, a Virtual Machine (VM) or a
Docker Engine [cit, cit].

Once resource placeholders are bound to a DCI, tasks need to be dispatched to
those placeholders for execution. Task dispatching does not depend on the
functionalities of the DCI middleware so it can be implemented as part of the
\pilot system. In this way, the control over the execution of a workload is
shifted from the DCI to the \pilot system. This shift is a defining
characteristic of the \pilot paradigm \mtnote{I do not think it is too
disruptive to speak about abstraction but, in case, we can just use \pilot
system instead of \pilot abstraction} \msnote{How about paradigm? I would not
be in favor of talking about systems here} \mtnote{Done.} as it decouples the
execution of a workload from the need to submit its tasks via the DCI
middleware. The tasks of a workload will not individually have to wait on the
DCI queues, but rather on the availability of the placeholder before being
executed. More elaborate execution patterns involving task and data
interdependence can thus be implemented outside the boundaries of the DCI
capabilities. Ultimately, this is why \pilot systems allow for the direct
control of workload execution and the optimization, for example, of execution
throughput. \msnote{I don't understand the bit about parametrization
..} \mtnote{Better?}

Communication and coordination are two distinguishing characteristics of
distributed applications and \pilot systems make no exception. The Workload
Manager, Pilot Manager, and the Task Manager need to communicate to coordinate
the execution of the given workload on the instantiated resource placeholders.
Nonetheless, \pilot systems are not defined by any specific communication
pattern and coordination strategy. The logical components of a \pilot system
may communicate with every suitable pattern (e.g. one-to-one, many-to-one,
one-to-many (cit) with a push or pull model) and coordinate adopting any
suitable strategy (e.g. time synchronization, static or dynamic coordinator
election, local or global information sharing, or \MW). The same applies to
network architectures and protocols: different network architectures and
protocols may be leveraged to achieve effective communication and coordination.
\msnote{The paragraph above is more ammunition for the argument that the
division between core and aux props is at least fuzzy, or potentially
arbitrary, or C\&C is just a core prop. This needs further discussion.}
\mtnote{In the agenda for next Tuesday? Otherwise, please feel free to propose
the changes in 4.1 and we can discuss via comments.}

As seen in~\S\ref{sec:history}, \MW is a very common coordination pattern among
\pilot systems. When the master is identified with the Workload Manager, and
the worker with the Task Manager, the functionalities related to task
description, scheduling, and monitoring will generally be implemented within
the Workload Manager, while the functionalities needed to execute each task
will be implemented into the Task Manager. Alternative coordination
strategies, for example where a Task Manager directly coordinates the
task scheduling, might require a functionally simpler Workload Manager but a
comparatively feature-rich Task Manager. The former would require capabilities
for submitting tasks, while the latter would require to coordinate with its
neighbor executors leveraging, for example, a dedicated overlay network. Both
these systems, adopting different coordination strategies, should be considered
\pilot systems.

Data management can have an important role within a \pilot system. For example,
functionalities can be provided to support the local or remote data staging
required to execute the tasks of a workload, or data might be managed according
to the specific capabilities offered by the targeted DCI. \pilot systems can be
devised in which tasks do not require any data management because they (i) do
not necessitate input files, (ii) do not produce output files, (iii) data is
already locally available or (iv) data management is outsourced to third-party
systems. Being able to read and write files to a local filesystem should then
be considered the minimal capability related to data required by a \pilot
system. More advanced and specific data capabilities like, for example, data
replication, (concurrent) data transfers, data abstractions other than files
and folders, or data placeholders should be considered special-purpose
capabilities, not characteristic of every \pilot system.

In the following subsection, a minimal set of terms related to the
logical components and capabilities so far described is defined.

%------------------------------------------------------------------------------
% 3.2
\subsection{Terms and Definitions}
\label{sec:termsdefs}

The terms `pilot' and `job' are arguably among the most relevant when referring
to \pilot systems. It is the case that \pilot systems are commonly referred to
as `\pilotjob systems' [ref, ref], a clear indication of the primary role
played by the concepts of `pilot' and `job' in this type of system. The
definition of both concepts is context-dependent and several other terms need
to be clarified in order to offer a coherent terminology. Both `job' and
`pilot' need to be understood in the context of DCIs, the infrastructures used
by \pilot systems. DCIs offer compute, storage, and network resources and
\pilots allow for the users to utilize those resources to execute the tasks of
one or more workloads.

\begin{description}

\item[Task.] A container for operations to be performed on a computing
platform, alongside a description of the properties of those operations, and
indications on how they should be executed. Implementations of a task may
include wrappers, scripts, or applications.

\item[Workload.] A set of tasks, possibly related by a set of arbitrarily
complex relations.

\item[Resource.] Finite, typed, and physical quantity utilized when executing
the tasks of a workload. Compute cores, data storage space, or network
bandwidth between a source and a destination are all examples of resources
commonly utilized when executing workloads.

\item[Infrastructure or DCI.] Structured set of resources, possibly
geographically and institutionally separated from the users that utilize those
resources to execute the tasks of a workload [ref]. Infrastructures can be
logically partitioned, with a direct or indirect mapping onto individual pools
of hardware (i.e. clusters, systems, and supercomputers).

\end{description}

As seen in~\S\ref{sec:history}, most of the DCIs leveraged by \pilot systems
utilize `queues', `batch systems' and `schedulers'. In such DCIs, jobs are
scheduled and then executed by a batch system.

\begin{description}

\item[Job.] Functionally defined as a `task' from the perspective of the DCI,
but in the case of a \pilot indicative of the type of container required to
acquire resources on a specific infrastructure.

\end{description}

When considering \pilot systems, jobs and tasks are functionally analogous but
qualitatively different. Functionally, both jobs and tasks are containers --
i.e. metadata wrappers around one or more executables often called `kernel',
`application', or `script'. Qualitatively, the term `task' is used when
reasoning about workloads while `job' is used in relation to a specific type of
infrastructure where such a container can be executed. Accordingly, tasks are
considered as the functional units of a workload, while jobs as a way to
schedule tasks on a certain infrastructure. It should be noted that, given
their functional equivalence, the two terms can be adopted interchangeably when
considered outside the context of \pilot systems. Indeed, workloads are encoded
into jobs when they have to be directly executed on infrastructures that
support or require that type of container.

As described in~\S\ref{sec:compsandfuncs}, a resource placeholder needs to be
submitted to the target DCI wrapped in the type of container supported by that
specific DCI. For example, for a DCI exposing a HPC or Grid middleware [cit,
cit], a resource placeholder needs to be wrapped within a `job'. For other type
of DCIs, the same resource placeholder will need to be wrapped within a
different type of container as, for example, a VM or a Docker Engine. For this
reason, the capabilities exposed by the job submission system of the target DCI
determine the submission process of resource placeholders and how or for how
long they can be used. For example, when wrapped within a `job', placeholders
are provisioned by submitting a job to the DCI queuing system, become available
only once the job is scheduled on the resources out of the queue, and is
available only for the duration of the job lifetime.

A `pilot' is a resource placeholder. As a resource placeholder, a \pilot holds
portion of a DCI's resources for a user or a group of users, depending on
implementation details. A \pilot system is a software capable of creating
\pilots so to gain exclusive control over a set of resources on one or more DCIs
and then to execute the tasks of one or more workloads on those \pilots.

\begin{description}

\item[Pilot.] A container (e.g., a job) that functions as a resource
placeholder on a given infrastructure and is capable of executing tasks of a
workload on that resource.

\end{description}

It should be noted that the term `pilot' as defined here is named differently
across \pilot systems. Depending upon context, in addition to the term
`placeholder', \pilot is also named `agent' and, in some cases, `\pilotjob'
[cit]. All these terms are, in practice, used as synonyms without properly
distinguishing between the type of container and the type of executable that
compose a \pilot. This is a clear indication of how necessary the minimal and
consistent vocabulary offered here is when reasoning analytically about
multiple implementations of a \pilot system.

The term `pilotjob' is often used to identify a \pilot system too. This is an
unfortunate choice as the term `job' identifies just the way in which a \pilot
is provisioned on a DCI exposing specific capabilities, not a general
property of all the \pilot systems. The use of the term `\pilotjob system'
should therefore be regarded as a historical artifact, viz., the targeting of a
specific class of DCIs in which the term `job' was, and still is, meaningful.
With the development of new types of DCI middleware as, for example, cloud
infrastructures, the term `job' has become too restrictive, a situation that
can lead to terminological and conceptual confusion.

We have now defined resources, DCIs and \pilots. We have established that a
\pilot is a placeholder for a set of resources. When combined, the resources of
multiple \pilots form a resource overlay. The \pilots of a resource overlay can
potentially be distributed over multiple resources and/or DCIs.

% The details of this aggregation or federation is outside the scope of this
% paper.

\begin{description}
\item[Resource Overlay.] The aggregated set of resources of multiple \pilots.
\end{description}

Three more terms associated with \pilot systems (cit, cit, cit) need to be
explicitly defined: `Multi-level scheduling', `early binding', and `late
binding'.

Pilot systems are said to implement multi-level scheduling because they require
the scheduling of two types of entities: \pilots and tasks. A portion of the
resources of a DCI is allocated to one or more \pilots in the form of
containers supported by that DCI, and the tasks of a workload are dispatched to
those \pilots. This is a fundamental feature of \pilot systems because (i) a
potentially faster and more flexible execution of workloads is achieved by
avoiding the overhead imposed by a centralized job management system shared
among multiple users; and (ii) the tasks of a workload can be bound to a set of
\pilots before or after it becomes available on a remote resource.  (Depending
on the implementation of a \pilot system, there can be another level of
scheduling, where the \pilot makes scheduling decisions about task placement
within the \pilot's resource allocation.)

The simplification obtained as a consequence of bypassing the job submission
system of the DCI is one of the main reasons for the success of \pilot
systems. As mentioned in~\S\ref{sec:compsandfuncs}, the tasks of a workload can
be executed on a \pilot without waiting in the queuing system of the given
infrastructure, and as a result, increasing the throughput of the workload
execution. Moreover, \pilots can be reused to execute multiple workloads until
the \pilots' walltime expire.  It should be noted that how tasks are actually
assigned to \pilots is a matter of implementation. For example, a dedicated
scheduler could be adopted, or tasks might be directly assigned to a \pilot by
the user.

The binding of tasks to \pilots depends on the state of the \pilot. A \pilot is
inactive until it is executed on a DCI, is active thereafter, until it
completes (or fails).  Early binding indicates the binding of a task to an
inactive \pilot; late binding the binding of a task to an active \pilot. Early
binding is potentially useful to increase the information about which \pilots
can be deployed. By knowing in advance the properties of the tasks that are
bound to a \pilot, specific deployment decisions can be made for that \pilot.
Additionally, in case of early binding, other type of decisions related to the
workload could be made, e.g., the transfer of data to a certain resource while
the \pilot is still inactive. Late binding is critical in assuring the
aforementioned high throughput by allowing task execution without additional
queuing time or container instantiation time. \amnote{that gives fast startup
of tasks, but not high throughput.  Also, the pilot must start up, late binding
won't change that.  I know we can pick the first pilot which becomes active --
but that is not what is being said here...} \mtnote{I think this does not refer
to backfilling but to scheduling tasks to an active pilot. There is no queuing
time for the tasks and tasks are executed as soon as they are scheduled into
the pilot (barring resource exhaustion of the pilot itself) i.e. high
throughput.}\amnote{All else being equal, isn't the difference then that the
pilot is submitted earlier rather then the effect of early vs. late binding?
Assume a pilot is submitted a $T_0$, tasks are submitted at $T_1$, the pilot
becomes active at $T_2 > T_1$.  On both early an late binding, the tasks can
only start execution after $T_2$ -- what shifts is time of scheduling, not time
of task execution.  For a single pilot, it should not make a difference.}
\mtnote{Consider a system where pilots are routinely submitted independently
from the availability of a workload on an array of resources. Or a system where
pilots always ask for the maximum walltime serving a VO instead of a single
user. Or a pilot system using VMs. The case you present looks radical-pilot
dependent and probably too near to implementation details. This subsection is
about a set of definitions that need to fit every possible pilot system. Said
that, if you have an alternative definition of late/early binding and an
alternative explicative paragraph, please fell free to contribute them and I
will be more than happy to integrate them with/instead of the current
definitions and explanation. I ask this because I am struggling to understand
what you are concretely proposing to change to the paragraph and to the
definition(s). } It should be noted that some aspects of early binding can also
be achieved without a \pilot, but, importantly, the \pilot paradigm allows to
do both, even within the confinements of a single workload. \msnote{I would
think parts of this paragraph also have their place in S5 (if its not already
there)} \mtnote{The notion of binding needs to be fully understood before
getting into 4. I would leave this as it is here but I am open to discuss about
it.}

\begin{description}

\item[Multi-level scheduling.] Scheduling \pilots onto resources and tasks onto
active or inactive \pilots.

\item[Early binding.] Binding one or more tasks to an inactive \pilot.

\item[Late binding.] Binding one or more tasks to an active \pilot.

\end{description}

% Depending on the specific capabilities implemented in the Workload Manager
% component, some \pilot systems allow for \pilots to be specified by taking
% into consideration the properties of (the tasks of) a workload. This type of
% specification should not be confused with early binding as the latter
% requires for a \pilot to have been already bound to a resource. \msnote{I
% think this paragraph is orthogonal to what we write earlier. I suggest to
% remove this paragraph altogether.} \amnote{+1, this confuses more than it
% clarifies...}

\definecolor{term}{RGB}{153,39,38}
\definecolor{funct}{RGB}{0,128,64}
\definecolor{lcomp}{RGB}{0,128,255}

\begin{table*}
 \centering
 \begin{tabular}{|p{4cm}|p{3.2cm}|p{3.2cm}|}
  \hline
    \textbf{Term} &
    \textbf{Functionality} &
    \textbf{Logical Component} \\
  \hline
  \hline
    \textcolor{term}{Workload} &
    \textcolor{funct}{Task Dispatching} &
    \textcolor{lcomp}{Workload Manager} \\
  \hline
    \textcolor{term}{Task} &
    \textcolor{funct}{Task Dispatching} \newline
      \textcolor{funct}{Task Execution} &
    \textcolor{lcomp}{Workload Manager} \newline
      \textcolor{lcomp}{Task Manager} \\
  \hline
    \textcolor{term}{Resource} &
    \textcolor{funct}{Pilot Provisioning} &
    \textcolor{lcomp}{Pilot Manager} \\
  \hline
    \textcolor{term}{Infrastructure} or \textcolor{term}{DCI} &
    \textcolor{funct}{Pilot Provisioning} &
    \textcolor{lcomp}{Pilot Manager} \\
  \hline
    \textcolor{term}{Job} &
    \textcolor{funct}{Pilot Provisioning} &
    \textcolor{lcomp}{Pilot Manager} \\
  \hline
    \textcolor{term}{Pilot} &
    \textcolor{funct}{Pilot Provisioning} \newline
      \textcolor{funct}{Task Execution} &
    \textcolor{lcomp}{Pilot Manager} \newline
      \textcolor{lcomp}{Task Manager} \\
  \hline
    \textcolor{term}{Multi-level scheduling} &
    \textcolor{funct}{Pilot Provisioning} \newline
      \textcolor{funct}{Task Dispatching} &
    \textcolor{lcomp}{Pilot Manager} \newline
      \textcolor{lcomp}{Workload Manager} \\
  \hline
    \textcolor{term}{Early binding} &
    \textcolor{funct}{Task Dispatching} \newline
      \textcolor{funct}{Pilot Provisioning} &
    \textcolor{lcomp}{Workload Manager} \newline
      \textcolor{lcomp}{Pilot Manager} \\
  \hline
    \textcolor{term}{Late binding} &
    \textcolor{funct}{Task Dispatching} \newline
      \textcolor{funct}{Pilot Provisioning} &
    \textcolor{lcomp}{Workload Manager} \newline
      \textcolor{lcomp}{Pilot Manager} \\
  \hline
 \end{tabular}
 \caption{\textbf{Mapping of the core terminology of \pilot systems into the
     functionalities and logical components described in
     \S\ref{sec:compsandfuncs}.}\msnote{What are the considerations for the
     mapping?}\mtnote{I am not sure I properly understand the comment but
     should we say common sense? We are defining the vocabulary and the logical
     model so I would assume we are fairly unbounded in our mapping choices.}
     \jhanote{The mapping is implicit and the reader will be confused by the
     purpose and will struggle to see the intent other than a listing of all
     terms encountered. I think this table will benefit from a more detailed
     caption.}\mtnote{Done in the text. Is it enough?}}
 \label{table:terminology}
\end{table*}

Table~\ref{table:terminology} offers an overview of the defined minimal and
consistent vocabulary alongside its mapping into both the logical components of
a \pilot system and its required minimal set of functionalities as defined
in~\S\ref{sec:compsandfuncs}. The same mapping is diagrammatically represented
in Figure~\ref{fig:core_vocabulary}.

Relevant to note how scheduling happens both at the infrastructure and at the
pilot level. Accordingly, in Table~\ref{table:terminology} the term
`multi-level scheduling' pertains to both the pilot and workload managers and
their respective pilot provisioning and task dispatching functionalities. In
Figure~\ref{fig:core_vocabulary}, scheduling clearly happens at the level of
the resource pool of an infrastructure, for example by means of a cluster
scheduler, and then of the pilot once it holds its resources.

Table~\ref{table:terminology} and  Figure~\ref{fig:core_vocabulary} help also
to appreciate the critical distinction between the container of a pilot and the
pilot itself. A container, for example a job, is used by the pilot manager to
provision the pilot. Once the pilot has been provisioned, it is the pilot and
not the container to be responsible of both holding a set of resources and
offering the functionalities of the task manager.

Figure~\ref{fig:core_vocabulary} should not be confused with an architectural
diagrams. No indications are given about the interfaces that should be used,
how the logical component should be mapped into software modules, or what type
of communication and coordination protocols should be used among such
components. This is why no distinction is made diagrammatically between early
and late binding. Their difference is temporal and, as such, it can be
highlighted only when describing the succession of the states and operations of
a \pilot system implementation. In the early case, the binding of the tasks to
a pilot will happen before the submission of such a pilot to the remote
infrastructure. In the late case, the binding will happen once the pilot has
been instantiated and holds already its resources.

The wide spectrum of available implementations of the logical components of a
\pilot system is explored in the next section.

\begin{figure}[t]
    \centering
        \includegraphics[width=.48\textwidth]{figures/core_vocabulary.pdf}
    \caption{Diagrammatic representation of the logical components,
             capabilities, and core vocabulary of a \pilot system. The terms of
             the core vocabulary are highlighted in red, those of the logical
             components of a \pilot system in green, and those of their
             functionalities in blue.
             \amnote{'schedule tasks' should move to the 'early/late binding'
             arrow} \mtnote{Also this was altered before me seeing your
             comment. Any better?}
             \amnote{I still think that 'Task Dispatching' is not
                 related to 'Multilevel Scheduling', dispatching
                 follows after scheduling.  The 'Multilevel
                 Scheduling' term should go to the 'Task Binding'
                 functionality, IMHO?}
             \mtnote{Better?}
     }
    \label{fig:core_vocabulary}
\end{figure}

\mtnote{For S3, the TODO list is: 1. Argue explicitly that the offered `model'
is sufficient and necessary in order to discriminate between \pilot and
not-\pilot systems;} \mtnote{This is still somewhat an open issue.}

\mtnote{[\ldots], and possibly another one relative to overlay enacting as
distinguished from task dispatching;} \mtnote{I think this is not relevant
anymore. If you agree please delete this comment.}

\mtnote{3. Clean up Table 1 removing 'Infrastructure'.} \mtnote{I see no
problem with having infrastructure in the table but please let me know whether
you think differently.}

\jhanote{Given that we've introduced Figure 4, a paragraph that summarized this
section by way of explaining Figure 4 would be useful to the reader as well
as provide a soft transition to next section. Thoughts?} \mtnote{Done.}


%------------------------------------------------------------------------------
% SECTION 4
%------------------------------------------------------------------------------
\section{Pilot Systems: Analysis and Implementations}
\label{sec:analysis}

Section \S\ref{sec:understanding} offered two main contributions: (i) the
minimal sets of logical components and functionalities of \pilot systems
\footnote{ From here on we will be internally consistent and use \pilot system
instead of \pilotjob system.}; (ii) and a well-defined core terminology to
support reasoning about such systems. The former defines the necessary and
sufficient requirements for a distributed system to be a \pilot system, while
the latter enables consistency when referring to different \pilot systems. Both
these contributions are used in this Section to review critically a selection
of \pilot systems.

The goal of this Section is twofold. Initially, the \pilot functionalities
presented in \S\ref{sec:history} are used as the basis to infer core \pilot
implementation properties. Auxiliary properties are also defined when useful
for a critical comparison among different \pilot systems. Subsequently, several
\pilot systems are analyzed and then clustered around the properties previously
defined. In this way, insight is offered about how to choose a \pilot system
based on functional requirements, how \pilot systems are designed and
engineered, and the design principles that underly such systems.

% -----------------------------------------------------------------------------
% 4.1
%
\subsection{Core and Auxiliary Properties}
\label{sec:properties}

\jhanote{We should be consistent with UC or LC for section and
subsection. FWIW, I prefer LC when saying ``this section'' but upper case when
we reference a section, i.e. Section 2.}

This subsection \jhanote{do we mean subsection or section?} \mtnote{Done.}
analyzes \jhanote{I don't think we define, we analyze..?} \mtnote{Done.} the
properties of diverse implementations of a \pilot system. Two sets of
properties are introduced: core and auxiliary (see
Table~\ref{table:property_component_mapping}). Both sets of properties are
chosen by considering the implementation requirements of the \pilot
capabilities as defined in~\S\ref{sec:history}, Table~\ref{table:terminology}.
A property belongs to the set of core properties if it is required by the
implementation of one or more of the \pilot functionalities. Auxiliary
properties are instead required only when added functionalities needs to be
implemented for a specific \pilot system. As such, auxiliary properties are not
necessarily shared among all \pilot systems.

% As shown in Table~\ref{table:core_properties}, the core properties are
% derived consistently with the functionalities presented in \S\ref{sec:3} -
% Pilot Provisioning, Task Dispatching, and Task Execution.
\msnote{I dont see how it is derived or consistent ;-)}

\mtnote{Derived and consistent: they are required by (every: core, at least an:
auxiliary) implementation of those functionalities. I rewrote the paragraph
avoiding the two terms but keeping the semantic just explained.}

\mtnote{I have changed and shortened the names of the core properties. After
many months, they made less sense for me that they use to. This is currently
just a suggestion that I would like to discuss during our next call. Once
reached an agreement I will take care of revert/iterate/change naming across
the whole 4.1 (and, in case, 4.2).}

% As such, Core Properties are both necessary and sufficient for an
% implementation of a distributed system to be classified as a
% \pilot system. On the contrary, auxiliary properties are not defining of a
% \pilot system but they are implementation details that further characterize a
% \pilot system.  The set of auxillary properties we discuss is not closed,
% i.e., the complete set of auxillary properties includes, but is not limited
% to the set of auxillary properties we discuss.

\msnote{TODO: Fix/merge the two non-compatible definitions of aux props}
\mtnote{Not sure it is still the case. Will address once we will have decided
whether to keep the changes I made.}

\begin{table*}
\centering
\begin{tabular}{c|c|p{3cm}|p{5cm}|}
\cline{2-4}
& \textbf{Property} & \textbf{Component} & \textbf{Description} \\
\hline
% Core
\multirow{6}{*}{\textit{Core}}
  & Pilot Resources & Pilot Manager &
  Types and capabilities of the resources held by the pilot. \\
  \cline{2-4}
  & Pilot Deployment & Pilot Manager &
  Modalities and protocols for scheduling and aggregation of pilots. \\
  \cline{2-4}
  & Workload Semantics & Workload Manager &
  The specification of the semantics (between tasks) captured in the workload
  description. \\
  \cline{2-4}
  & Task Binding & Workload Manager &
  Modalities and policies for binding tasks to pilots. \\
  \cline{2-4}
  & Task Execution & Task Manager &
  Types of tasks and the mechanisms to execute tasks. \\
  \cline{2-4}
  & Infrastructure Interaction & Pilot Manager &
  Modalities and protocols used to coordinate the pilot system/infrastructure
  interaction. \\
\hline
% Auxiliary
\multirow{10}{*}{\textit{Auxiliary}}
  & Architecture &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    Frameworks and architecture that the components and their whole are build
    with.\\
  \cline{2-4}
  & Coordination and Communication &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    The interaction between the components of the system.\\
  \cline{2-4}
  & Interface &
    Pilot Manager\newline Workload Manager &
    Interface that the user can use to interact with the system.\\
  \cline{2-4}
  & Interoperability &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    Interoperability between \pilots on multiple DCIs.\\
  \cline{2-4}
  & Multitenancy &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    The use of components by multiple (simultaneous) users.\\
  \cline{2-4}
  & Resource Overlay &
    Pilot Manager\newline Workload Manager &
    The aggregation of resources from multiple pilots into overlays.\\
  \cline{2-4}
  & Robustness &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    The measures in place to increase the robustness of the components and the
    whole.\\
  \cline{2-4}
  & Security &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    AAA considerations for the components and the whole.\\
  \cline{2-4}
  & Files and Data &
    Pilot Manager\newline Workload Manager &
    The mechanisms that the system offers to explicitly deal with files and
    data.\\
  \cline{2-4}
  & Performance and Scalability &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    A description of scale and limitations and measures to reach that.\\
  \cline{2-4}
  & Development Model &
    Pilot Manager\newline Workload Manager\newline Task Manager &
    The development- and support model for the software.\\
\hline
\end{tabular}
\caption{\textbf{Mapping of the Properties of \pilot system implementations
onto the components described in \S\ref{sec:compsandfuncs}.}}
\label{table:property_component_mapping}
\end{table*}

\jhanote{I found it hard to read through the next three paragraphs. Two reasons:
  general style: too many break outs into parenthesis etc. That should be easy
  to fix. Second and more important: We're forward referencing Table 2 and
  thereby core and auxillary properties without having discussed either. One
  temporary solution -- I am sure it has its own drawbacks is to move these 3
  paragraphs to the end of 4.1.1. Lets discuss.}


Implementations of the Pilot Provisioning functionality are analyzed by focusing
on two properties: (i) the type of resources the pilot system exposes such as
compute, data, and networking (Table~\ref{table:property_component_mapping},
Pilot Resources); and (ii) the modalities and protocols used by the \pilot
system and required by the target infrastructure(s) to schedule, utilize, and
aggregate pilots (Table~\ref{table:property_component_mapping}, Pilot Management
and Infrastructure Interaction \jhanote{there is no pilot management in
  properties}). For example, to schedule a pilot onto a specific infrastructure
\pilot systems need to know what type of container to use (e.g.  job, virtual
machine), what type of scheduler the resource exposes, but also the amount of
cores and for how long they can be asked for, whether a low-latency interconnect
is available, or specific filesystems can be accessed.

Two properties are also used to analyze the implementations of Task
Dispatching: (i) the semantics of workloads
(Table~\ref{table:property_component_mapping}, Workload Semantics); and (ii)
how the tasks of a given workload can be bound to single or multiple pilots
(Table~\ref{table:property_component_mapping}, Task Binding). Semantically, a
workload description contains all the information necessary for it to be
dispatched to the appropriate resource. For example, type, number, size,
duration of tasks alongside their grouping into stages or their data
dependences need to be known when deciding how many resources of a specific
type should be used to execute the given workload but also for how long such
resources should be available. Executing a workload requires for its tasks to
be bound to the resources. Both the temporal and spatial dimensions of the
binding operations are relevant for the implementation of Task Dispatching.
Depending on the concurrency of a given workload, tasks could be dispatched to
one or more pilots for an efficient execution. Furthermore, tasks could be
bound to pilots before or after its instantiation, depending on resource
availability and scheduling decisions.

Finally, implementations of Task Execution are analyzed by reviewing different
strategies for task scheduling and by describing how \pilot systems support
task execution (Table~\ref{table:property_component_mapping}, Task Execution).
\pilot implementations may offer multiple scheduling strategies depending on
varying factors related to the nature of the workload, the state of the
resources, or the capabilities exposed by the underlying middleware.
Furthermore, \pilot systems often are responsible for setting up the execution
environment for the tasks of the given workload. While each task can be seen as
a self-contained and self-sufficient unit with a kernel ready to be executed on
the underlying architecture, often tasks require their environment to be set up
so that some libraries, data, or accessory programs are made available.

Several auxiliary properties play a fundamental role in distinguishing
\pilot systems implementations, as well as addressing, setting and providing
constraints on their usability (Table~\ref{table:property_component_mapping},
Auxiliary). Programming and user interfaces; interoperability across differing
middleware and other \pilot systems; multitenancy; strategies and abstractions
for data management; security including policies alongside authentication and
authorization; support for multiple usage modes like HPC or HTC; or robustness
in terms of fault-tolerance and high-availability; are all examples of
properties that might characterize a \pilot implementation but that, in of
themselves, would not distinguish a \pilot as a unique system.

Both core and auxiliary properties have a direct impact on the multiple use
cases for which \pilot systems are currently engineered and deployed. For
example, while every \pilot system offers the opportunity to schedule the tasks
of a workload on a pilot, the degree of support of specific workloads varies
vastly across implementations. Furthermore, some \pilot systems support Virtual
Organizations and running tasks from multiple users on a single pilot while
others support jobs using a Message Passing Interface (MPI). Analogously, all
\pilot systems support the execution of one or more type of workload but they
differ when considering execution modalities that maximize throughput (HTC),
computing (HPC) or container-based high scalability (Cloud).

\subsubsection{Core properties}
\label{sec:coreprops}

Following is the list of core properties. This list of properties is minimal
and complete. Note that these are the properties of \pilot implementations, and
not of (individual) instantiations of \pilots.

\amnote{I don't think this list is 'minimal and complete', there are
    many properties of any software which are 'core' in the definition given
    above.  Programming language.  Code license.  Year of implementation.
    Those are properties needed to implement one or more functionalities,
    obviously.  I guess that those (and other) properties are not interesting
    in this context?  Can you motivate why the chosen properties are
    intersting?  Possibly they mostly apply *only* to pilot systems?}
    \mtnote{Correct. This is not a minimal and complete list of software
    properties but of \pilot system implementations.}

\begin{itemize}

%core because: it says something elementary about the principle element
\item \textbf{Pilot Resources}. Usually, pilots expose compute resources but,
  depending on the capabilities offered by the infrastructure where the pilot
  is instantiated, pilots might also expose data and network resources. Some of
  the typical characteristics of pilots resources are: size (e.g. number of
  cores), lifespan, intercommunication (e.g. low-latency or inter-domain),
  computing platforms (e.g. x86, or GPU), file systems (e.g. local, shared, or
  distributed). The coupling between pilot and the resources that it holds may
  vary depending on the architecture of the resources in which it is
  instantiated. For example, a pilot may bind multiple compute nodes, single
  nodes, or portion of the cores of each node. The same applies to file systems
  and its partitions or to software defined or physical network resources.

  % Another consideration in discussing \pilot resource capabilities is the
  % notion of granularity, e.g. is there one \pilot for a set of resources
  % within an infrastructure or is there a more fine grained coupling between
  % \pilot and resource and how does this effect the suitability for HTC and/or
  % HPC?

  \msnote{This should not overlap with Task Execution Modes}

  % \item \textit{Pilot Granularity (spatial)}: What unit of resource is
  % controlled by a single pilot? A core, a node, multiple nodes, a cluster,
  % ...?

% core because: its fundamental to our model
\item \textbf{Pilot Deployment}. Pilots are scheduled and then bootstrapped on
  the remote infrastructures. The characteristic of both operations varies
  depending on both the implementation details of the \pilot systems and the
  architecture, interfaces, and capabilities offered by the remote
  infrastructures. For example, pilot scheduling may be fully automated or
  directly controlled by applications and end-users. The bootstrapping can
  offer explicit or implicit localization by allowing to load specific
  libraries, compilers, and support software. Both scheduling and bootstrapping
  varies depending on whether the remote infrastructures expose HPC, grid, or
  cloud interfaces.

  % resource acquisition/release policies, (e.g. the automatic provisioning of
  % \pilots).
  % \item \textit{Pilot instantiation (spatial+temporal)}: When are pilots
  % instantiated where and which entity (system/user) has control over it at
  % which level?

% core because: both tasks and workload are core components
\item \textbf{Workload Semantics}. The tasks of a workload are dispatched to
  pilots depending on their semantics. Specifically, dispatching decisions
  depends on the relationships among tasks, the affinity between data and
  compute resources required by the tasks, and the type of capabilities needed
  for their execution. \pilot systems support a varying degree of semantic
  richness for the workload and its tasks. Additionally of interest is the
  (standard) format or language in which the workloads are described.

  \amnote{why are task dependencies (which seems to be what you call
  'workload semantics') a core property?  A pilot system is still a pilot
  system if it does not implement task dependencies (ahem! ;-).}
  \mtnote{A pilot system implementation without tasks is a useless pilot
  system. In this section we are not modeling (that was done in 3) but
  reviewing/analyzing existing, real world implementations designed and used to
  run tasks.}

% core because: this is of the heart of pilot systems
\item \textbf{Task Binding}. The Task Dispatching functionality implies the
  capability of binding tasks to pilots. Without such a capability, it would
  not be possible to know where to dispatch tasks, \pilots could not be used to
  execute tasks and, as such, the whole \pilot system would not be usable. As
  seen in \S\ref{sec:understanding}, \pilot systems may allow for two types of
  binding between tasks and \pilots: early binding and late binding. \pilot
  system implementations differ in whether and how they support these two types
  of binding. Specifically, while there might be implementations that only
  support a single type of binding, they might also differ in whether they
  allow for the users to control directly what type of binding is performed,
  and in whether both types of binding are available on an heterogeneous pool
  of resources. Besides the binary decision between early and late binding, the
  \pilot system can expose, for example, more detailed application-level
  scheduling decisions, dispatch policies, or even include more levels of
  scheduling.

  % does the framework support advanced workload management capabilities
  % \item \textit{Task-pilot relationship (temporal)}: What is the lifetime of
  % a pilot in relation to the length of a task or workload? \msnote{What kind
  % of answers to you expect here in section 4.3?}
  % \item textit{Task-pilot relationship (spatial)}: Does a pilot execute one
  % or more tasks concurrently?

% core because: its the only property of the task executor
% component/functionality
\item \textbf{Task Execution}. Once the tasks are dispatched to a pilot, their
  execution may require for a specific environment to be set up. \pilot systems
  differ in whether and how they offer such a capability. \pilot systems may
  adopt dedicated components for managing execution environments, or they may
  rely on ad hoc configuration of the pilots. Furthermore, execution
  environments can be of varying complexity, depending on whether the \pilot
  system allows for data retrieval, dedicated software and library
  installations, communication and coordination among multiple execution
  environment and, in case, pilots.

  % \item textit{Supported ``physical'' task types}: What types of (physical)
  % tasks are supported, e.g., single-core, MPI, ...?

% core because: its a must-have to do pilot provision
% (alternative view: aux because its an implementation detail \ldots)
\item \textbf{Infrastructure Interaction}. \pilot systems interact with remote
  infrastructures at multiple levels. The degree of coupling between the \pilot
  system and the infrastructure can vary as much as the information shared
  between them. Depending on the capabilities implemented, \pilot systems have
  to negotiate the scheduling on \pilots, may be staging data in and out of the
  infrastructure, and may have to mediate task binding and execution by means
  of remote interfaces and protocols.

  \msnote{This should not overlap with Task Execution Modes} \mtnote{Is this
  still valid?}

  % \item \textit{Supported DCI systems/services}: What are the type of systems
  % (middlewares, services) that can be interfaced by the \pilotjob system to
  % launch pilots?
  % \item \textit{Supported Middleware:} What middleware does the system
  % connect with? Is it tightly coupled or flexible/extensible?
  % \item Supported Middleware (Auxiliary)

\end{itemize}


\subsubsection{Auxiliary properties}
\label{sec:auxprops}

\begin{itemize}

% aux because: its an "implementation" detail
\item \textbf{Architecture}. \pilot systems may be implemented by means of
  different type of architectures (e.g service-oriented, client-server, or
  peer-to-peer). Architectural choices may depend on multiple factors,
  including use cases, deployment strategies, or interoperability requirements.
  % For example, it is conceivable that architectural choices influence if not
  % preclude certain deployment strategies.
  The analysis and comparison of architectural choices is here limited to the
  trade-offs implied by such a choice, especially when considering how they
  affect the Core Properties.

  % \item \textit{Transfer Protocols}: What network protocols that are used to
  % connect the individual component of the \pilot system?

% (v) does the framework have dependencies to other third-party
% components/services?

% Further non-functional aspects describe the internals of a \pilotjob system,
% e.\,g.: (i) the architecture of the system (layers, sub-systems,
% communication \& coordination, central vs. decentral architectures; push vs.
% pull model, agent-based; number of supported resource types),

% aux because: its an implementation detail not directly coupled to one of the
% components or functions
\item \textbf{Communication and Coordination}. Communication and coordination
  are a features of every distributed system. In \ref{sec:compsandfuncs} it was
  suggested that \pilot systems are not defined by any specific communication
  or coordination pattern or protocol. The details of communication and
  coordination among the \pilot system components are distinguishing
  implementation properties.

% aux because: its an implementation detail
\item \textbf{Interface}. \pilot systems may present several types of
  interfaces: among the components of the \pilot system; between the
  application and the \pilot system; or between end users and one or more
  programming language interfaces for the \pilot system. Here the focus is on
  the interfaces exposed by the \pilot system instead of those justified by the
  internal architectural choices.\mtnote{Why?}

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Interoperability}. Two types of interoperability are
  relevant when analyzing different \pilot systems: interoperability with
  heterogeneous infrastructures and interoperability across diverse
  \pilot systems. The former allows for a \pilot system to provision pilots
  and execute workloads on different types of infrastructure (e.g. HTC, HPC,
  Cloud but also Condor, LSF, Slurm, or Torque). The latter is relevant when
  multiple \pilot systems are available with diversified characteristics and
  capabilities.\mrnote{Note from meeting with SJ: May want to change to discuss
  one first (including descriptive text), then change tracks to discuss the
  second - they are two very different concepts to grasp in the first sentence
  (I know this detracts from the mathematical/proof language of it, but I think
  its a good idea). Especially since the second type of interoperability is
  very much less-common to a single PJ system (except in cases like \panda)}
  \mtnote{Any better?}


  % (ii) resource access layer: What low-level infrastructures/middleware are
  % supported? How interoperable is the framework (vertically, horizontally)?,

  % Does the \pilotjob system provide interoperability support?

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Multitenancy}. \pilot systems may offer multitenancy at both
  system and local level. When offered at system level, multiple users are
  allow to utilize the same instance of a \pilot system. When available at
  local level, multiple users may share the same pilot.
  \msnote{Alternative names: deployment, service model}\mtnote{Any better? I
  think `deployment' is now covered by the homonymous core property and
  `service model' is part of the `architecture' auxiliary property.}

  % \item \textit{Multi-user support:} Does the pilot job system allows
  % \textbf{\pilots} to
  % be used by multiple users? (iii) the
  % deployment model: hosted service versus tool/library, application vs.
  % system-level,

  % \item \textit{Pilot ownership}: Who owns the pilot (job) -- the user or the
  % \pilotjob system? Or in other words, in which security context does the
  % pilot operate? \msnote{Dont think we have defined "user" by now, so thats
  % ambiguous}
  % \msnote{As there will be a wide spectrum of approaches in the various
  % implementation, we might just want to call this as general as "security"}
  % \aznote{I think this covers potentially multiple concepts: 1) security, as
  % in the level of direct access users have to pilots (can they for instance
  % force a kill directly?  Modify pilot configurations while it runs?) 2)
  % Deployment, as in does the entire pilot-job system operate in user-space or
  % is it part of the (admin/privileged) system stack? 3) I would argue that
  % this could relate directly to pilot-instantiation... both from a security
  % and deployment level. Who can instantiate pilots? What security level do
  % they run on once instantiated?  So, maybe a ``security'' item as Mark
  % suggested, but I'm not entirely sure if ``deployment'' should be part of
  % this as well...  they do seem connected to me though...} Further, some
  % \pilotjobs solely support single users, while more complex
  % \pilot-based workload managers commonly support multiple users
  % (e.\,g.\ using glexec).

% aux because: not all the pilot systems offer an abstraction for multiple (at least two) pilots.
\item \textbf{Resource Overlay}. The resources of multiple pilots may be
  aggregated into a resource overlay. Overlays may be directly exposed to the
  application layer and to the the end-users depending on the exposed
  interfaces and usability models. Overlays may abstract away the notion of
  pilot or offer an explicit semantic for their aggregation, selection, and
  management.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Robustness}. Used to identify those properties that contribute
  towards the resilience and the reliability of a \pilot system. In this
  section, the analysis focuses on fault-tolerance, high-availability, and
  state persistence. These properties are considered indicators of both the
  maturity of the development stage of the \pilot system implementation, and
  the type of support offered to the paradigmatic use cases introduced in
  Section 2.\mtnote{I do not think use cases are presented in Section 2
  anymore. Maybe in the introduction or should we add a subsection to 2?}

  % \item \textit{Fault-Tolerance}: What mechanisms are in place to shield the
  % system from component failures?

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Security}. Security of \pilot system implementations related to
  security would require a dedicated analysis, we limit the discussion to
  authentication, authorization and policies. The scope of the analysis is
  further constrained by focusing only on those elements that impact the Core
  Functionalities as defined in \S\ref{sec:compsandfuncs}.

  % \item \textit{Security}: How is a user identified in the system and how are
  % identities and access credentials delegated to individual resources /
  % pilots? Security describes the security mechanisms used by the framework,
  % e.\,g.\ for authentication of the user, for securing the communication
  % protocol and for sandboxing the application.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Files and Data}. As seen in Section~\ref{sec:compsandfuncs}, only
  basic data reading/writing functionalities are necessary for a \pilot system.
  Nonetheless, most of the user cases require more advanced data management
  functionalities that can be implemented within the \pilot system of delegated
  to third party tools.\mtnote{Should this be just `Data'? A file is a specific example of data container, not necessarily the only one a pilot might support.}

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Performance and scalability}. \pilot systems varies both in
  terms of overheads they add to the execution of a given workload and for the
  size and duration of the workloads a user can expect to be supported.
  Furthermore, \pilot systems can be designed specifically for a performance
  metric or for a type of workloads and/or community requirements.

% aux because: its not directly coupled to one of the components/functions
\item \textbf{Development Model}. The model used to develop \pilot systems is a
  distinguishing element, especially when considering whether the development
  is supported by a open community or by a specific project. Different
  development models have an impact on the life span of the \pilot system, its
  maintainability and, in case, evolution path.

\end{itemize}


\begin{table*}[t]
 \up
 \centering
 \begin{tabular}{|p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
    \textbf{Pilot\newline System} &
    \textit{Pilot\newline Resources} &
    \textit{Resource\newline Interaction} &
    \textit{Pilot\newline Deployment} &
    \textit{Workload\newline Semantics} &
    \textit{Task\newline Binding} &
    \textit{Task\newline Execution} \\
  \hline
  \hline
    DIANE &
    HTC &
    GANGA &
    Out-of-Band / explicit &
    Programmable &
    Late &
    Serial \\
  \hline
    DIRAC &
    HTC &
    Custom &
    Community Service / implicit &
    None (Data dependencies?) &
    Late &
    Serial, some MPI \\
  \hline
    Falkon &
    HPC &
    Unspecified &
    Web Service &
    None &
    Late (mixed push/pull) &
    Serial \\
  \hline
    HTCondor &
    HTC (and to some degree HPC) &
    Condor-G &
    Explicit in Glidein case &
    Graph &
    Late &
    All \\
  \hline
    MyCluster &
    HPC &
    Custom (SGE / PBS / HTCondor) &
    CLI tools from respective LRMS &
    Workload semantics from respective LRMS &
    Agnostic &
    All \\
  \hline
    \panda &
    HTC &
    Custom, SAGA &
    Community Service / implicit &
    Task type, priority &
    Late &
    Serial, some MPI \\
  \hline
    RADICAL-Pilot &
    HPC &
    SAGA &
    Programmable / explicit &
    Programmable &
    Early \& Late &
    Serial \& MPI \\
 \hline
 \end{tabular}
 \caption{\textbf{Overview of \pilot systems and a summary their core properties.}}
 \label{table:implementations-properties}
\end{table*}


% -----------------------------------------------------------------------------
% 4.2
%
\subsection{Analysis of \pilot system Implementations}
\label{sec:implementations}

In light of the common vocabulary discussion in \S\ref{sec:termsdefs}, a
representative set of \pilot systems has been chosen for further analysis.
Examining these \pilot systems using the common vocabulary exposes their core
similarities, and allows a detailed analysis of their differences.

The now following discussion of \pilot systems is ordered alphabetically.
To assist the reader, we make use of textual conventions: in \vocab{Bold} we
express the \vocab{Logical Components} and \vocab{Functionalities} of our model
from \S\ref{sec:compsandfuncs} and the \vocab{Terms and Definitions} from
\S\ref{sec:termsdefs}, in \prop{Italic} we refer to the \prop{Properties} from
\S\ref{sec:properties} and in \impterm{Typewriter} we display terminology from
the respective \pilot system under discussion.\mtnote{Do we need this
formatting devices? They are not always added to every use of the terms and
they make the text a bit `heavy' for me.}

% \jhanote{this might go to the section where we begin describing
%   pilotjobs.. also will need an updated description of the methodology
%   employed to choose} ....such that ``exemplars'' were chosen; these
% are \pilotjobs which either laid foundational \pilotjob concept or
% incorporated strides in interoperability, usability, architecture or
% otherwise to advance the understanding and usage of \pilotjobs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Coasters
%
%\subsubsection{Coasters}
% AS PER SJ P* CALL: COASTERS SECTION IS CANCELED
% mrnote: Reason - insufficient documentation, no citations to actual science problems
% msnote: We probably want to revisit this decision, but not today :-)
%\subsubsection{Coasters}
%
%\msnote{Main coasters ref: \url{http://www.ci.uchicago.edu/swift/papers/UCC-coasters.pdf}}
%\mrnote{Coasters wiki: \url{http://wiki.cogkit.org/wiki/Coasters}}
%
%The Coaster System (or "Coasters") is a Java CoG based Pilot-Job system
%created for the needs of the Swift parallel scripting language.
%
%\paragraph{Design Goals}
%
%\begin{itemize}
%  \item Driven by the needs of Swift
%  \item Automatically-deployed to endpoint. Does not need user login to endpoint.
%  \item Supports file staging, on-demand opportunistic multi-node allocation,
%  remote log gin, and remote monitoring
%\end{itemize}
%
%\paragraph{Applications}
%
%\begin{itemize}
%  \item It has been used since 2009 for applications in fields
%that include biochemistry, earth systems science,
%energy modeling, and neuroscience.
%  \item Above claim made in Coasters paper - need to obtain citations
%\end{itemize}
%
% Swift~\cite{Wilde2011} is a scripting language designed for expressing abstract
% rest of this cut, but making a note of this in case we want
% to bring swift into the discussion later that i can find more info in 2011 paper
%% Swift~\cite{Wilde2011} is a scripting language designed for expressing
%% abstract workflows and computations. The language provides, amongst many other
%% things, capabilities for executing external applications, as well as the
%% implicit management of data flows between application tasks.
%% % For this
%% % purpose, Swift formalizes the way that applications can define
%% % data-dependencies. Using so called mappers, these dependencies can be
%% % easily extended to files or groups of files.
%% The runtime environment handles the allocation of resources and the spawning of
%% the compute tasks.
%% % Both data- and execution management capabilities are provided
%% % via abstract interfaces.
%% Swift supports e.\,g.\ Globus, Condor and PBS resources.
%% % The pool of resources
%% % that is used for an application is statically defined in a configuration file.
%% % While this configuration file can refer to highly dynamic resources (such as OSG
%% % resources), there is no possibility to manage this resource pool
%% % programmatically.
%% By default, Swift uses a 1:1 mapping for \cus and \sus. However,
%% Swift supports the grouping of SUs as well as PJs. For the PJ functionality, Swift uses the
%% Coaster~\cite{coasters} framework. Coaster relies on a master/worker
%% coordination model; communication is implemented using GSI-secured TCP sockets.
%% Swift and Coaster support various scheduling mechanisms, e.\,g.\ a FIFO and a
%% load-aware scheduler. Additionally, Swift can be used in conjunction with
%% Falkon~\cite{1362680}, which also provides \pilot-like functionality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIANE
%
\subsubsection{DIANE}
\label{sec:diane}

DIANE~\cite{Moscicki:908910} is a task coordination framework, which follows
the \MW pattern. It was developed at CERN for data analysis in the
LHC experiment, but has since been used in various other domains, though mainly
in the Life Sciences. As DIANE is a software framework, some of its semantics
are not predetermined, as they can be implemented in various ways depending on
the user requirements. In this way, DIANE also provides \pilot functionality
for job-style executions.\mtnote{I am not sure I understand what a `job-style'
execution is. Do we need to make it explicit as we did not define the term
before?}

\paragraph{Resource Interaction}
The \pilot provisioning with GANGA~\cite{Moscicki20092303} provides a unified
interface for job submissions to various resource types\mtnote{do we need
examples of these types of resource?}. In this way, DCI specifics are hidden
from other components of DIANE as well as from the user.

\paragraph{Overlay Management}
The \prop{Overlay Management} of \pilots is done out-of-band by means of the
\impterm{diane-submitter} script which can launch a \pilot on any supported
infrastructure.

\paragraph{Workload Semantics}
DIANE is mainly a \MW framework and, as such, the \vocab{Tasks} are
fully independent, consistently with a Bag-of-Tasks \vocab{Workload Semantics}.
The \vocab{Workload} that is managed by the application-specific component of
DIANE can freely structured, as long as it can be translated to independent
tasks\cite{diane-dag, diane-etc}. Plugins for other types of workload (e.\,g.\
DAGs or for data-intensive applications) exist or are under development. The
framework is extensible: for example, applications can implement a custom
application-level scheduler.

\paragraph{Coordination and Communication}
The \prop{Coordination \& Communication} in DIANE is based on
CORBA~\cite{OMG-CORBA303:2004} and uses TCP/IP. The CORBA layer is invisible to
the application layer. Networking-wise, the workers are clients of the master
server. On TCP/IP level, communication is always unidirectional from the
\impterm{WorkerAgent} to the \impterm{RunMaster}. \prop{Security} is provided
by a secret token that the \impterm{WorkerAgent} needs to communicate back to
the \impterm{RunMaster}. This implies that the \impterm{WorkerAgent} needs to
be able to reach the \impterm{RunMaster} via TCP/IP but not the other way
around. Bidirectional communication is achieved by periodic polling through
heartbeats by the \impterm{WorkerAgent}, where the \impterm{RunMaster} responds
with feedback.

\paragraph{Task Binding}
DIANE is primarily designed for HTC environments (such as EGI~\cite{egi}),
i.\,e.\ one \pilot consists of a single worker agent with the size of 1 core.
Although the semantics of the binding are ultimately controllable by the
user-programmable scheduler, the general architecture is consistent with a pull
model. The pull model naturally implements the late-binding paradigm as every
worker pulls a new task once it is available and has free resources.

\paragraph{Task Execution Modes}
DIANE is primarily designed for HTC environments (such as EGI~\cite{egi}),
i.\,e.\ one \pilot consists of a single worker agent with the typical size of 1
core/node. As such, DIANA is not able by default to run (widely) parallel
applications as those based, for example, on MPI.

\paragraph{Pilot Resource Capabilities}
DIANE implements the notion of \mtnote{resource?} ``capacity'', a property that
applications can use to diverge from the ``1 worker = 1 core''
mode.\mtnote{Should we make explicit what this divergence is and implies?}

\paragraph{Architecture}
The central component of DIANE is the \impterm{RunMaster}, consisting of a
\impterm{TaskScheduler} and an \impterm{ApplicationManager} module. Both
modules are abstract classes that need to be implemented for the specific
purpose at hand. Examples and/or default implementations are provided to the
user as guidelines and initial code stubs. Together the \impterm{TaskScheduler}
and \impterm{ApplicationManager} implement the \vocab{Workload Manager}
component.

The \impterm{TaskScheduler} keeps track of the task entries and is responsible
for \vocab{Binding Tasks} to the \impterm{ApplicationWorkers}. The
implementation of the \impterm{ApplicationManager} is responsible for defining
the application \vocab{Workload} and for describing the \vocab{Tasks} that are
passed to the \vocab{TaskScheduler}. As the \impterm{ApplicationWorker} asks
the \impterm{TaskScheduler} for new \vocab{Tasks}, the natural way of
implementing the scheduler is by following a \vocab{Late Binding} approach.

\vocab{\pilots} in DIANE are \impterm{WorkerAgents}. The core component of a
\impterm{WorkerAgent} is the \impterm{ApplicationWorker}, an abstract class
that defines three methods that need to be implemented by every implementation.
Two of these methods are for initialization and cleanup while the third method,
(\impterm{do\_work()}) receives the task description and executes the work.
\impterm{do\_work()} is therefore an implementation of the
\vocab{Task Manager} component.

\paragraph{Interface}
DIANE's \prop{Architecture} is based on the \impterm{Inversion of Control}
design pattern. Implemented in Python, it exposes well-defined hooks for
\vocab{Application} programming. The aforementioned abstract classes need to be
implemented to provide the application-specific semantics. The \prop{Interface}
to these ``DIANE-\vocab{Applications}'' is to start them through the
\impterm{diane-run} command\mtnote{I am not sure I understand this sentence}.
As discussed in the \prop{Overlay Management}, the creation of \pilots is done
out-of-band.

\paragraph{Performance and Scalability}
The authors report that in first instance they had implemented full
bi-directional communication, but that turned out to be difficult to correctly
implement and created scalability limitations.\mtnote{In absence of more
precise data about the performance profile of DIANE and it scalability
boundaries, I would delete this paragraph.}

\paragraph{Robustness}
The \prop{Robustness} in DIANE comes from the mature CORBA communication layer,
and from the custom task-failure policies implemented in the
\impterm{TaskScheduler}. \prop{Robustness} is achieved in DIANE by: (i)
offering mechanisms to supports fault tolerance: basic error detection and
propagation mechanisms are in place; (ii) implementing an automatic
re-execution of tasks; and (iii) leaving room for application specific
strategies.

\paragraph{Files and Data}
The CORBA communication channel between the Master and the Worker also allows
for file transfers, meaning that the \pilot not only exposes cores, but also
the file system on the worker nodes.

\paragraph{Interoperability and Security}
\prop{Interoperability} to various middleware security mechanisms
(e.\,g.\ GSI, X509 authentication) and backends is achieved through its
integration with GANGA.

\paragraph{Multitenancy}
DIANE is a single-user \pilot system, i.\,e.\ each \pilot is executed with the
privileges of the respective user. Also, only workload of this respective user
can be executed by DIANE. However, \prop{Multitenancy} with DIANE does happen
when it is used as a backend for Science Gateways, where it serves multiple
users on the same installation, but with a single credential.\mtnote{I would
argue that multitenancy is achieved by the application layer and not by DIANE.}

\paragraph{Development Model}
DIANE is developed, maintained, and used by the CERN community with external
contributions and users.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIRAC
%
\subsubsection{DIRAC}
\label{sec:dirac}

DIRAC (Distributed Infrastructure with Remote Agent Control) is a software
product developed by the CERN LHCb project\cite{diracgrid2004}. DIRAC
implements a \impterm{Workload Management System} (WMS) to manage the
computational payload of its community members, and the DIRAC Pilot framework
to execute that payload.\mtnote{Should we had details about the architecture of
the pilot framework? For example, we speak about a Pilot Director and a DIRAC
Agent.}

DIRAC's claim to fame is that ``[\ldots] the LHCb data production run in summer
2004 was the first successful demonstration of the massive usage of the LCG
grid resources. This was largely due to the use of the DIRAC Workload
Management System, which boosted significantly the efficiency of the production
jobs.''~\cite{Tsaregorodtsev:2010cj}

\paragraph{Pilot Resource Capabilities}

The bootstrapping of the \impterm{DIRAC Agent} performs a full DIRAC
installation including a download of the most current version of the
configuration files. After completion of the bootstrapping, the installer
checks for the working conditions (e.g. where execution takes place, available
resources, etc.) and then a \impterm{DIRAC Agent} is started.

\paragraph{Resource Interaction}

\pilots are sent to the resource by the \impterm{Pilot Director} and all
resource interaction is hidden behind that abstraction \mtnote{Is the Pilot
Director an abstraction or a (implemented) module of the Pilot Framework?}. The
Pilot Director is reported \mtnote{Should we just say that it is able? Do we
have doubts about what is reported?} to be able to interact with different
batch systems like PBS/Torque, LSF, SGE and BQS.

\paragraph{Overlay Management}

\pilots are being launched by the \impterm{Pilot Directors} to the supported
DCIs. The DIRAC \pilots create a resource overlay that presents a homogeneous
layer to the other components. The user has no control over where \pilots are
started.

\paragraph{Workload Semantics}

The user \vocab{Workload} (in DIRAC terminology: \impterm{payloads}),
consisting of independent \vocab{Tasks} have varying levels of priorities for
LHCb as a whole, as well as a variety of requirements for them to execute.
Additionally, users can specify requirements like needed input data for their
tasks and this will be taken into account when placing the task on a
resource\mtnote{pilot?}.

\paragraph{Task Binding Characteristics}

Pilots implement a pull scheduling paradigm. Once they are running on the
computing resource, they contact central WMS servers for a late binding of the
resource to the payload.

When \vocab{Tasks} are assigned to the \impterm{WMS}, a consistency check is
performed and optional input data requirements are
translated\mtnote{transferred? Staged?} to resource candidates.

Once this is done, \vocab{Tasks} are placed into \impterm{TaskQueues}.
\impterm{TaskQueues} are sorted groups of \impterm{Payloads} with identical
requirements (e.g., user identity, number of cores, etc.) that are ready to be
executed.

\impterm{TasqQueue Directors} direct the workload from the \impterm{TaskQueues}
for execution to the Job Agents\mtnote{Are these the same as DIRAC Agents?}.

\paragraph{Task Execution Modes}

Once the \vocab{Task} has been pulled by a pilot, it is executed by means of a
\impterm{Job Wrapper}. Given the nature of the resources DIRAC is aimed at,
\vocab{Task} are generally single or few-core tasks. Work on supporting MPI is
ongoing\cite{}.

\paragraph{Coordination and Communication}

The \impterm{DIRAC Agent} is responsible for sending the \impterm{payload}
request to the central DIRAC WMS server and for the subsequent execution of the
received \impterm{payload}. The client/service communication is performed with
a light-weight protocol (XML-RPC) with additional standard-based authentication
mechanisms.

\paragraph{Architecture}

The design of DIRAC relies on three main components: the \impterm{DIRAC Agent}
\impterm{TaskQueue}, and \impterm{TaskQueue Director}.\mtnote{Should we mode
this at the beginning of the description and add all the other components we
use in the subsections?}

\paragraph{Interface}

The DIRAC project provides many command line tools as well as a comprehensive
Python API to be used in a scripting environment. In DIRAC3 a full-featured
graphical Web Portal was developed.

\paragraph{Interoperability}

DIRAC is not inter-operable with other \pilot systems but supports execution of
\pilots on a variety of computing resources by means of the \impterm{Pilot
Director}.

\paragraph{Multitenancy}

The community nature of the \impterm{DIRAC WMS} makes it an intrinsically
multi-user system. The WMS holds the workload for multiple users and the pilot
agents are shared amongst the workload of multiple user.

\paragraph{Robustness}

The \impterm{Job Wrapper} creates a uniform environment to execute
\vocab{Tasks} independent of the \vocab{DCI} where they run.
Furthermore, it retrieves the input sandbox, checks availability of required
input data and software, executes the payload, reports success or failure of
the execution, and finally uploads output sandbox and output data if required.

At the same time it also instantiates a \impterm{Watchdog} to monitor the
proper behavior of the \impterm{Job Wrapper}. The watchdog checks periodically
the situation of the \impterm{Job Wrapper}, takes actions in case the disk or
available CPU is about to be exhausted or the payload stalls, and reports to
the central \impterm{WMS}. It can also execute management commands received
from the central service, e.g. to terminate the \impterm{Payload} execution.

\paragraph{Security}

The DIRAC Secure client/service framework called \impterm{DISET} (DIRAC SEcure
Transport) is full-featured to implement efficient distributed systems in the
grid environment\cite{}. It provides X509 based authentication and a
fine-grained authorization scheme. Additionally, the security framework
includes logging service that provides history up to 90 days to investigate
security incidents.

\paragraph{Files and Data}

Tasks within DIRAC that specify their input data are specifically managed. The
WMS ensures that tasks are only started on resources where that data is
available and it makes sure that that data becomes available.

\paragraph{Performance and Scalability}

DIRAC has well documented numbers about scaling in supporting many users, on
many resources, for a variety of applications over a long period of
time.\mtnote{Should we summarize these numbers?}

\paragraph{Development Model}

DIRAC is actively development and used by the LHCb community\cite{}. More
recently, they\mtnote{Who?} have been reaching out to other communities
too\cite{}. From a development perspective, there is ongoing work on enriching
the data capabilities\cite{} and to be able to execute MPI applications\cite{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Falkon
%
\subsubsection{Falkon}
\label{sec:falkon}
% Falkon refers to \pilots as the so called provisioner, which are created
% using the Globus GRAM service. The provisioner spawns a set of executor
% processes on the allocated resources, which are then responsible for managing
% the execution of SUs. \cus are submitted via a so called dispatcher service.
% Similar to Coaster, Falkon utilizes a M/W coordination model, i.\,e.\ the
% executors periodically query the dispatcher for new SUs. Web services are
% used for communication.

% \mrnote{Main Falkon ref:
% \url{http://dev.globus.org/images/7/78/Falkon_SC07_v42.pdf}}
% \mrnote{Best Falkon ref: \url{http://dev.globus.org/wiki/Incubator/Falkon}}

The Fast and Light-weight tasK executiON framework (Falkon)~\cite{1362680} was
created with the primary objective of enabling many independent tasks to run on
large computer clusters (an objective shared by most \pilot systems).
Performance and time-to-completion for jobs on such clusters drove Falkon
development. In addition to being \textit{fast}, Falkon, as its name suggests,
also focused on lightweight deployment schemes.

\paragraph{Pilot Resource Capabilities}

Falkon exposes resource as a set of cores as tasks are by definition single
core only.

\paragraph{Resource Interaction}

Falkon was originally developed for use on large computer clusters in a grid
environment, but has since been expanded to work on other types of
infrastructures. Falkon has been shown to run on TeraGrid (now XSEDE),
TeraPort, Amazon EC2, IBM Blue Gene/L, SiCortex, and Workspace
Service~\cite{1362680}.

\paragraph{Overlay Management}

The Dispatcher service in Falkon is implemented by means of a web service.
This Dispatcher implements a factory/instance deployment scenario.
When a new client sends task submission information, a new instance
of a Dispatcher is created.

\paragraph{Workload Semantics}

Falkon has been integrated with the Karajan workflow language and execution
engine, meaning that applications that utilize Karajan to describe their
workflow will be able to be executed by Falkon.

The Swift parallel programming system~\cite{Wilde2011} was integrated
with Falkon for the purpose of task dispatching.

\paragraph{Task Binding Characteristics}

Cores and tasks are considered homogeneous. Tasks are pulled to the nodes
and are thereby of late-binding nature.

\paragraph{Task Execution Modes}

Falkon does not support MPI jobs, a limiting factor in its adoption for certain
scientific applications.

\paragraph{Coordination and Communication}

Interaction between the components is by use of the Globus Web Services model.

\paragraph{Architecture}

Falkon's architecture relies on the use of multi-level scheduling as well as
efficient dispatching of tasks to heterogeneous DCIs. As mentioned above, there
are two main components of Falkon: (i) the Dispatcher for farming out tasks and
(ii) the Provisioner for acquiring resources.

The overall task submission mechanism can be considered a 2-tier architecture;
the Dispatcher (using the terminology defined in Section~\ref{sec:termsdefs},
this is the \pilot Manager) and the Executor (the \pilot Agent).

The Dispatcher is a GRAM4 web service whose primary function is to take task
submission as input and farm out these tasks to the executors. The Executor
runs on each local resource and is responsible for the actual task execution.
Falkon also utilizes \textit{provisioning} capabilities with its Provisioner.

The Falkon Provisioner is the closest analogous entity to a \pilot: it is the
creator and destroyer of Executors, and is capable of providing both static and
dynamic resource acquisition and release.

Further, in order to process more complex workflows, Falkon has been integrated
with the Karajan workflow execution engine~\cite{karajan}. This integration
allows Falkon to accept more complex workflow-based scientific applications as
input to its \pilot-like job execution mechanism.\mtnote{repetition?}

\paragraph{Interface}

Simpler task execution can be achieved without modifying the existing
executables. Sufficient task description in the web service is all that is
required to utilize the Falkon system.

\paragraph{Interoperability}

For launching to resources Falkon relies on the availability of GRAM4.
This abstracts the resource details but also limits the use of other resources.

\paragraph{Multitenancy}

Each instantiation of the Dispatcher maintains its own task queue and state -
in this way, Falkon can be considered a single-user deployment scheme, wherein
the ``user'' in this case refers to an individual client request.

\paragraph{Robustness}

Falkon supports a fault tolerance mechanism which suspends and dynamically
readjusts for host failures.

\paragraph{Security}

Thanks to the use of Globus transport channels, Falkon allows for both
encrypted and non-encrypted operation. The non encrypted version was used to
gain most throughput, obviously lowering the security of real-world use.

\paragraph{Files and Data}

The data management capabilities of Falkon extend beyond the core
\pilotjob functionalities as described in Section~\ref{sec:coreprops}. Falkon
encompasses advanced data-scheduling and caching but also a data diffusion
approach. Using this approach, resources for both compute and data are acquire
dynamically and compute is scheduled as close as possible to the data it
requires. If necessary, the data diffusion approach replicates data in response
to changing demands~\cite{raicu2008accelerating}.

\paragraph{Performance and Scalability}

As previously stated, the design of Falkon was centered around the goal of
providing support to run efficiently large numbers of jobs  on large clusters
and grids. Falkon realizes this goal through the use of: (i) a dispatcher to
reduce the time to actually place tasks as jobs onto specific resources (such a
feature was developed to account for different issues amongst distributed
cyberinfrastructure, such as multiple queues, different task priorities,
allocations, etc); (ii) a provisioner which is responsible for resource
management; and (iii) data caching in a remote environment~\cite{1362680}.

Falkon has been tested for throughput and performance with multiple
applications: fMRI (medical imaging), Montage (astronomy workflows), and MolDyn
(molecular dynamics simulation). Falkon shown favorable results in terms of
overall execution time when compared to GRAM and GRAM/Clustering
methods~\cite{1362680}.

The per task overhead of Falkon execution has been shown to be in the
millisecond range. Furthermore, Falkon has been demonstrated to achieve
throughput in the range of hundreds to thousands of tasks per second for very
fine-grained tasks.

\paragraph{Development Model}

The Falkon project ran from 2006 to 2011 and the source code is not publicly
available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% GWPilot
%
%\subsubsection{GWPilot}
%\label{sec:gwpilot}
%% \aznote{Considering the new direction of the paper, I am not sure whether
%% GWPilot~\cite{gwpilot} requires its own section for analysis...}
% msnote: To be reconsidered: but not now
%% %\begin{lstlisting}[breaklines]
%% %\url{https://indico.egi.eu/indico/materialDisplay.py?contribId=18&sessionId=46&materialId=slides&confId=1019}
%% %\url{http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266981}
%% \begin{itemize}
%% \item Integrates with GridWay metascheduler
%% \item Pilots advertise to GridWay, GridWay scheduler schedules pilots
%% \item Pilots pull tasks from scheduler
%% \item Installation as a GridWay driver -- written in Python
%% \item Interoperability managed by GridWay drivers (DRMAA, JDSL, BES, more?)
%% \item Using GWPilot requires only adding a single line to their GridWay task
%% \item ``Lightweight and scalable''
%% \end{itemize}
%% %\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% HTCondor
%
\subsubsection{HTCondor}
\label{sec:htcondor}

HTCondor can be considered one of the most prevalent distributed computing
projects of all time in terms of its pervasiveness and size of its user
community. Is often cited as the project that introduced the \pilot
concept\cite{}. Similar in many respects to other batch queuing systems,
HTCondor puts special emphasis on high-throughput computing (HTC) and
opportunistic computing. HTC is defined as providing large scale computational
power on a long term scale. Opportunistic computing is about making pragmatic
use of computing resources whenever they are available, without requiring full
availability. The former can be can be achieved by applying the latter.

% But despite this fact, describing HTCondor along the lines of a \pilotjob
% system has turned out to be difficult, if not partly impossible for several
% reasons.

% The \pilot concept is in its heart centered around Condor Glideins.
% Many Workload Management Systems make use of the core functionality, ranging
% from many specific WMS's to the ``generic`` GlideinWMS.


\paragraph{Architecture}

HTCondor is a high-throughput distributed batch computing system.

%
% Core HTCondor (ca 1988)
%
The core components of HTCondor are collectively named the \impterm{Condor
Kernel}:\mtnote{I do not understand the use of the colon here. Is `Agent'
synonym of `Condor Kernel'?} \impterm{Agent} (also referred to as
\impterm{schedd}), \impterm{Resource} (also referred to as \impterm{startd}),
\impterm{Shadow}, \impterm{Sandbox} (also referred to as \impterm{starter}),
and \impterm{Matchmaker} (also referred to as \impterm{central manager}).

The user submits \vocab{Tasks} (in HTCondor called \impterm{Jobs}) to the
\impterm{Agent}. \impterm{Agents} provide persistent storage for the user's
\impterm{Jobs} and tries to find computing \vocab{resources} \impterm{Resource}
to execute them.

Both \impterm{Agents} and \impterm{Resources} advertise themselves to the
\impterm{Matchmaker}, which is then responsible to match the latter to the
former. The \impterm{Matchmaker} notifies the \impterm{Agent} of a potential
match, and the \impterm{Agent} is in turn responsible to contact the
\impterm{Resource} and validate the suggested match.

Once the match is confirmed by the \impterm{Resource}, both \impterm{Resource}
and \impterm{Agent} have to start a new process to make the execution of the
\impterm{Job} happen. The \impterm{Agent} starts a \impterm{Shadow} which
is responsible for providing all the details required to execute a
\impterm{Job} (the executable, the arguments, the environment, the input
files).

The \impterm{Resource} creates a \impterm{Sandbox} which is an appropriate
execution environment (the \impterm{Sand} for the \vocab{Task} while at the
same time protecting the \impterm{Resource} from a misbehaving \impterm{Job}
(the \impterm{Box}).\mtnote{Is there a missing parenthesis?}

Following on from this, an \impterm{Agent}, \impterm{Matchmaker} and a (set of)
\impterm{Resources} together form a \impterm{Pool}. \vocab{Resources} in a pool
can span a wide spectrum of systems. While some \impterm{Pools} are comprised
of regular desktop PCs (sometimes collectively called a campus grid), other
\impterm{Pools} incorporate large HPC clusters and cloud resources.  Hybrid
pools with heterogeneous types of resources are also possible.

Within the \impterm{Pool} the \impterm{Matchmaker} is responsible for enforcing
the policies defined by the \impterm{Community}.

%The matchmaker, also called the \impterm{central manager}, is another system
%service that realizes the concept of late binding by matching user tasks with
%one or more of the resources available to an HTCondor pool.
%The matchmaking process is based on \impterm{ClassAds} a description language
%that can capture both, resource capabilities as well as task requirements.

%
% Gateway Flocking (ca 1994)
%
While technically a \impterm{Pool} could grow as big as needed, reality was
that (which was also only natural)\mtnote{I am not sure what `natural' means in
this context. I would eliminate the parenthetical sentence altogether or
explain it better.} many independent \impterm{Pools} were created. As a user
(or \impterm{Agent}) could only belong to one \impterm{Matchmaker}, the
\impterm{Resources} that a user could access were therefore limited to one
\impterm{Pool}. This lead to the concept of \impterm{gateway
flocking}~\cite{Epema:1996:flocking}. With this, an individual \impterm{Pool}
would be interfaced to another \impterm{Pool} by letting associated
\impterm{gateway} machines expose information about the \impterm{Pool's}
participants to another \impterm{Pool}, e.g. about idle \impterm{Resources} or
\impterm{Agents}. These \impterm{gateway} nodes can be configured to adhere to
policies and are not necessarily bi-directional.

% It is possible for two or more HTCondor pools to ``collaborate'' so that one
% pool has access to the resources of another pool and vice versa. In HTCondor,
% this concept is called and allows agents to query matchmakers outside their
% own pool for compatible resources. Flocking is used to implement
% load-balancing between multiple pools but also to provide a broader set of
% heterogeneous resources to user communities.

In this model, however, \impterm{flocking} is still bounded to single
\impterm{organizations}/\impterm{communities}, and a user (or \impterm{Agent})
could not join multiple of them at the same time.
%
% Direct Flocking (ca 1998)
%
This limit was overcome by introducing \impterm{direct flocking} in which an
\impterm{Agent} can report itself to multiple \impterm{Matchmakers}
While both forms of \impterm{flocking} had their \textit{raison d'\^{e}tre},
and they could even be mixed, \impterm{gateway} flocking did not stand the test
of time as with scaling up, both technical overhead and organizational burden
became too high.

%
% Condor-G (ca 2000)
%
Around that time (1998) the vision of the Grid began to surface. One of the
initial activities was a uniform interface for batch execution. This was
achieved by the Globus project with the design of the Grid Resource Access and
Management (GRAM) protocol.

The HTCondor \impterm{Agent} was extended to speak GRAM and Condor-G
\cite{condor-g} was born. GRAM services are deployed as remote job submission
endpoints on top of HPC cluster batch queuing systems. Condor-G allows a user
(an \impterm{Agent}) to incorporate those HPC resources temporarily to an
HTCondor pool.

Although Condor-G opened up the execution of Condor \impterm{Jobs} at any
GRAM-enabled computing resource, the drawback of this approach was that
resource allocation and job execution became coupled again, i.e. the
\impterm{Agent} had to direct the \impterm{Job} to a specific GRAM endpoint
without (potentially) knowing about the availability of resources on that
endpoint. Because GRAM was an abstraction of so many batch queuing systems, it
could only implement the greatest common denominator of all the functionalities
of these various implementations. This implied that in this mode of operation
there was no \impterm{Matchmaker} as the required functionality at the
endpoints for that was not provided by GRAM.

%
% Condor-G and Gliding In (ca 2001)
%
\impterm{Gliding In} was engineered to undo this coupling by offering a
personal Condor \impterm{Pool} out of GRAM enabled resources. \impterm{Gliding
In} consist of a three-step procedure: (i) a Condor-G agent uses the GRAM
protocol to launch HTCondor servers ad hoc via a GRAM endpoint service on a
remote system; (ii) the Condor servers, once gotten through the batch queue,
connect to the \impterm{Matchmaker} started by the \impterm{Agent} and thereby
form a personal Condor \impterm{Pool}; and (iii) \impterm{Jobs} submitted by
the user to the \impterm{Agent} which are then matched to and executed on the
\impterm{Resources} within the \impterm{Pool}.

\paragraph{Pilot Resource Capabilities}

The resources managed by a single \pilot are generally limited to compute
resources only and more specifically to single compute nodes.

\paragraph{Resource Interaction}

In its native mode as a batch queuing system, HTCondor is a middleware, and
therefore the discussion of resource interaction does not apply. With the
advent of Condor-G, Condor can be deployed to any site that offers a GRAM
interface.

\paragraph{Overlay Management}

%However, the provisioning, allocation and usage of resources within a pool can
%differentiate between different pools and multiple different approaches and
%software systems have emerged over time, all under the umbrella of the wider
%HTCondor project.

% \paragraph{glideinWMS -- Automated \pilot Provisioning}
GlideinWMS, a workload management system (WMS)~\cite{1742-6596-119-6-062044}
based on Condor GlideIns, introduces advanced \pilot capabilities to HTCondor
by providing automated \pilot (\impterm{startd}) provisioning based on the
state of an HTCondor pool.

\paragraph{Workload Semantics}

The upper most layer of the Condor stack is the \impterm{problem solver}.
A \impterm{problem solver} is a application layer structure built on top of the
Condor agent. Two problem solvers are supplied in the distribution:
master-worker and the directed acyclic graph manager (DAGMan).
Other problem solvers can be built using the public interfaces of the agent.
The task dependencies that are defined e.g. with DAGMAN are managed outside of
the Agent and the lower layers are not aware of the workload semantics.

\paragraph{Task Binding Characteristics}

%Gliding-In implements \vocab{late-binding} on top of GRAM/HPC
%systems: the (user-)agent can assign tasks to \impterm{startd}s after
%they have been scheduled and started through the HPC queueing system. This
%effectively decouples resource allocation (\impterm{startd} scheduling through
%GRAM) and task assignment.

None of the details of the \impterm{Job} are made known outside of the
\impterm{Agent} (\impterm{Shadow}) until the actual moment of execution.
This allows for the \impterm{agent} to defer placement decisions until the last
possible moment. If the \impterm{Agent} submits requests for resources to
several matchmakers, it may award the highest priority job to the first
resource that becomes available, without breaking any previous commitments.

\paragraph{Task Execution Modes}

A \impterm{universe} in HTCondor defines an execution environment. There are
distinct implementations of these universes available: the \impterm{Standard
Universe} provides checkpointing (relevant because of the opportunistic mode of
Condor) and remote system calls, which requires special compilation of the
application though; the \impterm{Java Universe} hides the details of setting up
the remote JVM; the \impterm{Vanilla Universe} is for applications that can't
be modified; the \impterm{Grid Universe} is the mode for Condor-G; the
\impterm{VM Universe} for exploiting VMware and XEN virtual machines; and the
\impterm{Parallel Universe} for executing MPI applications. It should be noted
that these \impterm{Universes} don't mix and that there is no (real) support
for MPI jobs in the Condor-G mode.

\paragraph{Coordination and Communication}

The various kernel elements can be deployed in multiple configurations and can
all be run on a single machine but also fully distributed. The HTCondor system
has their history in institutional environments and assume not too many network
restrictions.\mtnote{I do not understand the previous sentence.} The
communication between the kernel components consist of both UDP and TCP based
proprietary protocols that uses an array of dynamically allocated ports. With
Condor-G the requirements were significantly reduced and the main communication
is with the GRAM service.

%
% TODO: Condor-C aka Condor-to-Condor, when the LRMS is Condor, and using
% Condor-G, there is no need for additional Condor daemons, and the two
% Condor instances can be coupled.
%

\paragraph{Interface}

In a pool, user tasks are represented through job description files and
submitted to a (user-)\impterm{agent} via command-line tools. The main HTCondor
distribution provided command line utils to setup these Glideins. These are now
replaced by Bosco.

% -----------------------------------------------------------------------------
%
%\subsubsection{Corral}
%\onote{Corral seems to be a component in the glidein-WMS landscape and
%not an independent pilot-job implementation. I don't think that we should
%dedicate an extra section to it.}
%\aznote{SJ asked for an investigation of Corral -- not sure that this
%deserves a full analysis at this point on a technical level despite
%being commonly used, open to suggestions}

%Corral is designed to allow hybrid HTC/HPC execution, in which
%many small jobs may be executed in conjunction with larger runs.
%\cite{Rynge:2011:EUG:2116259.2116599}
%\aznote{Back this up w/ paper refs -- paper is somewhat dated,
%verify this is still true today}.  Corral operates as a Glidein
%WMS frontend\aznote{main reason that I think we shouldn't include
%Corral in its own section...}, where GlideinWMS manages the size
%of Condor glide-in pools.
%\begin{itemize}
%\item Workflow - handled by Pegasus workflow management system
%  \aznote{True in the paper I am using, but not handled by Corral itself, so should we include this?}
%\item Placeholder - handled by multislot requests
%  \aznote{multislot request requests a single large
%    GRAM job, and then starts glideins within this container}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% -----------------------------------------------------------------------------
%
%\paragraph{BoSCO}

%\onote{It seems that BOSCO is some sort of a user-space version of Condor.
%It can interface with single clusters as well as complex HTC grids (GlideinWMS).
%Multiple resource-scanrios are possible. There is a bosco-submit node which
%holds the user jobs. Bosco uses Condor glidein (-agents) internally as a
%resource overlay. The glideins pull data from the bosco-submit node. The
%main difference between BOSCO and Condor (even though both expose the same
%user API) is that BOSCO allows ad-hoc usage, while Condor requires a rather
%complex setup. In that regard, BOSCO is somehwat similar to BigJob.
%This page is somehwat insightful: http://bosco.opensciencegrid.org/about/}

%\subsubsection{Bosco}
%% \\
%% \begin{itemize}
%% \item Condor used as batch system/user interface
%% \item Single submit model for different cluster types (LSF/PBS/SGE/etc) via SSH with \texttt{BLAHPD}
%% \item Campus Factory (condor overlay generator) creates glideins, checks users queue for idle jobs,
%%   enforces submission policies
%% \item Bosco = ``BLAHPD Over SSH Condor Overlay''
%% \item Workstation-based (run Bosco client locally)
%% \item Multi-user (Bosco workstation install can be used by multiple researchers)
%% \item Supports multiple cluster submission (but what about coordination...)

BoSCO is a user-space job submission system based on HTCondor. BoSCO was
designed to allow individual users to utilize heterogeneous HPC and grid
computing resources through a uniform interface. Supported backends include
PBS, LSF and GridEngine clusters as well as other grid resource pools managed
by HTCondor. BoSCO supports an agent-based (\textit{glidein}/worker) and a
native job execution mode through a single user-interface.

BoSCO exposes the same \impterm{ClassAd}-based user interface as HTCondor.
However, the backend implementation for job management and resource
provisioning is significantly more lightweight than in HTCondor and it
explicitly allows for \textit{ad hoc} user-space deployment. BoSCO provides a
\pilot system that does not require the user to have access to a
centrally-administered HTCondor campus grid or resource pool. The user has
direct control over \pilot agent provisioning (via the \impterm{bosco\_cluster}
command) and job-to-resource binding via \impterm{ClassAd} requirements.

The overall architecture of BoSCO is very similar to that of HTCondor. The
\impterm{BoSCO submit-node} (analogous to Condor \impterm{schedd}) provides the
central job submission service and manages the job queue as well as the worker
agent pool. Worker agents communicate with the \impterm{BoSCO submit-node} via
pull-requests (TCP). They can be dynamically added and removed to a
\impterm{BoSCO submit-node} by the user. BoSCO can be installed in user-space
as well as in system space. In the former case, worker agents are exclusively
available to a single user, while in the latter case, worker agents can be
shared among multiple users. The client-side tools to submit, control and
monitor BoSCO jobs are the same as in Condor (\impterm{condor\_submit},
\impterm{condor\_q}, etc).

%\impterm{CorralWMS:}
CorralWMS is an alternative frontend for GlideinWMS-based infrastructures. It
replaces or complements the regular GlideinWMS frontend with an alternative
API which is targeted towards workflow execution. Corral was initially designed
as a standalone pilot (glidein) provisioning system for the Pegasus workflow
system where user workflows often produced workloads consisting of many
short-running jobs as well as mixed workloads consisting of HTC and HPC jobs.

Over time, Corral has been integrated into the GlideinWMS stack as CorralWMS.
While CorralWMS still provides the same user interface as the initial, stand-
alone version of Corral, the underlying pilot (glidein) provisioning is now
handled by the GlideinWMS factory.

%\paragraph{Interoperability}

\paragraph{Multitenancy}

The main differences between the GlideinWMS and the CorralWMS front-ends lie
with identity management and resource sharing. While GlideinWMS pilots
(glidins) are provisioned on a per-VO base and shared/re-used among members of
that VO, CorralWMS pilots (glideins) are bound to one specific user via
personal X.509 certificates. This enables explicit resource provisioning in
non-VO centric environments, which includes many of the HPC clusters that are
part of U.S. national cyberinfrastructure (e.g., XSEDE).

\paragraph{Robustness}

As stated before, one of the initial drivers for Condor was the heterogenuous
environments and the need to provide a robust runtime environment for the
application by means of the sandbox.
Additionally, for applications that support it, Condor provides checkpointing
which allows applications to be restarted in case of failure or migrated to
another resource.

\paragraph{Security}

HTCondor inter-component communication is based on the home grown CEDAR
message-based communication library. It allows client and servers to negotiate
an appropriate security protocol. Regarding the execution of tasks, there is a
level of trust assumed between resource and application.

\paragraph{Files and Data}

The handling of files and data was initially not a primary design goal and did
not extend well beyond staging relatively small data into the sandbox. The
\impterm{Standard Universe} offers the ability to re-route file I/O back to the
\impterm{Shadow} of the \impterm{Agent}. Over the years, HTCondor has been
integrated with various data tools. Out of the same stable as HTCondor comes
NeST\cite{}.\mtnote{I do not understand the previous sentence.} NeST is a
software-only storage appliance that supports a myriad of storage protocols and
integrates with the \impterm{Matchmaker} to announce its availability. With
storage systems like NeST being available, one can then use a system like
Stork\cite{} to manage the transfers to and from this storage resources. Stork
can make educated decisions about data placement by coordinating with the
\impterm{Matchmaker}. Finally, Parrot\cite{} is a mechanism that offers a POSIX
I/O interface to applications for files that are otherwise only available
remotely.

%\paragraph{Performance and Scalability}

\paragraph{Development Model}

HTCondor is used in multiple different contexts: the HTCondor project, the
HTCondor software, and HTCondor grids. But even if we only look at the software
parts of the landscape, we are faced with a plethora of concepts, components,
and services that have been grown and curated for the past 20 years.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MyCluster
%
\subsubsection{MyCluster}
\label{sec:mycluster}

MyCluster~\cite{1652061} was developed to allow users to submit and manage jobs
across heterogeneous NSF TeraGrid resources in a uniform, on-demand manner.
TeraGrid, the predecessor of XSEDE, existed as a group of compute clusters
connected by high-bandwidth links facilitating the movement of data, but with
many different middlewares requiring cluster-specific submission scripts.
MyCluster allowed all cluster resources to be aggregated into one personal
cluster with a unified interface, being either SGE, OpenPBS or Condor. This
enhancement to user control was envisioned as a means of allowing users to
submit and manage thousands of jobs at once and across heterogeneous
distributed computing infrastructures, while providing a familiar and
homogeneous interface for submission.

Applications are launched via MyCluster in a ``traditional'' HPC manner, via a
\impterm{virtual login session} which contains usual queuing commands to submit
and manage jobs. This means that applications do not need to be explicitly
rewritten to make use of MyCluster functionality; rather, MyCluster provides
user-level \pilot capabilities which users can then use to schedule their
applications.

MyCluster was designed for and successfully executed on NSF TeraGrid resources,
enabling large-scale cross-site submission of ensemble job submissions via its
virtualized cluster interface. This approach makes the \vocab{multi-level
scheduling} abilities of \pilot implicit. Rather than directly \prop{binding}
tasks to individual TeraGrid resources, users allow the virtual grid overlay to
\vocab{schedule} tasks to multiple allocated TeraGrid sites presented as a
single cohesive, unified resource.

\paragraph{Task Execution Modes}

MyCluster's \prop{Task Execution Modes} are explicitly limited to
``serial'' tasks although it is unspecified whether this also confines
it to single core \vocab{tasks} only.

\paragraph{Pilot Resource Capabilities}

As per the earlier description, MyCluster was specifically targeted to
TeraGrid resources and thereby the \prop{Pilot Resource Capabilities} are those
of the TeraGrid, being HPC resources. As the \vocab{Task Execution Modes} are
limited to serial tasks, the only relevant resource that is exposed is the
``CPU'' or ``core''.

\paragraph{Resource Interaction}

The \prop{Resource Interaction} is exclusively by the \impterm{Agent Manager}
through Globus GRAM to start a \impterm{Proxy Manager} at each site.

\paragraph{Workload Semantics}

All \vocab{tasks} are considered independent and no other \prop{Workload
Semantic} is described.

\paragraph{Task Binding Characteristics}

As the \pilots are not exposed (and the \vocab{tasks} are considered
homogeneous) there are no explicitly controllable \vocab{Task Binding
Characteristics}. This in effect makes it a \vocab{Late Binding} mechanism.

\paragraph{Overlay Management}

With respect to \vocab{Overlay Management} MyCluster allows the user to
configure the number of \impterm{Proxies} per site, the number of CPUs per
\impterm{Proxy} and the list of sites to submit to. All resources that are
acquired through the \impterm{Proxy Managers} are pooled together. These
resource specifications are just requests, it depends on the behavior of the
queue whether (and when) these resources are actually acquired. In addition to
these static resource specifications, MyCluster includes a mode of operation
where \impterm{Proxies} can be \impterm{Migrated} to other sites. The rationale
for \impterm{migration} is that the weight of the resource requests can be
moved to the site with the shortest queuing time.

\paragraph{Architecture}

When the user starts a session via the \impterm{vo-login} command a
\impterm{Agent Manager} is instantiated. The \impterm{Agent Manager} in turn
starts a \impterm{Proxy Manager} at every resource site gateway host through
Globus GRAM and a \impterm{Master Node Manager} at the local client machine.
The \impterm{Proxy Manager} has an accompanying \impterm{Submission Agent} that
also runs on the resource gateway. The \impterm{Submission Agent} interacts
with the local queuing systems and submits the \impterm{Job Proxy} that
launches a \impterm{Task Manager} process on a compute host. The \impterm{Task
Manager} starts one or more \impterm{Slave Node Manager} processes on all the
allocates compute hosts. Ultimately, the \impterm{Slave Node Manager} in
control of the worker node. Depending on the configuration, the \impterm{Slave
Node Manager} starts the Condor, SGE, or OpenPBS job-starter daemons, which in
turn connect back to their master daemon started earlier on the local client
machine by the \impterm{Master Node Manager}.

\paragraph{Coordination and Communication}

The communication between \impterm{Proxy Manager} and \impterm{Agent Manager}
is over TCP.

\paragraph{Security}

The TCP communication channels are not encrypted but do use GSI-based
authentication and has measures in place to prevent replay attacks.

\paragraph{Robustness}

The \impterm{Agent Manager} maintains little state. It is assumed that ``lost''
\impterm{Proxy Managers} will re-connect back when they recover.
The \impterm{Proxy Manager} is restartable and will try to re-establish the
\impterm{Proxies} based on a last-known state stored on disk when they got
lost due to exceeding wallclock limitations or node reboots;

\paragraph{Interoperability}

The primary goal of MyCluster was \prop{Interoperability} over multiple
TeraGrid resources.

\paragraph{Interface}

The \prop{Interface} of MyCluster depends on the choice of the underlying
system as that one is exposed through the \impterm{Virtual Login
Session}.\mtnote{I do not understand the previous sentence, especially `as that
one'.} The interface also allows for users to interactively monitor the status
of their \vocab{\pilots} and \vocab{tasks}.

\paragraph{Multitenancy}

The created cluster resides in userspace, and may be used to marshal multiple
resources ranging from small local resource pools (e.g.  departmental Condor or
SGE systems) to large HPC installations. The clusters can be created per user,
per experiment or to contribute to the resources of a local cluster.

\paragraph{Files and Data}

MyCluster doest not offer and file staging capabilities other than those of the
underlying systems provide. This means that it exposes the file capabilities of
Condor, but doesn't offer any file staging capabilities when using SGE.

\paragraph{Development Model}

MyCluster is no longer developed or supported after the TeraGrid project came
to a conclusion in 2011. \mtnote{Should we order the descriptions of the pilot
systems in 4.2 starting from those that are not available any more?}

\paragraph{Conclusion}

MyCluster illustrates how an approach aimed at \vocab{multi-level scheduling}
by marshaling multiple heterogeneous resources lends itself to\mtnote{is
consistent with?} a \pilot-based approach. The fact that the researchers behind
it developed a complete \pilot system while working toward
interoperability/uniform access is a testament to the usefulness of \pilots in
addressing these problems. The end result is a complete \pilot system, despite
the authors of the system being constructed\mtnote{I do not understand what
this means} not having used the word ``pilot'' once in their main publication.

While not an official product, a recent and similar approach has been adopted
at NERSC. The operators of the Hopper cluster provide a tool called MySGE which
allows for the user to provision a personal SGE cluster on Hopper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% PanDA
%
\subsubsection{\panda}
\label{sec:panda}

\panda (Production and Distributed Analysis)~\cite{1742-6596-331-7-072069} was
developed to provide a multi-user workload management system (WMS) for
ATLAS~\cite{aad2008atlas}. ATLAS is a particle detector at the Large Hadron
Collider at CERN that requires a WMS to handle large numbers of jobs for their
data-driven processing workloads. In addition to the logistics of handling
large-scale job execution, ATLAS needed also integrated monitoring for analysis
of system state and a high degree of automation to reduce the need for
user/administrative intervention.

\panda has been initially deployed as an HTC-oriented, multi-user WMS system
for ATLAS, consisting of ~100 heterogeneous computing
sites~\cite{maeno_pd2p:_2012}.
Recent improvements to \panda have extended the range of deployment scenarios
to HPC and Cloud infrastructures making \panda a general-use \pilot
system~\cite{nilsson2012recent}.

\paragraph{Architecture}

\panda's central component is called the \impterm{\panda server}. It keeps track
of jobs, matches jobs with pilots and resources, dispatches jobs to resources,
and coordinates data management.
The \panda server is implemented as a stateless multi-process REST web service,
with a database backend, to which it communicates through the \impterm{Bamboo}
interface.
A central job queue allows users to submit jobs to distributed resources in a
uniform manner.
\panda's \vocab{\pilot} is called, appropriately enough, a \impterm{Pilot}, and
handles the execution environment.
When pilots are launched, they collects information about their worker nodes
that is sent back to the job dispatcher.
The main functionality of the \pilot is the so-called multi-job loop.  It
connects to the \panda server to ask for jobs to run.  If a matching job does
not exist, the pilot ends. If a job does exist, the pilot forks a separate
thread (\impterm{runJob}) for the job and starts monitoring its execution from
the \impterm{monitor}.

\paragraph{Pilot Deployment}

An independent component, AutoPyFactory\cite{Caballero:2012ka}, manages the
delivery of \pilots to worker nodes via a number of schedulers (\impterm{pilot
factories}) serving the sites at which \panda operates.
This component handles \pilot submission, management, and monitoring.
AutoPyFactory supersedes the first generation component called
\impterm{PandaJobScheduler}, as well the second generation one called
\impterm{AutoPilot}).

\paragraph{Task Binding}

Workload jobs are assigned to activated and validated pilots by the \panda
server based on brokerage criteria like data locality and resource
characteristics.

\paragraph{Workload Semantics}

On the \pilot level there are no rich workload semantics.
Workload semantics exists only in the \impterm{ProdSys} component that feeds
jobs into the database from which the \panda server retrieves its tasks.

\paragraph{Task Execution Modes}

Given the nature of the application workload and the targetted infrastructure,
\panda is targetted (exclusively?) towards sequential program execution.

\paragraph{Pilot Resources}

\panda began as a specialized \pilot for the LCG Grid, and has been
extended into a generalized \pilot which is capable of working across other
grids as well as HPC\cite{} and cloud\cite{} resources.

\paragraph{Resource Interaction}

\panda traditionally relies on Condor-G for its interaction with DCIs.
For recent endeavors in HPC \panda uses SAGA to interface with the queuing
systems.

\paragraph{Coordination and Communication}

Communication between the client and the server is based on HTTP/S RESTful
communication.
Coordination of the workflow is maximally asynchronous.

\paragraph{Interface}

Jobs are submitted to \panda via a simple Python client.
The client needs to define job sets, their associated datasets as well as
the input and output files at submission.
The client API has been used to implement the PanDA front ends for ATLAS
production, distributed analysis and US regional production.

\paragraph{Multitenancy}

\panda is a \textit{true} multi-user system, as both the WMS manages workloads
for multiple users (the whole ATLAS community), but also the \pilots are
capable of executing tasks on the resources for multiple users by means of
glExec\cite{}.
The latter enables the re-use of \pilots amonst users which has a positive
effect on latency from a user perspective.

\paragraph{Files and Data}

\texttt{\panda Dynamic Data Placement} \cite{maeno_pd2p:_2012} allows for
additional, automatic data management by replicating popular or backlogged
input data to underutilized resources for later computations and enables jobs
to be placed where the data already exists.

\paragraph{Robustness}

The wrapper of the \pilot takes care of disk space monitoring, and enables job
recovery of failed jobs and restarting from crash whenever possible.

For production jobs the majority of the errors are site or system related,
while for user analysis jobs the most common issues are related to application
software.  Pilot mechanisms like job recovery contribute to the robustness
against site related failures.

A comprehensive monitoring system supporting production and analysis
operations, including site and data management information for problem
diagnostics, usage and quota accounting, and health and performance monitoring
of \panda subsystems and the computing facilities being utilized.

\paragraph{Interoperability}

\panda enables the concurrent execution of workload on heterogeneous resources.

\paragraph{Security}

Job specifications from the client are sent to the PanDA server via a secure
HTTPS connection authenticated with a GSI certificate proxy.
gLExec is used to use the job user's identity instead of Pilot credentials for
executing tasks.

\paragraph{Performance and Scalability}

\panda has been used to process approximately a million jobs a
day~\cite{pandapresentation2013-06} which handle simulation, analysis, and
other work~\cite{maeno_pd2p:_2012}. The ATLAS experiment itself produces
several petabytes of data a year which must be processed and analyzed.

The \panda system is serving over 100k production jobs and 35k user analysis
jobs concurrently.

%PanDA manages \O(102) sites, \O(105) cores, O(108) jobs per year, O(103) users,
%and ATLAS data volume is O(1017) bytes.

\paragraph{Development Model}

\panda is a multi-stakeholder project that has been active for many years. As
such, it went through many iterations. We try to refer to the current state as
much as possible.
Current funding for \panda enables the project to reach out to new user
communities.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% RADICAL-Pilot
%
\subsubsection{RADICAL-Pilot}
\label{sec:radicalpilot}

The authors of this paper (the RADICAL group) have been engaged in theoretical
and practical aspects of \pilot systems for the past several years. In addition
to formulating the P* Model\cite{} which by most accounts is the first complete
conceptual model of \pilots, the RADICAL group is responsible for the
development and maintenance of RADICAL-Pilot\cite{}. RADICAL-Pilot is the
groups long-term effort for creating a production level \pilot system. The
effort is build upon the experience gained from developing, maintaining and
using BigJob\cite{}, an initial prototype of \pilot system.

\paragraph{Pilot Resource Capabilities}

RADICAL-Pilot is mainly tailored towards HPC environments such as the resources
of XSEDE and NERSC\cite{}. The primary \vocab{resource} that is exposed is the
compute node, or more specifically the cores within such a node.

\paragraph{Resource Interaction}

RADICAL-Pilot uses the Simple API for Grid Applications
(SAGA)~\cite{ogf-gfd-90, sagastuff} in order to interface with different DCIs.
Thanks to SAGA, RADICAL-Pilot submits the Pilot Agent as a \vocab{job} through
the reservation or queuing system of the DCI. Once the \impterm{Agent} is
started, there is no direct \prop{resource interaction} with the DCI except for
monitoring of the state of the \impterm{Agent} process. Adopting SAGA has
enabled RADICAL-Pilot to expand to many of the changing and evolving
architectures and middleware.

\paragraph{Overlay Management}

The programming interface of RADICAL-Pilot enables (and requires) the explicit
configuration of the \vocab{overlay}. The user is expected to specify the size,
type and destination of the pilot(s) he wants to add to the \vocab{overlay}.
Through the concept of \impterm{UnitManagers} on the client side, the user can
group \pilots together and foster them under a single scheduler or have
multiple independent \impterm{UnitManagers} and \pilots, all with their own
\impterm{scheduler}. On HPC style systems there is typically one \pilot that
manages all the resources that are allocated to that specific run, but there
can be as many \pilots per resource as the policies allows concurrently running
jobs.

\paragraph{Workload Semantics}

The \vocab{workload} within RADICAL-Pilot consists of \impterm{ComputeUnits}
that represent \vocab{tasks}. From the perspective of RADICAL-Pilot there are
no dependencies between these \impterm{ComputeUnits} and once a
\impterm{ComputeUnit} is given to the control of RADICAL-Pilot it is assumed to
be ready to be executed. RADICAL-Pilot includes the concept of \impterm{kernel
abstractions}. These are generic application descriptions that can be
configured on a per-source basis. Once an \impterm{application kernel} is
configured for a specific resource and available in the repository, the user
only needs to refer to the \impterm{application kernel}. RADICAL-Pilot then
takes care of setting up the right environment and executing the right
executable. This facility is especially useful in the case of
\vocab{late-binding}, when it is unknown on which \pilot (and thereby resource)
a task will run at the time of its submission.

\paragraph{Task Binding Characteristics}

\vocab{Task} to \vocab{resource} \vocab{binding} can be done either implicitly
or explicitly. A user can bind a task explicitly to a pilot, and thereby to the
resource the pilot is scheduled to, making this a form of early binding. The
user can also submit a task to a unit manager that schedules units to multiple
pilots. In this case it is the semantics of the scheduler that decides on the
task binding. RADICAL-Pilot supports multiple schedulers that can be selected
at runtime. Two schedulers are currently implemented: (i) a round-robin
scheduler that binds incoming tasks to associated pilots in a rotating fashion,
irrespective of their state, and thereby performing \vocab{early binding}. (ii)
a BackFilling scheduler that binds tasks to pilots once the pilot is active and
has available resources, thereby making it a \vocab{late binding} scheduler.

\paragraph{Task Execution Modes}

RADICAL-Pilot supports two type of \vocab{tasks}. One is the generic single
node task, that can be single core, or multi-core threaded/OpenMP applications.
In addition, RADICAL-Pilot has extensive support for MPI applications, where it
supports a large variety of launch methods that are required to successfully
run MPI applications on a wide range of HPC systems. The \impterm{launch
methods} are a modular system and new \impterm{launch methods} can be added
when required.

\paragraph{Architecture}

From a high level perspective RADICAL-Pilot consist of two components. A client
side python module that is programmable through an API and is used by
scripts/applications, and the agent (or multiple agents) that runs on a
resource and executes tasks.

One of the primary features of RADICAL-Pilot is that it completely lives in
user-space and thereby requires no collaboration from the resource owner /
administrator. Other than an online database that maintains state during the
lifetime of a session, there are no other (persistent) service components that
RADICAL-Pilot relies on.

\paragraph{Coordination and Communication}

MongoDB is the bridge between the client side and agent. MongoDB is a
document-oriented database that needs to run in a location that is accessible
for both type of components. The client side publishes the \vocab{workload} in
the MongoDB which is picked up and then executed by the agent. The agent in
turn publishes the status and the output of the \impterm{ComputeUnit} back to
the MongoDB which can be retrieved by the client side. The main advantage of
this model is that it works in situations where there is no direct
communication channel between the client host and the compute resource, which
is a very common scenario. The MongoDB database also offers (some) persistency
which allows the client side to re-connect to active sessions stored in the
database. The main drawback of the use of a database for communication is that
all communication is by definition indirect with potential performance
bottlenecks as a consequence.

\paragraph{Interface}

The client side RADICAL-Pilot is a Python library that is programmable through
the so called \impterm{Pilot-API}. The application-level programmability that
RADICAL-Pilot offers was incorporated as a means of giving to the end-user
control over their job management. Users define their pilots and their tasks
and submit them through the \impterm{Unit Manager}. They can query the state of
pilots and units or rely on a \impterm{callback} mechanism to get notified of
any state change that they are interested in. Users define their pilots and
tasks in a declarative way, but the flow of activity is more of an imperative
style.\mtnote{Should we mention that all this is implemented in Python?}

\paragraph{Interoperability}

RADICAL-Pilot allows for \prop{interoperability} with multiples types of
resources (heterogeneous schedulers and clouds/grids/clusters) at one time. It
does not have \prop{interoperability} across different \pilot systems.

\paragraph{Multitenancy}

RADICAL-Pilot is installable by a user onto the resource of his or her choice.
It is capable of running only in ``single user'' mode; that is, a \pilot
belongs to the user who spawned it and cannot be accessed by other users.

\paragraph{Robustness}

For fault-tolerance RADICAL-Pilot relies on the user to write the application
logic to achieve \prop{Robustness}, to assist in that process, it does report
task failures.

\paragraph{Security}

The compute resource \prop{Security} aspects of RADICAL-Pilot are that it
follows security measures of the respective scheduler systems (i.e. policies
like allocations, etc).\mtnote{I do not understand the previous sentence.}

\paragraph{Files and Data}

RADICAL-Pilot supports many modes of \impterm{data staging} although all of
them are exclusively file based. Files can be staged at both \pilot and
\impterm{ComputeUnit} level, and they can be transferred to and from the
staging area of the pilot and of the compute unit \impterm{sandbox} from and to
the client machine. RADICAL-Pilot also allows the transfer of files from third
party locations onto the compute resource. In addition, the ComputeUnits can be
instructed to use files from the pilot sandbox. In this way, the pilot sandbox
is turned into a shared file storage for all its compute units. The user has
the option to Move, Copy or Link data based on the performance and usage
criteria.

\paragraph{Performance and Scalability}

RADICAL-Pilot is one of the largest pilot based consumers of resources on
XSEDE\cite{}. Many thousands concurrent ComputeUnits can be managed by each
\pilot Agent and scalability can be achieved in many dimensions as multiple
pilots can easily be aggregated over multiple distinct DCIs.

\paragraph{Development Model}

RADICAL-Pilot is an Open Source project released under the MIT license. Its
development is public and takes place on a public Github repository. The
project is under active funding and development and has a number of external
projects that use it as a foundation layer for job execution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section 4's concluding remarks
%
\subsection{Overall observations}

\msnote{Maybe this subsection should go and address it in the discussion and
conclusion}

Some of systems we have discussed, like DIRAC, X, are inherently service
oriented, or exposed as a service. On the other side of the spectrum, systems
like DIANE and RADICAL-Pilot take the form of a library or a framework. From a
formal standpoint, services can be abstracted by a library and a library can be
abstracted by a service, so the main distinction between these architectures if
the default form they come in and how they are meant to be deployed.

\mtnote{Should we avoid this subsection altogether?}


\jhanote{As discussed we'll have a closing paragraph or two, about all the pilot
  systems that we don't discuss}

% -----------------------------------------------------------------------------
% SECTION 5
% -----------------------------------------------------------------------------
%
\section{Discussion and Conclusion}
\label{sec:discussion}

\jhanote{Mark: We need 1-2 paragraphs describing the relationship of section 3
  to the P* model/paper. Else we are going to get dinged by reviewer or cause
  confusion to the reader. Where this paragraph goes is open for discussion, but
  please draft a paragraph.}


% \jhanote{the structure of section 5 is: (i) revisit the myths. (ii)
% state clearly how section 3 and 4 help us define what a pilot is,
% the necessary and sufficient conditions (if possible), discuss the
% classifiers and apply/discuss them to the set of pilot systems here.
% (iii) then we go on to say motivate P*/pilot for data, (iv) discuss
% implications for WF systems and conclude with a summary for
% tools/sustainability/etc. At some point, we discuss how/why pilots
% are more than just pilots, eg can be RM layer for middleware,
% runtime framework etc}\mtnote{I have organized the following items on
% the lines you have traced, with some changes in the order and number
% of the items.}

\mtnote{In Section 3 we use pilotjob. We will have to move to \pilot
or decide to use the less correct pilotjob here too.}\mtnote{Following
discussion, we decided to use \pilot. Section 3 has been updated, the
other Sections will have to follow.}

Sections~\ref{sec:understanding} and~\ref{sec:analysis} offered respectively a
description of the minimal capabilities and properties of every \pilot system;
a vocabulary defining `pilot' and its cognate concepts; a classification of the
core and auxiliary properties of pilot system implementations; and the analysis
of an exemplar set of pilot system implementations. Considered altogether,
these contributions show the notion of \pilot system as a paradigm for the
execution of tasks on distributed resources by means of resource placeholders.

In this Section we critically assess the properties of the \pilot paradigm. The
goal is to show the generality of this paradigm and how \pilot systems go
beyond the implementation of a special purpose trickery to speed up the
execution of a certain type of workload. Once understood the breath of the
\pilot paradigm, we contextualize it by describing its relation with relevant
domains such as middleware, applications, security, and enterprise. Finally, we
close the Section with a look into the future of \pilot systems moving from the
current state of the art and discussing the sociotechnical and engineering
challenges that are being faced both by developers and target users.

\mtnote{Just an idea: Section 2 -- pilot abstraction; Section 3 --- pilot
system; Section 4 --- Pilot implementation; Section 5 --- pilot paradigm.
Abstraction: the conceptualization of pilot; system: the modeling of pilot
conceptualization; implementation: the implementation of the model; paradigm:
methods, tools, and procedures pertaining to/enabled by the use of pilots. What
do you think? Happy to refine/change/abandon if you think it does not work.}

% -----------------------------------------------------------------------------
% 5.1
%
\subsection{The Pilot Paradigm}
\label{sec:paradigm}

% \jhanote{Need thinking: Following on from discussion yesterday, is early/late
% binding a logical consequence of multi-level scheduling? Can we absorb the
% latter into the former?}\mtnote{Following the revision of S3, early/late
% binding seems to be related to having resource placeholder while multi-level
% scheduling seems to be the means to have placeholders. Both early and late
% binding refers to the same entities: tasks bound to pilots, i.e. place
% holders. Multi-level scheduling refers to two pairs of entities: container
% (i.e. jobs) scheduled to DCIs, and tasks scheduled to placeholders. This
% opens the issue of whether binding is a form of scheduling, whether binding
% and scheduling are the same. We may want to discuss about it.}

% \mtnote{The following paragraphs address: PJs have well defined semantics and
% model. PJs dont help with data intensive applications.}

% The scope of the \pilot paradigm encompasses multiple types of resource.

% Usually, the tasks executed on pilots leverage mainly the cores of the
% compute nodes for which the pilot is a placeholder. Tasks may also use
% data or network resources but mostly as accessories to their execution
% on computational resources.

% Local or shared filesystems may be used to
% read input and write output data while network connectivity may be
% required to download, for example, the code of the task executor.

% The elements of generality of the \pilot paradigm have been modeled in
% Ref.~\cite{Luckow:2008la}. This investigation was motivated by the desire to
% provide a single conceptual framework -- the so called \pstar Model -- that
% would be used to subsume the design characteristics of the various \pilot
% system implementations, a sample of which have been analyzed in detail in
% Section~\ref{sec:analysis}. \mtnote{Is this still the case? I think this has
% become one of the goals of this paper. How do you want to mention P* here?
% What do you want to say about it? Is the paragraph above sufficient?}
% Interestingly, the \pstar model was amenable and easily extensible to
% \pilotdata.  The consistent and symmetrical treatment of data and compute in
% the model led to the generalization of the model as the {\it P* Model of
% Pilot Abstractions}.

% \mtnote{The following paragraphs address: (i) PJs are just for HTC and they
% just circumvent job queuing delays; PJs unfairly game HPC queuing system;
% (ii) PJs have to be tied to specific infrastructure and infrastructures have
% to be tied to specific pjs; (iii) PJs are such a simple concept, it doesn't
% need more attention: Conversely, everyone should write their own PJ just
% because they can; Note: the second part of this statement is better addressed
% when discussing the fragmentation of the pilot landscape. (iv) PJs are
% stand-alone tools passive (system) tools, as opposed to user-space, active
% and extensible components of a CI;}

The \pilot paradigm identifies a type of software system with both general and
unique characteristics. This paradigm is general because, in principle, it does
not depend on a single type of workload, a specific infrastructure, or a unique
performance metric. Systems implementing the \pilot paradigm can execute
workloads composed by any number of tasks with disparate requirements. For
example, as seen in~\S\ref{sec:analysis}, \pilot systems can execute
homogeneous or heterogeneous bags of independent or intercommunicating tasks
with arbitrary duration, data or computation requirements.

The same generality applies to both the type of resource and infrastructure on
which a \pilot system can execute given workloads. As seen in
\S\ref{sec:understanding}, the \pilot paradigm demands resource placeholders
but does not specify the type of resource that the placeholder should expose.
Most of the \pilot systems illustrated in \S\ref{sec:analysis} are designed to
expose only compute resources, i.e. the cores of the compute nodes for which
the pilot is a placeholder. Tasks executed on these \pilots may also use data
or network resources but mostly as accessories to their execution on
computational resources.

It is important to note that the focus on computational resources is not
mandated by the \pilot paradigm. In principle, pilots can be placeholders also
and exclusively for data or network resources, depending on the capabilities
exposed by the middleware of the target DCIs. For example, in Ref.~[pilot
data], the notion of \pilotdata has been conceived using the power of symmetry,
i.e., \pilotdata taken to be a notion as fundamental to dynamic data placement
and scheduling as \pilot is to computational tasks.

Currently, no \pilot system exposes networking resources by means of
placeholders but there is no theoretical limitation to the implementation of
\pilot networks.

\pilot systems already operate on multiple platforms. Originally devised for
Grid infrastructures, \pilot systems have been engineered to operate also on
HPC and Cloud infrastructures. Important to note that many of the \pilot
systems described in \S\ref{sec:analysis} have been adapted to work with
different infrastructures without undergoing major re-architecturing. This is
further indication of the independence of the \pilot paradigm from
infrastructural details, especially those concerning the type of container used
to instantiate a placeholder.

Usually, \pilots are thought as a means to optimize the throughput of
single-core, short-lived, uncoupled tasks execution. The analysis presented in
\S\ref{sec:analysis} showed how restrictive such a view is. The \pilot paradigm
has been successfully integrated within workflow systems to support execution
of workloads with articulated data and stage dependences. Furthermore, single
and multi-core tasks can be executed by many existing \pilot system
implementations, and BigJob and Radical-Pilot offer MPI support. As such,
depending on the type of the given workload, the \pilot paradigm can be
implemented to optimize different metric of task execution other than
throughput.

Characterized and defined by resource placeholding, multi-level scheduling, and
late binding, \pilot systems represent a category of software in itself. \pilot
systems are not just an alternative implementation of a scheduler, of a
workload manager, of a batch system, or of any other special-purpose software
component. This specific system is used when abstracting one or more type of
physical resources with well-defined spatial and time properties can lead to
the optimization of one of more parameters of the workload execution.

Appreciating the generality of the \pilot paradigm and the independence its
implementations from other types of software, helps also to misspell a
potential misunderstanding involving \pilots and how multi-level scheduling
works. \pilot systems are not circumventing the infrastructure middleware but
simply abstracting away some of its properties in order to optimize one or more
user-defined performance metric.

Multi-level scheduling is not replacing the infrastructure-level scheduler. As
clearly illustrated in~\S\ref{sec:understanding}, resource containers are still
created on the target infrastructure by means of that infrastructure
capabilities and the second level of scheduling is completely contained within
the boundaries of the resource container. Optimizing the usage of the resource
containers, for example by minimizing idling time, is an implementation issue,
not an intrinsic limitation of the \pilot paradigm.

It should be noted that shifting the control over tasks scheduling away from
the infrastructure middleware does not imply that the end-user will become
necessary responsible for an efficient resource utilization. A \pilot system
can be implemented as a middleware component, as seen with HTCondor and its
role as an integral part of the infrastructure middlware stack.

The generality of the \pilot paradigm may come as a surprise when considering
the most common requirement that motivates its implementations. Leveraging
multi-level scheduling so to increase the execution throughput of large
workloads made of short running tasks has been achieved without having to
devise a new paradigm. For example, as seen in~\S\ref{sec:analysis} \panda,
Falkon, or DIRAC were initially developed as single-point solutions, focusing
on either a type of workload, a specific infrastructure, or the optimization of
a single performance metric.

Appreciating the properties of the \pilot paradigm becomes necessary once
requirements of infrastructure interoperability, support from multiple types of
workloads, or flexibility in the optimization of execution are introduced.
Satisfying those requirements requires abstracting the specificity of
middleware, infrastructure architectures, and application patterns. As shown in
this paper, this process of abstraction means to develop an understanding of
the \pilot paradigm.

% The \pilot paradigm... Why it is a independent distributed system
% abstraction? Independent = unique characteristics and properties when
% compared to other distributed system paradigms. What `other' paradigms
% should we consider? In what sense the pilot paradigm is distributed?
% Why should we define the \pilot paradigm an abstraction?

% -----------------------------------------------------------------------------
% 5.2
%
\subsection{Pilot Paradigm in Context}
\label{sec:context}

The adoption of the \pilot paradigm in a scientific computing environment
brings to the fore a richer, more granular set of details about its
specification, requirements, and functionalities.

% \paragraph*{HTC and HPC}

The analysis offered in~\S\ref{sec:analysis} shows a widespread adoption of the
\pilot paradigm with multiple implementations serving diverse use cases. While
the paradigm remains the same, its application varies depending on the
characteristics of the type of computation considered. These differences are
well illustrated by looking at how \pilot systems support the execution of
parallel and distributed applications.

Both types of applications may have similar \pilot and task durations but their
spatial dimension varies sensibly. Typically, parallel applications require
tightly coupled computations [ref] that, in turn, need \pilots to be
instantiated on infrastructures offering potentially large-core, large-memory
compute nodes. These pilots are usually submitted by means of the
infrastructure headnode, each pilot having at least as many core as those
offered by a node of the target infrastructure.

Differently from parallel applications, distributed applications usually
require loosely coupled or fully independent computations. Consequently,
\pilots supporting their execution depend less from the size of the nodes of
the target infrastructure and more on the overall number of possibly
independent cores that they can hold. These \pilots can be instantiated also by
means of decentralized services (e.g. WMS) aggregating resources from possibly
heterogeneous infrastructures.

% to HPC and HTC presents practical differences. The ratio of tasks and \pilots
% might be similar for both type of infrastructures in the temporal dimension
% (i.e., the walltime of the \pilots and the duration of tasks execution) but
% the spatial dimension varies sensibly. Consider HPC as a system with $O(10)$
% tightly coupled sites. Typically, this system has a head/login/service node
% where the \pilot will operate from. For one site, the ratio of \pilots vs
% compute resources will typically be $O(1):O(Nodes)$ and therefore the ratio
% of \pilots vs tasks will be $O(1):O(Tasks)$. With the given $O(10)$ sites
% this leaves us with a \pilot:tasks ratio of at most $O(10):O(Tasks)$. When
% HTC is defined as a system with O(100) loosely coupled sites without a
% central node to foster these resources, the ratio of \pilots to compute
% resources will typically be closer to $O(1):O(Cores)$. These differences have
% not only implications on the type of applications that can be run, but also
% on the nature of the multi-level scheduling. In the HPC case there is a large
% degree of freedom for the \pilot layer scheduling while in HTC the scheduling
% on that level is much more predetermined.

% \paragraph*{MPI, OpenMP, etc.}

The notion of task as defined in~\S\ref{sec:understanding} and then utilized
in~\S\ref{sec:analysis} shapes both the conceptualization and implementation of
the \pilot paradigm. Conceptually, adding a wrapper to the executables of a
distributed or parallel application means creating a well-defined space for
their localization. This may involve not only the specification of program
switches, data locations, or library dependences but also loading modules,
downloading supporting code, or on-the-fly compilations and execution of
supporting applications. Implementation-wise, the process of localization
requires tight coupling with the target infrastructure, its capabilities,
policies, and specific configurations.

The implementation of the localization process becomes challenging in presence
of heterogeneous infrastructures. The same executable may require different
localization operations depending on where it will be executed. More
importantly, different types of localizations may or may not be available or
allowed in some infrastructures. This imposes decision capabilities on the task
scheduler and the development of a robust interoperability layer between the
\pilot system and the heterogeneous target infrastructures.

Interoperable localization offers the opportunity to execute heterogeneous
executables via \pilot systems. As seen in~\S\ref{sec:history}, the abstraction
of \pilot was progressively developed in a distributed context leaving
under-explored the execution of parallel applications by means of \pilot
systems. This is merely a historical byproduct, not an intrinsic limitation of
the \pilot abstraction and its implementations. For example, by leveraging
localization, different flavors of MPI libraries can be loaded and specific
execution commands may be used to run MPI applications via a task wrapper and a
\pilot agent.

The benefits of executing tightly-coupled parallel code via \pilot systems are
especially evident when considering workloads involving homogeneous tasks and
possibly diverse computational phases. For example, applications with separated
simulation and analysis phases [cit], or applications integrating multiple
types of analysis [cit], \ldots.\mtnote{To be expanded with better/further
examples of why we want to run MPI with pilots.}

% In the \pstar model and in the discussion so far we have mainly been
% concerned about the abstract notion of task, i.e., the application payload to
% be run on the DCI. While this abstraction is very natural, it does pose
% challenges in practice when we are operating in a real-life heterogeneous
% environment, i.e. what does ``run'' mean \ldots. And if we are able to define
% what it means, how do we pass that information down the stack?\mtnote{How
% this relate to MPI, OpenMP?}

% \paragraph*{Pilot and Middleware Layer}

The definition of the term `pilot' as a resource placeholder
in~\S\ref{sec:understanding} and the review of its implementations
in~\S\ref{sec:analysis} helps to explore the relation between \pilot systems
and middleware. The notion of pilot as a resource placeholder is more general
than that of a job as defined in the context of HPC and Grid middleware. This
is exemplified when considering cloud-based DCIs.

Within a IaaS [cit], the instantiation of virtual machines (VMs) introduces
overheads due to the bootstrapping of an operating system, including data and
networking subsystems. Nonetheless, VMs are instantiated without waiting into a
queue and have, as a consequence, largely predictable instantiation time. This
is relevant when compared to job queuing time and its unpredictability in HPC
and grid infrastructures [ref Vishal paper, AIMES paper].

\pilot can benefit from the more predictable VM instantiation time because VMs
can be used as resource placeholders. A pilot agent can be executed within a
VM, retrieving and executing tasks on the resources of (one or more) VM.
Conceptually, the role and functionalities of the \pilot agent are the same
when used in a IaaS, HPC, or grid infrastructure. The implementation details
are isolated within the agent code itself and have no bearing on the overall
design of the \pilot system.

This analysis highlights not only the generality of the notion of resource
placeholder but it also shows the independence of the \pilot paradigm from the
specificities of the infrastructures and their middleware. The \pilot paradigm
abstracts the notion of resources, alongside that of scheduling and task
execution, creating a well-defined and isolated logical space for the
management of the application execution.

% usually minimal when compared to the queuing time averagely experienced on
% Grid-based DCIs. Furthermore, the limitations on the execution time of a VM
% can be virtually absent, a limitation that on job-based middleware is imposed
% by the queue walltime. Clearly, overheads are introduced by instantiating VMs
% including their data and networking subsystems, and the model adopted within
% a IaaS-based DCI to assign resources to each VM can affect the flexibility of
% the \pilot system. A similar assessment could be done for a DCI deploying a
% PaaS model of cloud computing.

\mtnote{Use of DCIs for plural needs to be consistent across the paper.}

% \paragraph*{Pilot and Application Layer}

Importantly, an analogous degree of isolation is offered by the \pilot paradigm
to the application layer. The scientific application landscape is diverse and
fragmented, especially when considering distributed applications [cit]. The
current state of the workflow (WF) systems (a type of distributed application)
is a paradigmatic example of this situation.

Many WF systems have been implemented with significant duplicated effort and
limited means for extensibility and interoperability. One important
contributing factor to these limitations is the lack of suitable, open, and
possibly standard-based interfaces for the resource layer. Most WF engines are
developed with proprietary solutions to access the resource layer, solutions
that cannot be shared with other engines and that often serve specific
requirements, use cases, and infrastructures.

The \pilot paradigm and a open \pilot API like the one specified in [ref
Pilot-API] offer the missing layer on which WF and, more generally, distributed
applications could be built upon. As with the resource layer, the \pilot
paradigm offers a well-defined and well-isolated layer between applications and
resources. This fosters extensibility, interoperability, and modularity by
separating the application description and logic from the management of its
execution, and from the provisioning and aggregation of resources. In turn,
this avoids the need to develop special-purpose, vertical, and end-to-end
applications, the main source of duplication and fragmentation in the current
distributed application landscape.

% \paragraph*{Pilot and AAA}

The multi-level scheduling and the heterogeneity of the infrastructures used by
\pilot systems present several challenges for the authentication,
authorization, and accounting processes (AAA). As seen in~\S\ref{sec:analysis},
AAA requirements for \pilot systems are limited to: (i) the multitenancy of the
instantiated \pilots; and (ii) to the credentials used to submit \pilots to
the remote infrastructure and to use them to execute the application tasks.

The authentication and authorization used by the user to access her own \pilots
is a well-isolated implementation detail while the credential used for \pilot
deployment depends on the target infrastructure. \pilot systems that do not
implement multitenancy for each pilot instance have no need to implement shared
AAA as it is instead required when virtual organizations need to share one or
more pilots.

The requirements of the target infrastructures are a diverse and often
inconsistent array of mechanisms and policies. These affect the deployment of
\pilots and, possibly, the binding of tasks to pilots. This is especially the
case when HPC, grid, and cloud systems are targeted by the same \pilot system
implementation. The \pilot paradigm is gregarious in the face of such a
diversity as no AAA requirements are imposed on the resource layer. \pilot
systems just need to present the credentials required by the resource layer
consistently with the infrastructure policies. Such credential are supposed to
be provided by the application layer therefore reducing to a minimum the AAA
implementation requirements for the \pilot systems.

% Because the \pilot can run as a different ``user'' than the ``owner'' of the
% \vocab{Task} there are challenges for many aspects of Authentication,
% Authorization, and Accounting (AAA). The issue mainly arises because
% multi-level scheduling decouples the DCI user from the task} owner. While
% technically one could make the choice not to be concerned about this
% decoupling and only be concerned with the DCI credential of the Pilot, in
% practice this does not hold true. [ref glexec].\mtnote{Need to be discussed
% and then extended.}

% \paragraph*{Pilot and the Enterprise}

Moving away for the relationship between the \pilot paradigm and critical
implementation details associated with its use in a scientific,
production-grade computing environment, the \pilot paradigm has relevant
implications also for the Enterprise sector. This connection can be traced back
to the increased adoption of distributed computed paradigm into the private
sector.

Over the past years, Hadoop~\cite{hadoop} emerged as distributed computing for
data-intensive tasks in the enterprise sector. While early versions of Hadoop
were monolithic, tightly coupling the Map Reduce programming framework to the
underlying infrastructure resource management, Hadoop~2 introduced the
YARN~\cite{yarn-paper} resource manager to support not only Map Reduce but also
heterogeneous workloads.

YARN supports multi-level scheduling enabling the application to deploy their
own application-level scheduling routines on top of Hadoop-managed storage and
compute resources. With YARN managing the lower resources, the higher-level
runtimes typically use an application-level scheduler to optimize resource
usage for the application. Applications need to initialize their so-called
Application-Master via YARN; the Application Master is then responsible for
allocating resources in form of so called containers for the applications. It
then can execute tasks in these containers.

As with the HPC environment, the support for different application workloads
and job types as long-lived vs. short-lived applications, or homogeneous vs.
heterogeneous tasks is challenging. \pilot-like frameworks are emerging for
YARN to address such a challenge. Llama~\cite{llama} offers a long-running
application master for YARN designed for the Impala SQL engine. TEZ~\cite{tez}
is a DAG processing engine primarily designed to support the Hive SQL engine
allowing the application to hold containers across multiple phases of the DAG
execution without the need to de/reallocate resources.
REEF~\cite{Chun:2013:RRE:2536274.2536318} is a similar runtime environment that
provides applications a higher-level abstractions to YARN resources allowing it
to retain memory and cores supporting heterogeneous workloads.

\jhanote{We should briefly discuss cluster management and container advances
(e.g. Docker) and their relation to pilots} \mtnote{Is this still valid?
Apologies but I am not sure what comments are old and what are new. If so, I
will have a go at it with a paragraph but I am by no means an expert of these
technologies.}

% -----------------------------------------------------------------------------
% 5.3
%
\subsection{Future Directions and Challenges}
\label{sec:future}

The \pilot landscape is currently fragmented with a high degree of duplicated
effort and capabilities. The reasons for such a balkanization are to be traced
back mainly to two factors: (i) the relatively recent discovery of the
importance of the \pilot paradigm; and (ii) the development model fostered
within academic institutions.

As seen in~\S\ref{sec:history} and~\S\ref{sec:analysis}, \pilot systems emerged
as a pragmatic solution for improving the throughput of distributed
applications. \pilot systems were not thought from the beginning as an
independent class of middleware but, at best, as a module within a specific
framework. This not only promoted duplication of development effort across
frameworks but also hindered the appreciation for the breath of the \pilot
abstraction, the underlying theoretical framework to the \pilot systems.

\pilot systems inherited the development model of the scientific projects
within which they were initially developed. Most of the early \pilot systems
were developed to serve a specific use case, often within the remit of a
specific research project. As a consequence, these systems were not engineered
to promote (re)usability, modularity, well-defined interfaces, or long-term
sustainability. Furthermore, code was not build around a community, following
recognized and widely-accepted coding standards.

All this is likely to change with the progressive appreciation of the
generality, richness, and versatility of the \pilot abstraction. The ongoing
re-engineering of PANDA and the development of RADICAL-Pilot indicate that the
\pilot paradigm is becoming the leading approach for large scale scientific
applications.\mtnote{we may want to add some data here, showing the staggering
amount of resources consumed by means of these two pilot systems.}

It should be noted that `large scale scientific applications' includes both
distributed and parallel applications executed on diverse types of
infrastructure. This shows how historical distinctions between, for example,
HPC and HTC are progressively loosing their meaning when considering resource
provision and application execution by means of \pilot systems.

As argued in the previous section, \pilot systems are agnostic towards the type
of application that is executed. They need to be engineered so to support
different types of applications but the \pilot abstraction and its underlying
notions of resource placeholder and multi-level scheduling do not require for
the application to be distributed, parallel, MPI, closed-coupled, or
communication independent.

Pilots holds resources and the properties of such resources can be arbitrarily
specified. Analogously, pilots allow for tasks to be scheduled but the type of
matching between the requirements of these tasks and the capabilities offered
by the resources held by the pilot is not mandated by the \pilot paradigm
itself. For this reason, supporting diverse types of resources and applications
reduces to a set of implementation details when adopting the \pilot paradigm.

% It is relevant to stress how \pilot systems allow for the unification of the
% execution process of diverse type of applications outside the remit of the
% middleware of target resources. This offers the opportunity to overcome one
% of the main limitations encountered when developing other types of
% middleware, especially those supporting grid computing [cit].

% \paragraph*{Performance and Scalability}

Scalability and performance are two important elements that should see further
development in the \pilot system landscape. The ongoing increase of the scale
required by scientific applications [cit] poses new set of challenges to the
development of effective \pilot systems. Increase in scale brings the need to
focus on federation of resources, integration of leadership machines, and
scaling-out to cloud infrastructures. Furthermore, large scale executions make
fault-tolerance and advance mechanisms for fault-recovery essential. \pilot
systems need to be able to rerun those few tasks that failed among the millions
that have been successfully executed. Analogously, the failure of a \pilot
among the many instantiated must not be an unrecoverable event.

With scale, performance becomes an issue. Not only in terms of overall task
throughput but, more subtly and yet not less relevant, also in terms of
overheads imposed by the \pilot system on the tasks execution. Such overheads
need to be quantifiable and accountable for. Especially in a context of
heterogeneous types of applications, \pilot overheads may be relevant to decide
what type of resource to target, how many pilots to instantiate, or whether a
\pilot system should be used at all.

% \paragraph*{Generality or Optimization} The existence of many \pilot systems
% is tempting to frown at from a perspective of Not-Invented-Here-Syndrome.
% Workload management. [this has a clear link to the pilot and application
% layer paragraph]\mtnote{Happy to write this after discussing what was the
% original idea. Should we move this to future trends?}

While most of the analysis offered in this paper supports the conclusion that
the number of \pilot systems should be reduced, it does not support the idea
that one solution should fits all. As with other types of middleware [cit] the
problem is not to eliminate special purpose systems in favor of a single
solution but it is, instead, to have both depending on the application and use
case requirements. A multi-purpose, functionally encompassing \pilot system is
desirable for all those use cases in which applications are heterogeneous in
the type of computation, time, or space. For specific applications designed to
run on a single type of infrastructure, a special purpose system might still be
more effective.

\mtnote{Consider whether to add a paragraph about interoperability among \pilot
systems. I would probably not do it as it does not seem of primary relevant to
me.}

% \paragraph*{Sociotechnical} \jhanote{What is the future of PJ?};
% \jhanote{\pilotjobs have potential, but it is not being realized due to
% ad hoc nature of theory and practise,}; Fragmentation/balkanization of the
% pilot landscape. While in this paper we also talk about HTC and HPC and their
% differences and commonalities, we have referred to an existing and growing
% demand for convergence between the two from an application perspective. [ref
% MTC?] HPC users are no longer exclusively interested in running one large
% job, but applications become more versatile and dynamic and are conflicting
% with the conventional thougths and operational realities on how to ``use''
% HPC resources. [ref cray, cram, etc.] If the HPC world is going to accept
% this mode of operation, then recognizing, embracing and investing in the
% \pilot Paradigm is inevatable.

% \paragraph*{Engineering} \jhanote{Why should we do to enhance the
% usability?}; \aznote{I would argue that our work is important to
% \pilotjobs because by expressing a common model, we enable researchers
% to 1) understand the commonalities between existing \pilotjob approaches
% in order to 2) motivate innovation + construction of ``next-gen''
% \pilotjob systems. E.G., implement the basics (or work from an existing
% system) + understand where the boundaries/unexplored territory is
% without having to first completely understand all 15+ existing \pilotjob
% systems and their unique vocabulary.}

% While we are in no way to argue for a one-size-fits-all \pilot we do believe
% that our work has shown that there is enough common ground that establishing
% a new recognized layer in the stack would not only help conceptually, but
% would also pay-off from a practical engineering standpoint. There are many
% intricate details for example in the runtime environment, that sharing
% efforts on that layer would allow the various systems to focus their
% attention where there is less overlap and more tailored to their own
% communities. [ref Open RunTime Environment] Standardisation?

\paragraph*{Contributions} \jhanote{(i) we provide first comprehensive
historical and technical analysis, (ii) set the stage for a common conceptual
model and implementation framework, and (iii) provide insight and lessons for
other tools and higher-level frameworks, such as Workflow systems possibly}
\mtnote{(iii) is probably already addressed. I think we will need to speak
about (i) and (ii) as a form of conclusion to the paper adding also the
contributions offered by Section 4 and 5. I leave these as the last paragraphs
to write after the second pass and consolidation of the whole draft.}

% -----------------------------------------------------------------------------
% \item \textbf{Pilot abstraction} as a well-defined, independent
% distributed system abstraction. \jhanote{state clearly how section 3
% and 4 help us define what a pilot is, the necessary and sufficient
% conditions (if possible), discuss the classifiers and apply/discuss
% them to the set of pilot systems here.} ``pilot-abstractions works!''
% , p* as a model ok. Our initial investigation~\cite{Luckow:2008la}
% into \pilot- Abstractions was motivated by the desire to provide a
% single conceptual framework --- referred to as the P* Model, that
% would be used to understand and reason the plethora and myriad
% \pilotjob implementations that exist.

% \item \textbf{Generality of the pilot abstraction} when compared to
% different types of resources. Is pilot useful only for compute
% resources? What about pilot data? Pilot network? Other types but
% compute, data, network?

% \item \textbf{Revisit the myths about pilot abstraction}: how they
% evolve from being a simple hack to get around queuing/scheduling
% policies to become a first class citizen of the abstractions for
% distrubuted systems. What are the characteristics/properties that make
% the pilot abstract a first class citizen? Do we notice a progressive
% evolution in the multiple implementations we introduced and described
% in Section 4?

% \item \textbf{Relationship between pilot abstraction and different
% types of middleware}: grids, cloud, HPC.

% \item \textbf{Pilot abstraction and `lessons for the workflow
% systems'}.

% \mtnote{Do we want to put this at the end of S4, in the analysis that
% we will have to write in 4.3?}

% \section*{Notes - to be commented out by their authors}

% \jhanote{The question is what are the fundamental ``concepts''. It is
%   not necessary that the concepts have a specific implementation or
%   map to a component in \pilotjob system. As we know there are
%   different ways in which tasks get committed to the \pilotjob
%   system. One possible primary concept is that of logical grouping;
%   all tasks are committed to a logical grouping -- where the grouping
%   is such that all entities in this group will be executed
%   (disregrarding details of how this grouping will happen, or who will
%   perform the execution).  It appears that the concept of logical
%   grouping of tasks is a fundamental one, and avoids a requirement of
%   any further specification of of details of who/where/when; if so,
%   then the notion of a pool can be dispensed with, which will have the
%   advantage of liberating us from the requirement of imposing on the
%   the manager the need to push/pulling tasks from a pool.}

% \mtnote{The paragraph relative to this comment is gone --- no more
% pool concept. I do like the idea of grouping though so I would suggest
% to wait for the entire Section to stabilize further and then see
% whether we can add/extend a paragraph by introducing the concept of
% grouping.}

% \jhanote{AppLeS is not strictly \pilotjob based?  but \pilotjob like
%   capabilities?} \alnote{The question is: when is a \pilot a \pilot?
%   When the use the term \pilot and when \pilot-like? Apples has a
%   component -- the Actuator (Quote from paper: "... handles task
%   launching, polling, and cancellation ..."), which is quite similar
%   to a \pilot. But maybe AppLeS is something for the history section
%   or a separate category for Master/Worker frameworks...  PJ evolved
%   from the need to map master/worker style computations to
%   heterogeneous, dynamic distributed grid environments. Added a
%   pre-\pilot category to the history sub-section.}

% \jhanote{I think all the functionality of PJs is predicated on the
%   following core capability: enable the decoupling between assigning a
%   workload from the spatial/temporal execution properties of its
%   execution. In condor, this is mentioned as ``separating between job
%   scheduling and resource''. This get mentioned somewhere in core
%   functionality?}

% \jhanote{deployment and provisioning are not the same in my
%   opinion: provisioning is about resources, i.e., provisioning is not
%   the same as scheduling, it is more like arranging. Deployment is
%   about ``setting up T=0 requirements, which could be software
%   environment, input/data dependencies, as well as resources.''}

% \aznote{Some additional thoughts for this section... 1) The early
% history of \pilotjobs seems to have a few ``independent'' \pilotjob
% approaches, where systems (eg MyCluster/AppLeS) incorporate \pilotjob
% techniques with sometimes wildly differing implementations/vocabulary
% but when boiled down, offering the same attempts at base/minimal
% functionality.  This helps push the value of the approach (multiple
% groups working on it independently helps imply its value), with
% increasingly complex \pilotjob systems coming out as time pushes on.
% Our review aids both in the construction of these increasingly complex
% systems by exposing their core functionality + allowing researchers/etc
% to push the field forward by reducing the apparent complexity to a set
% of common terms/comparisons/classifiers/etc.  I don't know if this is
% too prescriptive/hand-wavey -- feel free to ignore if so, but I feel
% that the essence of this is one potential contribution of our work.}

\section*{Acknowledgements}
{\footnotesize{This work is funded by the Department of Energy Award
    (ASCR) DE-FG02-12ER26115 and NSF CAREER ACI-1253644. This work has
    also been made possible thanks to computer resources provided by
    TeraGrid TRAC award TG-MCB090174 and BiG Grid. This document was
    developed with support from the US NSF under Grant No. 0910812 to
    Indiana University for ``FutureGrid: An Experimental,
    High-Performance Grid Test-bed''.}}

% \bibliographystyle{IEEEtran}
\bibliographystyle{abbrv}
\bibliography{pilotjob,literatur,saga,saga-related,urls,hadoop}


\end{document}
