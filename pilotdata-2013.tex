\documentclass{sig-alternate}
%\documentclass[conference]{IEEEtran}
%\documentclass[conference,final]{IEEEtran}

\input{head}
\input{include}

\usepackage{listings}

\lstnewenvironment{code}[1][]%
{
\noindent
%\minipage{0.98 \linewidth} 
\minipage{1.0 \linewidth} 
\vspace{0.5\baselineskip}
\lstset{
    language=Python,
%    numbers=left,
%    numbersep=4pt,
    frame=single,
    captionpos=b,
    stringstyle=\ttfamily,
    basicstyle=\scriptsize\ttfamily,
    showstringspaces=false,#1}
}
{\endminipage}

\begin{document}
\conferenceinfo{HPDC'13}{2013, New York, USA}
% \conferenceinfo{ECMLS'11,} {June 8, 2011, San Jose, California, USA.}
% \CopyrightYear{2011}
% \crdata{978-1-4503-0702-4/11/06}
% \clubpenalty=10000
% \widowpenalty = 10000

\title{Pilot-Data: An Abstraction for Distributed Data}

% \alignauthor
% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{1932 Wallamaloo Lane}\\
%        \affaddr{Wallamaloo, New Zealand}\\
%        \email{trovato@corporation.com}
% % 2nd. author
% \alignauthor
% G.K.M. Tobin\titlenote{The secretary disavows
% any knowledge of this author's actions.}\\
%        \affaddr{Institute for Clarity in Documentation}\\
%        \affaddr{P.O. Box 1212}\\
%        \affaddr{Dublin, Ohio 43017-6221}\\
%        \email{webmaster@marysville-ohio.com}
% % 3rd. author
% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
% one who did all the really hard work.}\\
%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%        \affaddr{Hekla, Iceland}\\
%        \email{larst@affiliation.org}
% \and  % use '\and' if you need 'another row' of author names
% % 4th. author
% \alignauthor Lawrence P. Leipuner\\
%        \affaddr{Brookhaven Laboratories}\\
%        \affaddr{Brookhaven National Lab}\\
%        \affaddr{P.O. Box 5000}\\
%        \email{lleipuner@researchlabs.org}
% % 5th. author
% \alignauthor Sean Fogarty\\
%        \affaddr{NASA Ames Research Center}\\
%        \affaddr{Moffett Field}\\
%        \affaddr{California 94035}\\
%        \email{fogartys@amesres.org}
% % 6th. author
% \alignauthor Charles Palmer\\
%        \affaddr{Palmer Research Laboratories}\\
%        \affaddr{8600 Datapoint Drive}\\
%        \affaddr{San Antonio, Texas 78229}\\
%        \email{cpalmer@prl.com}
% }

\date{}
\maketitle

\begin{abstract} 


\end{abstract}

\section{Introduction} 

Data has become a critical factor in many science disciplines~\cite{hey2009}.
The data generated by scientific applications, scientific instruments and
sensors is experiencing an exponential growth in volume and complexity. The
ability to analyze prodigious volumes of data requires flexible and novel ways
to manage data, distribution, and computations~\cite{Jha:2011fk}. Not only is
the volume of data increasing, but also the diversity of data formats and the
velocity of change. These properties are also referred to as the 3V's of Big
Data: volume, velocity and variety~\cite{ibm_bigdata_book}. Also, data and 
computation has become increasingly distributed requiring the integration of 
different computational methods, compute resources and data~\cite{nsf_aci}.


Currently, a gap between data management capabilities (such as Hadoop) and
analytical capabilities (as provided by systems as R, SPSS) exists. While
there are multiple attempts in bringing analytical capabilities close to the
data, e.\,g.\ traditional databases started to integrate analytical libraries,
such as R, into their database system: Oracle and Postgresql e.\,g.\ support
the usage of R directly within the database.
Madlib~\cite{Hellerstein:2012:MAL:2367502.2367510} is an analytical, machine
learning library designed to run within a PostgreSQL or
Greenplum~\cite{greenplum} database. Similarly, Hadoop~\cite{hadoop} gained
massive popularity as low-cost, scale-out data store and processing platform.
Nevertheless, the majority of science applications still operate in legacy
modes, e.\,g.\ they often require manual data management (e.\,g.\ the stage-in
and out of files). Advanced analytics require a sophisticated of mix data
management operations (e.\,g.\ the distribution of data, the generation of
samples, etc.) and analytics capabilities (e.\,g.\ provided by custom tools or
domain-specific libraries, such as BWA for sequencing, R for statistics and
machines learning etc.).


In this paper, we explore the usage of \pilotjobs as a way to efficiently
managed distributed data and computation. \pilotjobs support effective
distributed resource utilization, and are arguably one of the most widely-used
distributed computing abstractions. \emph{\pilotdata (PD)} is a novel
abstraction for data-intensive applications that provides late-binding
capabilities for data by separating the allocation of physical storage and
application-level data units. Further, it provides an abstraction for
expressing and managing relationships between data units and/or compute units.
These relationships are referred to as \emph{affinities}.



\section{Terms and Definitions}


Figure~\ref{fig:figures_terminology} illustrates the possibly levels of
distributions. Distribution can occur on many levels. In the following, we
refer to a local distribution if data/compute resides on multiple nodes within
on data center. Highly distributed describes the distribution of data and/or 
compute across multiple data centers.


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{figures/terminology.pdf}
	\caption{Levels of Distribution \jhanote{be sure to
            distinguish between level and degree. Also, use this
            figure to capture the different functional components of
            an abstract architecture for data-intensive
            cyberinfrastructure} }
	\label{fig:figures_terminology}
\end{figure}

% TODO: Layered Diagram, Terminology, Levels, Granularity (distributed can have 
% different granularities)


\section{Challenges of Data-Intensive Science}



\section{Related Work}



\subsection{Scientific Data Management}

Several different tools, e.\,g. BitDew~\cite{Fedak:2008:BPE:1413370.1413416},
SRM~\cite{srm-ogf}, iRODS~\cite{Rajasekar:2010:IPI:1855046}, Condor
Matchmaking~\cite{Raman:1998:MDR:822083.823222} and HDFS/Hadoop~\cite{hadoop},
address the general ideas associated with distributed data management. Most of
the tools introduce the concept in semantically inconsistent and implicit
ways; consequently, affinities are not one of the degrees-of-freedom exposed
to or controllable at the application-level (i.e. by the developer/user).
Furthermore, few existing (distributed) programming models provide explicit
support or control for affinity~\cite{ideas}.


Notes from discussion:
\begin{itemize}
	\item What role do SQL and Non-SQL database play?
\end{itemize}

\note{We need to know how the data is structured in order to efficiently manage it?
application-knowledge is the key in order to reduce the amounts of necessary transfers
}
"Distributed Processing of Very Large Datasets with
DataCutter", by Beynon et. al, in Parallel Computing, Vol. 27, No. 11,
2001 

"Caravela: A Novel Stream-Based Distributed Computing
Environment", in IEEEE Computer, Vol. 40, No. 5, 2007.

Jungle Computing \\
http://www.cs.vu.nl/~fjseins/Papers/Conferences/maassen-3dapas2011.pdf \\
\url{https://github.com/saga-project/radical.wp/blob/master/journal_club/Drost-HCW2012.pdf}



\subsection{Pilot-based Approaches for Distributed Data}

Distributed Cloud Storage Services with FleCS Containers Hobin Yoon,
Madhumitha Ravichandran, Ada Gavrilovska, Karsten Schwan, 5th
OpenCirrus Summit, Moscow, Russia, June 2011.

\url{http://www.cc.gatech.edu/~ada/papers/opencirrus11.pdf}


Volley: automated data placement for geo-distributed services

\subsection{Prior Work}

Whereas we will discuss in greater detail some of the concepts upon which this
paper is built, for completeness we briefly outline them here.

The seamless uptake of distributed infrastructures by scientific
applications has been limited by the lack of pervasive and
simple-to-use abstractions at multiple levels – at the development,
deployment and execution stages. Of all the abstractions proposed to
support effective distributed resource utilization, a survey of actual
usage suggested that \pilotjobs were arguably one of the most
widely-used distributed computing abstractions – as measured by the
number and types of applications that use them, as well as the number
of production distributed cyberinfrastructures that support them.  Our
initial investigation~\cite{Luckow:2008la} into \pilot-Abstractions
was motivated by the desire to provide a single conceptual framework
--- referred to as the P* Model, that would be used to understand and
reason the plethora and myriad \pilotjob implementations that exist.
%\pilotjob framework
%that would work over heterogeneous and myriad types of grid middleware.

A specific implementation of the P* model that both unified
conceptually as well as was interoperable across myriad grid
middleware, led to BigJob~\cite{saga_bigjob_condor_cloud} -- a
SAGA-based \pilotjob, which was used to support a range of
applications, ranging from uncoupled ensembles of molecular dynamics
(MD) simulations~\cite{saga_bigjob_condor_cloud}, to Ensemble-Kalman
filter based applications~\cite{gmac09} with global synchronization to
loosely-coupled MD simulations with pair-wise
synchronization~\cite{async_repex11}.  Although BigJob provided the
syntactical uniformity, i.e., a common API and framework for different
grids, our experience led us to understand that different \pilotjobs
had different semantics and capabilities, and made vast if not
inconsistent assumptions of applications/users. This motivated our
efforts in search of a common minimally complete model of \pilotjobs,
and resulted in the \pstar model~\cite{pstar12}.

Once a common and uniform conceptual model was available, the notion
of \pilotdata was conceived using the power of symmetry, i.e., the
notion of \pilotdata was as fundamental to dynamic data placement and
scheduling as \pilotjobs was to computational tasks. As a measure of
validity, the \pstar model was amenable and easily extensible to
\pilotdata.  The consistent and symmetrical treatment of data and
compute in the model led to the generalization of the model as the
{\it P* Model of Pilot Abstractions}.

We explored the potentials of \pilot-Abstractions in conjunction with
different applications and application frameworks, e.\,g.\
Pilot-MapReduce~\cite{Mantha:2012:PEF:2287016.2287020} provides a \pilot-based
implementation of the MapReduce programming model, as well as different 
infrastructures (e.\,g.\ clouds~\cite{pilotdata-cloud-2012}).


\section{Infrastructure for Data-Intensive Applications}

In distributed settings, storage is often a black box for the application with
unknown quality of services, i.\,e. the application usually does not know what
bandwidths and latencies it can expect. Furthermore, physical and logical
distribution implies higher penalties for inefficient data access as the data
traverses several hardware and software layers. 

% In general, different types of 
% storage exist:
% \begin{enumerate}
% 	\item \textbf{Local Storage:} describes local hard disk directly attached 
% 	to the compute resource.
% 	\item \textbf{Network Filesystem:} refers to different forms of 
% 	distributed (possible parallel) filesystems. The filesystem is commonly 
% 	exported via the Posix API and a virtual filesystem layer.
% 	\item \textbf{Distributed Storage:} refers to a highly distributed type of 
% 	storage systems that spawn across multiple data centers. Access to such 
% 	storage systems via a common -- often simplified -- namespace and API. For 
% 	example, cloud systems, such as the Azure Blob Storage, Amazon S3 and 
% 	Google Storage, provide only a namespace with a 1-level hierarchy. 
% \end{enumerate}


In general, file-based cloud storage can be
classified as follows: (i) local storage refers to local hard disk
directly attached to the compute resource, (ii)
remote storage refers to different forms of distributed (possible
parallel) filesystems than can be attached to a VM either as block
storage device or NAS. Both type (i) and (ii) storage is commonly
integrated via the Linux virtual filesystem layer and thus, is
suitable in particular for legacy, file-based applications. 


Object stores (type (iii) are highly distributed storage systems that can
potentially spawn across multiple data centers. Object stores are optimized
primarily for “write once, read many” workloads and can support massive
volumes of data with their scale-out architectures. These stores are
particularly designed for Big Data workloads that (i) require the storage of
large volumes of data and (ii) have access pattern involving large amounts of
sequential reads. The most widely used object stores are: Amazon
S3~\cite{amazons3}, Azure Storage~\cite{azure-blob-storage} and Google Cloud
Storage~\cite{google-storage}. Further, several on-premise object stores
emerged: Eucalyptus Walrus~\cite{walrus}, OpenStack
Swift~\cite{openstack-swift}, GlusterFS~\cite{glusterfs} and
Lithium~\cite{Hansen:2010:LVM:1807128.1807134}.




While the coupling between compute and data in type (i) and (ii) is very high,
it is only small in the later case. In general, the buffer type (iii) storage
are more optimized to reliably storing large volumes of data and achieving
high read throughputs.


Table~\ref{tab:storage-systems} shows an overview of distributed storage 
systems. The focus of this analysis are file-based storage systems. Structured
storage types (e.g. relational databases) and key-/values stores are not 
considered.

\begin{table*}[t]
\centering
\begin{tabular}{|p{1.7cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.4cm}|p{1.4cm}|p{1.3cm}|p{1.2cm}|p{1.2cm}|}
	\hline
	\textbf{Storage Type} &\textbf{Azure} &\textbf{Amazon} &\textbf{Google} &\textbf{Open\-Stack} &\textbf{Euca\-lyptus} &\textbf{XSEDE}  &\textbf{OSG} &\textbf{EGI} \\
	\hline
	Local	&yes &yes &yes &yes &yes &yes &yes &yes\\
	\hline
	Remote &Azure Drive &EBS &GCE Disks &Nova Volumes &EUCA Block Storage &Lustre, GPFS 
	&no &no\\
	\hline
	Distributed Storage &Azure Blob Storage &S3 &Google Storage &Swift & Walrus &GFFS
	 &SRM &SRM\\
	\hline	
\end{tabular}
\caption{\textbf{Storage Types Supported by the Different Infrastructures:} 
Each infrastructure supports various storage options. Most commonly, storage 
is mounted as a local or remote filesystem. Distributed object storage 
provides some interesting characteristics for data-intensive applications. 
\label{tab:storage-systems}}
\end{table*}

For example, HPC infrastructures, such as XSEDE, typically provides access to
a parallel filesystem, such as Lustre~\cite{lustre} or
GPFS~\cite{Schmuck:2002:GSF:1083323.1083349}. On the Open Science Grid (OSG)
only local storage is available. In addition, both infrastructures provide or
are going to deploy distributed storage services: The Global Federated File
System (GFFS)~\cite{gffs} is currently evaluated in the context of XSEDE. GFFS
provides a global namespace on top of various XSEDE storage resources. The
system handles file movement, replication etc. transparently. OSG deploys
several SRM-based storage services~\cite{srm-ogf}.


\subsection{Distributed Data Management}

Storage Resource Manager (SRM) are a type of storage service that provide a
dynamic file management capabilities for shared storage resources via a a
standardized SRM interface~\cite{srm-ogf}. The SRM manager provides a logical
namespace on top of multiple physical storage areas. It was originally
designed for managing tape - hard disk storage hierarchies. Examples for SRM
implementations are DCache and Castor. While it is possible to build
geographically distributed systems based on the SRM specification, SRM is
primarily designed as an access layer to different site-specific storage
technologies. While SRM aims to hide the complexity of different lower-level
storage technologies, it limits the ways application can control and reason
about data locality. Further, while some infrastructures provide some means to
manage SRM and compute co-location, it is generally necessary to copy the file
out of the SRM before doing computation.

iRods~\cite{Rajasekar:2010:IPI:1855046}




\begin{table*}[ht]
	\centering
\begin{tabular}{|p{2.5cm}|p{2.3cm}|p{2.3cm}|p{2.3cm}|p{2.3cm}|p{2.3cm}|}
		\hline
		&\textbf{XSEDE} &\textbf{SRM} &\textbf{iRODS} &\textbf{Clouds} &\textbf{Hadoop}\\
		\hline
		Transfer &SCP, GridFTP &GridFTP &GridFTP &HTTP &HDFS, HTTP\\
		\hline
		Storage &Local disk, parallel filesystems  &Storage Element &Data 
		Server &S3, GS &Local Disk\\
		\hline
		Management &GFFS &SRM Manager &iRods Metadata Catalog &Object Store 
		Manager    
		&Namenode\\
		\hline
		Distribution &Geographic &Local &Geographic
		&Geographic, Local &Local\\
		\hline
		Data/Compute Locality &Data center, Machine &Data center &Data center 
		&Data center &Machine, Node\\
		
		\hline
\end{tabular}
\caption{Data-Management Infrastructures}
\end{table*}


Transfer Protocols:
\begin{itemize}
	\item bcp, \url{http://pcbunn.cithep.caltech.edu/bbcp/using_bbcp.htm}
\end{itemize}





Criteria/Requirements:
\begin{itemize}
	\item unstructured vs. semi-structured vs. structured data
	\item Volume of the data
	\item Access patterns
\end{itemize}

\alnote{Why do we exclude databases?}

These data store types are suitable mainly for structured/semi-structured and 
medium size data:
\begin{itemize}
	\item In-memory, key-value Stores
	\item Document Stores like MongoDB, CouchDB 
	\item SQL-based databases
\end{itemize}

Scale-out data stores for BigData:
\begin{itemize}
	\item Swift
	\item Walrus
	\item Cassandra
	\item Hadoop: HDFS/HBase
\end{itemize}


Highly distributed storage systems:
\begin{itemize}
	\item iRods
	\item Google Spanner
\end{itemize}


While some distributed storage systems provide support for data/compute 
co-location (e.\,g.\ iRODS~\cite{Rajasekar:2010:IPI:1855046}), commonly 
applications require a direct access to files via a Posix-based API.

\subsection{Data-aware Resource Manager}

A key feature of most data management system is the capability to decouple a
logical piece of data from its actual physical location. The system can then
flexibly manage data location and replicas, e.\,g.\ to ensure low access times
from a compute resources. The core of a data management system is the data
scheduler.

Mesos~\cite{Hindman:2011:MPF:1972457.1972488} is a data-aware scheduling
system developed at the University of California, Berkley. The Mesos is
designed to support different platforms, e.\,g.\ Hadoop and MPI. The framework
utilizes delay scheduling, which enables data-intensive applications to
achieve data locality by waiting for slots on nodes that contain the input
data.

Yarn~\cite{yarn} is the resource scheduler of the next generation Hadoop 2.0
framework. It is specifically designed for data-intensive applications that
are built on top of Hadoop and HDFS. Yarn utilizes so called \emph{containers}
as primary scheduling unit. The application can request a set of containers
from the Yarn resource manager, which can then be used to run applications.
The Yarn scheduler supports data locality, i.\,e.\ an application request to
request a container, which is local to the data. 

Both Mesos and Yarn enable applications to manage data/compute localities at a
very low level and with less flexibility than a \pilot-based approach.
Nevertheless, they provide the necessary system-level support for enabling
data/compute locality within shared cluster environments that is missing from
traditional resource management systems. Thus, combining these capabilities 
with a \pilot framework and the \pilot-API both provides a higher-level 
abstraction and an increasing amount of flexibility to the application.



\subsection{Research Questions}
\begin{itemize}
	\item How can one provide a interoperable, uniform access to these heterogeneous storage resources?
	\item How to manage distribution?
	\item How can these systems be used to implement affinities?
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{P* Model and Pilot-API: A Unified Model and Abstraction for 
Distributed and Dynamic Data}

The P* model~\cite{pstar12} is an attempt to provide the first minimal
but complete model for describing and analyzing \pilotjobs. We
particularly extended this model to \pilotdata (PD): PD provides
late-binding capabilities for data by separating the allocation of
physical storage and application-level \dataunits.  Similar to a
\pilotcompute, a \pilotdata provides the ability to create \pilots and
to insert respectively retrieve \dataunits. The
Pilot-API~\cite{pilot_api} provides an abstract interface to both
\pilotcompute and \pilotdata implementations that adhere to the P*
model.

\section{BigJob and BigData Framework}

We propose the use of \pilotdata as an application-level construct support
distributed data. In conjunction with \pilotjob, \pilotdata is able to support
different forms of data/compute affinities. Both abstractions are implemented
as reference implementation to the P* model -- a unifying, analytical
framework for \pilot abstractions -- introduced in~\cite{pstar12}. 
Similarly to \pilotjobs, \pilotdata allows the logical decoupling
of production and consumption of data. Among many things, \pilotdata 
facilitates the late binding of data and physical resources.  

The \pilotdata abstraction aims to provide the following capabilities:
\begin{compactitem}
\item \textbf{Distributed coupling and coordination:} Application
  components, e.\,g.\ the parts of a distributed workflow or a
  compute- and analysis job, need support for efficient distributed
  coordination.
\item \textbf{Description and management of data affinities and
    localities:} A \pd container represents a group of affine
  \dataunits; %Further affinities between several PD can be expressed;
  various meta-data, such as localities of all \dataunits and
  replicas, must be stored to facilitate scheduling and other types of
  decision making.
\item \textbf{Data-aware scheduling:} Utilizing the hints given to PD
  in form of affinity specifications, the data-aware scheduler should
  be able to optimize data localities, movements and
  streams. Depending on the current system state, computations will be
  moved to data or vice versa. Replicas of \dataunits can be
  automatically maintained by PD to facilitate e.\,g.\ a faster
  access.
\item \textbf{Data access patterns and data streaming:} Data contained
  within PD needs to be accessed online and streamed to different
  locations facilitating e.\,g.\ interactive analyses.  PD should
  support common data access patterns, e.\,g.\ replication,
  partitioning and scatter/gather.
\item \textbf{Discovery:} Application components should be able to
  query the system for \dataunits and their associated
  \pilots. Further, the system will provide support for shared \dus,
  which can be queried and discovered.
\end{compactitem}

\jhanote{this should be reorganised: we should structure design
  objectives/aims of pilot data to (i) link/relate to the functional
  components of section 5 and (ii) link/relate to other features.
  Pilot-data support for Affinity could be parsed in multiple ways;
  the first could be data-aware resource mgmt/scheduling;
  alternatively it could be data-dependencies. In all of this remember
  to slowly increase emphasis on distributed aspects (and thereby
  lower the importance of performance on any one system}.


The abstractions enables the
developer to specify data affinities, which will be utilized by the RTS to
map data onto a particular infrastructure topology and to support an
application-specific access pattern, e.\,g.\ as found in MapReduce
applications. The RTS places \dataunits according to the defined
objectives; for example, a resource could accept only data objects
$>5$~GB and above.  Ref.~\cite{ddia_ptrsa10} suggests that the
explicit support for affinities between and within the \pj and \pd
abstractions for distributed and dynamic data will yield better
performance and scalability compared to simplistic if not ad-hoc ways
of data-compute coupling while still maintaining a high level of
simplicity.


\subsection{Implementation}

Consistent with the P* model, BigJob
(BJ)~\cite{saga_bigjob_condor_cloud} and BigData
(BD)~\cite{Mantha:2012:PEF:2287016.2287020} provide a unified runtime
environment for \pilotcomputes and \pilotdata on heterogeneous
infrastructures (see Figure~\ref{fig:figures_cloud_pilot_job}). 
\begin{figure}[t]
	\centering
        \includegraphics[width=0.45\textwidth]{figures/cloud_pilot_job.pdf}
                \caption{\textbf{BigJob Pilot Abstractions and
                    Supported Resource Types:} BigJob and BigData
                  provide a unified abstraction to compute and data
                  resources of both grid and cloud
                  environments. Resources are either accessed via
                  SAGA~\cite{ogf-gfd-90,saga-bliss-pd} or via a custom
                  adaptor.}
	\label{fig:figures_cloud_pilot_job}
\end{figure}

For this purpose, BigJob provides a higher-level, unifying interface
to heterogeneous and/or distributed data and compute resources. The
framework is accessed via the \pilot-API~\cite{pilot_api}, which
provides two key abstractions: \pilotjob and \pilotdata. Applications
can specify their resource requirements using a \pilot
description. \pilots are started via the \pilotcomputeservice
respectively the \pilotdataservice.

\pilotjobs eliminate the need to interact with different kinds of compute 
resources, e.\,g.\ batch-style HPC/HTC resources as well as cloud resources, 
and provide a unified abstraction for allocating resources. 

\begin{figure}[t]
	\upp	\upp	\upp
	\centering
	\includegraphics[width=0.45\textwidth]{figures/bigjob-bigdata-architecture.pdf}
	\caption{\textbf{BigJob and BigData Architecture:} The \pilot-Manager is 
	the central coordinator of the framework, which orchestrates a set of 
	\pilots. Each \pilot is represented by a decentral component referred to 
	as the \pilot-Agent, which manages the set of resources assigned to it. }
	\label{fig:figures_bigjob-bigdata-architecture}
\end{figure}

Similarly, \pilotdata removing the need to interoperate with different
data sources and stores, e.\,g.\ repositories, databases etc, by
providing a virtualized data service layer, which can be used to
allocated and access a heterogeneous set of data sources and stores.

Figure~\ref{fig:figures_bigjob-bigdata-architecture} shows the
architecture of BigJob/BigData. The \pilot-Manager is the central
entity, which manages the actual \pilots via the \pilot-Agent. Each
agent is responsible for gathering local information, for pulling
\computeunits from the manager. The framework relies on a central
coordination service based on Redis~\cite{redis} for communication
between manager and agent.

As shown in Figure~\ref{fig:figures_cloud_pilot_job} BigJob/BigData supports
various storage and compute resource types via a flexible plugin architecture.
For \pilotcomputes BJ supports various types of HPC/HTC resources via
SAGA/Bliss (e.\,g.\ Globus, Torque or Condor resources). Further, various
cloud plugins have been developed for supporting EC2-style backends as well as
Google Compute Engines. The plugin utilizes the native EC2 and GCE API for
launching the VM: for EC2 the Boto library~\cite{boto} and for GCE 
the Google Python API client~\cite{google-api-client} is used.
The EC2-API~\cite{amazonec2api} is the de facto standard for
managing VMs in clouds and can also be used for accessing different science
clouds, e.\,g.\ the Eucalyptus and OpenStack cloud of FutureGrid. Google
Compute Engine provides modern HTTP/REST/JSON and OAUTH for authentication.
Having started the VM, the \pilot-Agent is launched using SAGA and the SSH
adaptor of SAGA.

Similarly, a \pilotdata can be placed on different types of storage, e.\,g.\ a
local or remote filesystem or on object stores as often found in cloud
environments. Depending on the storage type different access respectively
transfer protocols are supported. For example, local/remote storage both on
grid and cloud resources can be managed via SSH. On production grid
infrastructures, such as XSEDE~\cite{xsede}, this kind of storage can also be
accessed via Globus Online~\cite{10.1109/MIC.2011.64}, which underneath relies
on GridFTP~\cite{ogf-gfd-20} for file movement etc. A particularity of cloud
environments are the described object stores. Object stores are typically
accessed via custom API typically based on HTTP/REST. For example, the Amazon
S3 API~\cite{amazons3api} is based on HTTP and XML and is also supported by
Eucalyptus Walrus and OpenStack Swift. Google Storage provides two APIs: an S3 
compatible and a JSON-based REST API. Again, Boto is used for S3-based storage 
resources and the Google Python API client for Google Storage.


\begin{table}[t]
	\centering
\begin{tabular}{|p{1cm}|p{1.2cm}|p{1cm}|p{1cm}|p{1cm}|p{1.2cm}|}
	\hline
	&\textbf{XSEDE} &\textbf{SRM} &\textbf{iRODS} &\textbf{Clouds} &\textbf{Hadoop}\\
	\hline
Pilot &Directory &SRM directory referenced by SURL & &Bucket&HDFS Directory\\
	\hline
\du &File &File &File &Object &HDFS Chunk\\
\hline
\cu &Job &- &- &VM &YARN App Container\\
	\hline
\end{tabular}
\caption{Mapping of Pilot Data to different infrastructures (clouds refer to Google Storage/Amazon S3/Microsoft Blob Storage)}
\end{table}

In general, SRM works on individual file basis \alnote{Can SRM work on directories or group of files?}.


\subsection{Pilot-API}

The Pilot-API~\cite{pilot_api} is an interoperable and extensible API which
exposes the core functionalities of a Pilot framework via a unified interface
providing a common API that can be used across multiple distinct production
cyber infrastructures. It offers a unified API for managing both compute and
data pilots as well as application workloads. Both BigJob and BigData provide
a full implementation of the Pilot-API and enable the management of resources,
compute \& data units as well as the relationships between them. Specifically,
the Pilot-API promotes affinities as a first class characteristic for
describing such relationships between compute and data elements and to support
dynamic decision making.

The API separates the concerns (i) management of pilots and resources and (ii)
application workload management (see Figure~\ref{fig:figures_pilot-api}). The
\pilotcomputeservice is responsible for controlling the lifecycle of
\pilotcomputes; the \pilotdataservice managed \pilotdata. The application
workload is submitted via the ComputeDataService, which accepts both
ComputeUnits (CUs) and DataUnits (DUs). A CU represents a primary
self-containing piece of work, while a DU represents a logical set for data.

\begin{figure*}[htbp]
	\centering
		\includegraphics[width=0.8\textwidth]{figures/pilot-api.pdf}
	\caption{Control Flow Pilot-API: The API exposes two primary functionalities: The PilotComputeService and PilotDataService are used for the management of \pilotcomputes and \pilotdata. The application workload is submitted via the \computedataservice.}
	\label{fig:figures_pilot-api}
\end{figure*}

describe entities

usage modes...

start pilots, reconnect to pilots, cu, dus


how to manage data and compute dependency: e.g. a produced result set might be 
consumed by multiple cu (in possible multiple locations)



\subsection{Affinities and Scheduling}


\jhanote{Affinity related work. Discuss different contexts in which
  affinity has been implemented and used. how does our usage differ}

\note{Why are we using affinity labels? What other models could be used?
Can we model XSEDE with our affinity}

An important consequence of data and computation as equal first-class
entities, is that either data (D) can be provisioned where computation
(C) is scheduled to take place (as is done traditionally), or C can be
provisioned where D resides. This equal movement of computational
units leads to a richer set of possible correlations between the
involved data elements and computational units; correlations can be
either spatial and/or temporal. These correlations arise either as a
consequence of constraints of localization (e.g., D is fixed, C must
move, or vice-versa), or as temporal ordering imposed on the different
data and computational units.

\pilot-Abstraction provide the basis for application-level scheduling.
Applications can make placement decisions based on the \pilots it has
acquired. Additionally, applications can rely on an application-level
scheduling service as provided by BigJob and BigData. This service is exposed
via the \computedataservice interface and accepts both \cus and \dus. The
service decides how and when to place data and schedule its movement. 

We use {\it affinities} to describe resource relationship between \cus, \dus,
and resources (i.\,e.\ \pilots). A simple model for specifying affinities is
used: data centers and machines are represented as a tree (see
Figure~\ref{fig:figures_affinities}). The further the distance between two
resources, the smaller the affinity. The framework relies on this affinity
description to efficiently manage the different kinds of storage as well as
distribution of data/compute across WAN.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/storage-types.pdf}
	
    \caption{\textbf{Pilot-Compute and Pilot-Data on Different
                                Types of Storage Resources:} BigJob/BigData 
				supports various types of resources. Affinities allow the user 
				to describe the 
				relationship between different types of resources enabling the 
				scheduler to efficiently make placement decisions. }
	\label{fig:figures_storage-types}
\end{figure}

As explained before, even within a single cloud, managing data locality is a
great challenge due the variety of different storage options.
Figure~\ref{fig:figures_storage-types} summarized the different kinds of
storage options an application has to manage. \pilot-Abstractions not only
provide a uniform interface to these different kinds of storage, the also
support application-level scheduling. The core of the \pilot-Manager is an
affinity-aware scheduler. BigJob and BigData support different forms of
data/compute affinities via the \computedataservice. The internal scheduler of
the \computedataservice relies on affinity as first entity to describe
relationships between \dus, \cus and resources. Affinities are an essential
tool for modeling the different storage characteristics and to allow an
effective reasoning about different storage topologies and data/compute
placements. BJ relies on affinity label for describing resource topology.


We use a simple model to specify resource affinities: Data centers and
machines are represented as a tree. The further the distance between two
resources, the smaller the affinity. Every \pilot is assigned an affinity
label in the resource description. Affinities are organized in an hierarchical
way. Figure~\ref{fig:figures_affinities} shows e.\,g.\ how a distributed
system consisting of different types of cloud and grid resources can be
modeled. The shorter the distance between two nodes, the higher the affinity.
Commercial clouds, such as Amazon and Google, are typically organized into
geographic regions. Within each region multiple availability zones exist.
Commonly, data movements between the lower-level availability zones are cheap
and fast, while movements between regions costs some significant amount on
money and time. Even worse are movements between infrastructure, e.\,g.\
between Amazon, Google and FutureGrid. Both \cus and \dus can request a
certain affinity. The runtime system then ensures that the data and compute
affinity requirements of the \cu/\du are met.


\begin{figure}[t]
	\centering
		\includegraphics[width=0.5\textwidth]{figures/affinities.pdf}
	\caption{\textbf{Managing Affinities between Cloud and Grid Resources:} 
	\pilotdata assigns each resource an affinity based on a simple 
	hierarchical model. The smaller the distance between two resources, the 
	larger the affinity.}
	\label{fig:figures_affinities}
\end{figure}

Each of the storage types in Figure~\ref{fig:figures_storage-types} can be 
assigned an affinity. The model is able to deal with subtle differences 
between clouds and grids/clouds. For example, at Amazon S3 is service that is 
confined by a single region, while Google Storage spawns multiple regions.

The \computedataservice scheduler matches the affinity requirements of
a \cu/\du with the available set of resources before assigning it to a
\pilot.  \cus are placed closed to a \du. Before a \cu is actually
run, the \du is exported to the working directory of the \cu. In the
cloud cases e.\,g.\ this means that the data is copied from an object
store to the local filesystem where the \cu runs.

In summary, \pilots provide a well-defined abstraction for resources
supporting an effective application-level resource management without the
necessity to deal with lower-level, infrastructure specific details. At the
same time, \pilot-Abstraction and affinities enable the application to
trade-off, e.\,g.\ the geographic locations of data and compute, resource and
bandwidth availabilities. Using \pilot-abstractions to compute and storage
resources, higher-level frameworks can request resources (\pilots), on
which they then can run tasks (\computeunits). Thus, \pilots can be used as
basis for building capabilities on distributed infrastructures.


\subsubsection*{Affinities in the Pilot-API}


Each \pilot can specified the required affinity in it's description (see
listing~\ref{lst:pilot_compute_affinity}).

\begin{code}[
caption={Creation of a \textit{PilotCompute} on the specified  compute
resource endpoint.},
label={lst:pilot_compute_affinity}]
pilot_compute_description = 

{
 "service_url":"sge+ssh://kraken.nics.xsede.org",
 "number_of_processes": 1,                             
 "working_directory": "/tmp/pilot-compute/",
 "affinity_datacenter_label": "nisc",              
 "affinity_machine_label": "kraken" 
}
\end{code}

The runtime system will map the requirements of the \pilot with an appropriate
resources (see figure~\ref{fig:figures_pilot-affinities}). Once a \pilot is
assigned to a resource, it inherits the affinity of that resource.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/pilot-affinities.pdf}
	\caption{\pilot-Abstractions and Affinities}
	\label{fig:figures_pilot-affinities}
\end{figure}


In most cases it is necessary to co-locate \cus and \dus, i.\,e.\ either a \du
must be copied into a \pd that is placed on the same resource as the \pj or
vice versa. The other option is to stream the \du to the respective \cu.


\texttt{ComputeUnitContext}: Can be used by compute unit to query


TODO
explain how affinities can be used to model different storage types from 
section III

The scheduler is responsible for allocating storage resources, i.\,e.\ \sus in a \pilotdata, and compute resource, i.\,e.\ \sus in a \pilotcompute.


\subsubsection*{Scheduling}

BigJob/BigData currently provides two different scheduling schemes: (i) a 
centralized scheme and (ii) a hybrid scheme that has both centralized and 
decentralized elements.

The following algorithm describes the scheduling approach implemented by the 
\texttt{ComputeDataServiceDecentral}. For DataUnits, the scheduler aims to 
find a pilot with the closest affinity as requested by the \dataunit. 
Depending on the application requirements, the application e.\,g.\ chose the affinity close to a running \pilotcompute or close to the actual data source.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/computedataservice-scheduling.pdf}
	\caption{Multi-Level Scheduling}
	\label{fig:figures_computedataservice-scheduling}
\end{figure}

For \computeunits a multi-level scheduling scheme is used:
\begin{enumerate}
	\item The \pilot-Manager attempts to find a \pilot that best fulfills the 
	requirements of the \cu with respect to (i) the requested affinity and the 
	(ii) the location of the input data.
	\item If a \pilot with the same affinity exists and \pilot has an empty 
	slot, \cu is placed in this pilots queue.
	\item If delayed scheduling is active, wait for $n$ sec and recheck 
	whether \pilot has a free slot.
	\item If no \pilot is found, \cu is place in global queue and pulled by 	
	first \pilot which has an available slot.	
\end{enumerate}

\subsubsection*{Other}
\begin{itemize}
\item High levels of heterogeneity in the data infrastructure
\begin{itemize}
\item File systems, storage, transport protocols, …
\end{itemize}
\item Support application level capabilities to specify dependencies
  at a logical level rather than specific file level

\begin{itemize}
\item First class support for Affinities (D-C, D-D)
\end{itemize}

\item Typically placement and scheduling of data is decoupled from the compute-tasks

\begin{itemize}
\item Integrated approach to compute and data ?
\end{itemize}

\item Dynamic decision for data

\begin{itemize}
\item Analogous  to late-binding of data
\item Fluctuating resources as a fundamental property of DCI
\end{itemize}

\item Abstraction for other factors and not application specific way
\begin{itemize}
\item Varying data sources, fluctuating data rates, etc
\end{itemize}

\end{itemize}







Push vs. Pull data
High-Level abstraction on to of logical/abstract file name 







\section{Experiments}


\subsection{Characterizing BigJob/BigData and Distributed Infrastructures}
\begin{itemize}
	\item Interoperability: PJ/PD on heterogeneous backends
    \item understanding internals: overheads for coordination, data movements etc.
	\item Similar coordination graph as in the past
\end{itemize}


\jhanote{I see three levels of increasing sophistication: (i)
  application X - bog standard without any pilot-abstractions, (ii)
  application X with pilot abstractions (iii) application X using
  pilot abstations with advanced features of affinity and an
  interoperability component (the later could be used to demonstrate
  integration/use of advanced data-CI along with pilot abstractions).}

\jhanote{I definitely see a role for OSG and SAGA-iRODS to be a part
  of the experimentation for this paper}

\subsection{Real-Life Application Scenario}

\textbf{Application:}
\begin{itemize}
	\item Model a scenario after a 3DPAS application, e.g. LSST
    \item BWA/BFAST workload -- meta genomics scenario?
	\item Characterize application with respect to input, intermedia and 
	output data:
		\begin{itemize}
			\item volume of data
			\item degree of distribution
			\item dynamic data
		\end{itemize}
\end{itemize}

\textbf{Scenarios:}
\begin{itemize}
	\item Run scenario local
	\item Run scenario distributed
	\item Use intelligent task and data placement
\end{itemize}



\textbf{"Intelligent" data placements:}
\begin{itemize}
	\item ComputeDataService decides whether data, compute or both have to move
	\item 3 different cases:
\begin{itemize}
	\item 	i) move all data everywhere
	\item 	ii) move data when task has been placed
	\item     iii) move data and then place task
\end{itemize}
\end{itemize}


Assumption:
How is initial data placed?
- distributed
- central in one place

Second application track:
- Data movement in the context of MD jobs

Which application?
- exclusively bfast?
- bwa? another good application? scenario?
- in some cases reference data is decomposable...

Look at the difference in $T_c$ for each application in the three
cases. We might introduce the idea of alpha. We dont' need to
quantify, but can qualitatively show how alpha determines the
importance (as measured by time to solution). 

My hunch is that $T_c$ will be more sensitive for data-intensive
application than other way around.

The idea is that for an application which is



\section{Conclusion and Future Work}

\begin{itemize}
\item collaborative science: distributed by nature
\item what capabilities could be built on top of pilot abstractions
\end{itemize}


\section*{Acknowledgements}

This work is funded by NSF CHE-1125332
  (Cyber-enabled Discovery and Innovation), HPCOPS NSF-OCI 0710874
  award, NSF-ExTENCI (OCI-1007115) and ``Collaborative Research:
  Standards-Based Cyberinfrastructure for Hydrometeorologic Modeling:
  US-European Research Partnership'' (OCI-1235085).  MS is sponsored
  by the program of BiG Grid, the Dutch e-Science Grid, which is
  financially supported by the Netherlands Organisation for Scientific
  Research, NWO. SJ acknowledges useful related discussions with Jon
  Weissman (Minnesota) and Dan Katz (Chicago). We thank J Kim (CCT)
  for assistance with BFAST.  This work has also been made possible
  thanks to computer resources provided by TeraGrid TRAC award
  TG-MCB090174 (Jha) and BiG Grid.  This document was developed with
  support from the US NSF under Grant No. 0910812 to Indiana
  University for ``FutureGrid: An Experimental, High-Performance Grid
  Test-bed''.

  
% \bibliographystyle{IEEEtran}
\bibliographystyle{abbrv}
\bibliography{literatur,saga,saga-related,local}


\end{document}

