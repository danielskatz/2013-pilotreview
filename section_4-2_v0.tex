% -----------------------------------------------------------------------------
% 4.2
%
\subsection{Analysis of \pilot system Implementations}
\label{sec:implementations}

In light of the common vocabulary discussion in \S\ref{sec:termsdefs}, a
representative set of \pilot systems has been chosen for further analysis.
Examining these \pilot systems using the common vocabulary exposes their core
similarities, and allows a detailed analysis of their differences.

The now following discussion of \pilot systems is ordered alphabetically. To
assist the reader, we make use of textual conventions: in \vocab{Bold} we
express the \vocab{Logical Components} and \vocab{Functionalities} of our model
from \S\ref{sec:compsandfuncs} and the \vocab{Terms and Definitions} from
\S\ref{sec:termsdefs}, in \prop{Italic} we refer to the \prop{Properties} from
\S\ref{sec:properties} and in \impterm{Typewriter} we display terminology from
the respective \pilot system under discussion.\mtnote{Do we need this formatting
devices? They are not always added to every use of the terms and they make the
text a bit `heavy' for me.}

% \jhanote{this might go to the section where we begin describing
%   pilotjobs.. also will need an updated description of the methodology
%   employed to choose} ....such that ``exemplars'' were chosen; these
% are \pilotjobs which either laid foundational \pilotjob concept or
% incorporated strides in interoperability, usability, architecture or
% otherwise to advance the understanding and usage of \pilotjobs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Coasters
%
%\subsubsection{Coasters}
% AS PER SJ P* CALL: COASTERS SECTION IS CANCELED
% mrnote: Reason - insufficient documentation, no citations to actual science problems
% msnote: We probably want to revisit this decision, but not today :-)
%\subsubsection{Coasters}
%
%\msnote{Main coasters ref: \url{http://www.ci.uchicago.edu/swift/papers/UCC-coasters.pdf}}
%\mrnote{Coasters wiki: \url{http://wiki.cogkit.org/wiki/Coasters}}
%
%The Coaster System (or "Coasters") is a Java CoG based Pilot-Job system
%created for the needs of the Swift parallel scripting language.
%
%\paragraph{Design Goals}
%
%\begin{itemize}
%  \item Driven by the needs of Swift
%  \item Automatically-deployed to endpoint. Does not need user login to endpoint.
%  \item Supports file staging, on-demand opportunistic multi-node allocation,
%  remote log gin, and remote monitoring
%\end{itemize}
%
%\paragraph{Applications}
%
%\begin{itemize}
%  \item It has been used since 2009 for applications in fields
%that include biochemistry, earth systems science,
%energy modeling, and neuroscience.
%  \item Above claim made in Coasters paper - need to obtain citations
%\end{itemize}
%
% Swift~\cite{Wilde2011} is a scripting language designed for expressing abstract
% rest of this cut, but making a note of this in case we want
% to bring swift into the discussion later that i can find more info in 2011 paper
%% Swift~\cite{Wilde2011} is a scripting language designed for expressing
%% abstract workflows and computations. The language provides, amongst many other
%% things, capabilities for executing external applications, as well as the
%% implicit management of data flows between application tasks.
%% % For this
%% % purpose, Swift formalizes the way that applications can define
%% % data-dependencies. Using so called mappers, these dependencies can be
%% % easily extended to files or groups of files.
%% The runtime environment handles the allocation of resources and the spawning of
%% the compute tasks.
%% % Both data- and execution management capabilities are provided
%% % via abstract interfaces.
%% Swift supports e.\,g.\ Globus, Condor and PBS resources.
%% % The pool of resources
%% % that is used for an application is statically defined in a configuration file.
%% % While this configuration file can refer to highly dynamic resources (such as OSG
%% % resources), there is no possibility to manage this resource pool
%% % programmatically.
%% By default, Swift uses a 1:1 mapping for \cus and \sus. However,
%% Swift supports the grouping of SUs as well as PJs. For the PJ functionality, Swift uses the
%% Coaster~\cite{coasters} framework. Coaster relies on a master/worker
%% coordination model; communication is implemented using GSI-secured TCP sockets.
%% Swift and Coaster support various scheduling mechanisms, e.\,g.\ a FIFO and a
%% load-aware scheduler. Additionally, Swift can be used in conjunction with
%% Falkon~\cite{1362680}, which also provides \pilot-like functionality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIANE
%
\subsubsection{DIANE}
\label{sec:diane}

DIANE~\cite{Moscicki:908910} is a task coordination framework, which follows
the \MW pattern. It was developed at CERN for data analysis in the
LHC experiment, but has since been used in various other domains, though mainly
in the Life Sciences. As DIANE is a software framework, some of its semantics
are not predetermined, as they can be implemented in various ways depending on
the user requirements. In this way, DIANE also provides \pilot functionality
for job-style executions.\mtnote{I am not sure I understand what a `job-style'
execution is. Do we need to make it explicit as we did not define the term
before?}

\paragraph{Resource Interaction}
The \pilot provisioning with GANGA~\cite{Moscicki20092303} provides a unified
interface for job submissions to various resource types\mtnote{do we need
examples of these types of resource?}. In this way, DCI specifics are hidden
from other components of DIANE as well as from the user.

\paragraph{Overlay Management}
The \prop{Overlay Management} of \pilots is done out-of-band by means of the
\impterm{diane-submitter} script which can launch a \pilot on any supported
DCR.

\paragraph{Workload Semantics}
DIANE is mainly a \MW framework and, as such, the \vocab{Tasks} are
fully independent, consistently with a Bag-of-Tasks \vocab{Workload Semantics}.
The \vocab{Workload} that is managed by the application-specific component of
DIANE can freely structured, as long as it can be translated to independent
tasks\cite{diane-dag, diane-etc}. Plugins for other types of workload (e.\,g.\
DAGs or for data-intensive applications) exist or are under development. The
framework is extensible: for example, applications can implement a custom
application-level scheduler.

\paragraph{Coordination and Communication}
The \prop{Coordination \& Communication} in DIANE is based on
CORBA~\cite{OMG-CORBA303:2004} and uses TCP/IP. The CORBA layer is invisible to
the application layer. Networking-wise, the workers are clients of the master
server. On TCP/IP level, communication is always unidirectional from the
\impterm{WorkerAgent} to the \impterm{RunMaster}. \prop{Security} is provided
by a secret token that the \impterm{WorkerAgent} needs to communicate back to
the \impterm{RunMaster}. This implies that the \impterm{WorkerAgent} needs to
be able to reach the \impterm{RunMaster} via TCP/IP but not the other way
around. Bidirectional communication is achieved by periodic polling through
heartbeats by the \impterm{WorkerAgent}, where the \impterm{RunMaster} responds
with feedback.

\paragraph{Task Binding}
DIANE is primarily designed for HTC environments (such as EGI~\cite{egi}),
i.\,e.\ one \pilot consists of a single worker agent with the size of 1 core.
Although the semantics of the binding are ultimately controllable by the
user-programmable scheduler, the general architecture is consistent with a pull
model. The pull model naturally implements the late-binding paradigm as every
worker pulls a new task once it is available and has free resources.

\paragraph{Task Execution Modes}
DIANE is primarily designed for HTC environments (such as EGI~\cite{egi}),
i.\,e.\ one \pilot consists of a single worker agent with the typical size of 1
core/node. As such, DIANA is not able by default to run (widely) parallel
applications as those based, for example, on MPI.

\paragraph{Pilot Resource Capabilities}
DIANE implements the notion of \mtnote{resource?} ``capacity'', a property that
applications can use to diverge from the ``1 worker = 1 core''
mode.\mtnote{Should we make explicit what this divergence is and implies?}

\paragraph{Architecture}
The central component of DIANE is the \impterm{RunMaster}, consisting of a
\impterm{TaskScheduler} and an \impterm{ApplicationManager} module. Both
modules are abstract classes that need to be implemented for the specific
purpose at hand. Examples and/or default implementations are provided to the
user as guidelines and initial code stubs. Together the \impterm{TaskScheduler}
and \impterm{ApplicationManager} implement the \vocab{Workload Manager}
component.

The \impterm{TaskScheduler} keeps track of the task entries and is responsible
for \vocab{Binding Tasks} to the \impterm{ApplicationWorkers}. The
implementation of the \impterm{ApplicationManager} is responsible for defining
the application \vocab{Workload} and for describing the \vocab{Tasks} that are
passed to the \vocab{TaskScheduler}. As the \impterm{ApplicationWorker} asks
the \impterm{TaskScheduler} for new \vocab{Tasks}, the natural way of
implementing the scheduler is by following a \vocab{Late Binding} approach.

\vocab{\pilots} in DIANE are \impterm{WorkerAgents}. The core component of a
\impterm{WorkerAgent} is the \impterm{ApplicationWorker}, an abstract class
that defines three methods that need to be implemented by every implementation.
Two of these methods are for initialization and cleanup while the third method,
(\impterm{do\_work()}) receives the task description and executes the work.
\impterm{do\_work()} is therefore an implementation of the
\vocab{Task Manager} component.

\paragraph{Interface}
DIANE's \prop{Architecture} is based on the \impterm{Inversion of Control}
design pattern. Implemented in Python, it exposes well-defined hooks for
\vocab{Application} programming. The aforementioned abstract classes need to be
implemented to provide the application-specific semantics. The \prop{Interface}
to these ``DIANE-\vocab{Applications}'' is to start them through the
\impterm{diane-run} command\mtnote{I am not sure I understand this sentence}.
As discussed in the \prop{Overlay Management}, the creation of \pilots is done
out-of-band.

\paragraph{Performance and Scalability}
The authors report that in first instance they had implemented full
bi-directional communication, but that turned out to be difficult to correctly
implement and created scalability limitations.\mtnote{In absence of more
precise data about the performance profile of DIANE and it scalability
boundaries, I would delete this paragraph.}

\paragraph{Robustness}
The \prop{Robustness} in DIANE comes from the mature CORBA communication layer,
and from the custom task-failure policies implemented in the
\impterm{TaskScheduler}. \prop{Robustness} is achieved in DIANE by: (i)
offering mechanisms to supports fault tolerance: basic error detection and
propagation mechanisms are in place; (ii) implementing an automatic
re-execution of tasks; and (iii) leaving room for application specific
strategies.

\paragraph{Files and Data}
The CORBA communication channel between the Master and the Worker also allows
for file transfers, meaning that the \pilot not only exposes cores, but also
the file system on the worker nodes.

\paragraph{Interoperability and Security}
\prop{Interoperability} to various middleware security mechanisms
(e.\,g.\ GSI, X509 authentication) and backends is achieved through its
integration with GANGA.

\paragraph{Multitenancy}
DIANE is a single-user \pilot system, i.\,e.\ each \pilot is executed with the
privileges of the respective user. Also, only workload of this respective user
can be executed by DIANE. However, \prop{Multitenancy} with DIANE does happen
when it is used as a backend for Science Gateways, where it serves multiple
users on the same installation, but with a single credential.\mtnote{I would
argue that multitenancy is achieved by the application layer and not by DIANE.}

\paragraph{Development Model}
DIANE is developed, maintained, and used by the CERN community with external
contributions and users.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% DIRAC
%
\subsubsection{DIRAC}
\label{sec:dirac}

DIRAC (Distributed Infrastructure with Remote Agent Control) is a software
product developed by the CERN LHCb project\cite{diracgrid2004}. DIRAC
implements a \impterm{Workload Management System} (WMS) to manage the
computational payload of its community members, and the DIRAC Pilot framework
to execute that payload.\mtnote{Should we had details about the architecture of
the pilot framework? For example, we speak about a Pilot Director and a DIRAC
Agent.}

DIRAC's claim to fame is that ``[\ldots] the LHCb data production run in summer
2004 was the first successful demonstration of the massive usage of the LCG
grid resources. This was largely due to the use of the DIRAC Workload
Management System, which boosted significantly the efficiency of the production
jobs.''~\cite{Tsaregorodtsev:2010cj}

\paragraph{Pilot Resource Capabilities}

The bootstrapping of the \impterm{DIRAC Agent} performs a full DIRAC
installation including a download of the most current version of the
configuration files. After completion of the bootstrapping, the installer
checks for the working conditions (e.g. where execution takes place, available
resources, etc.) and then a \impterm{DIRAC Agent} is started.

\paragraph{Resource Interaction}

\pilots are sent to the resource by the \impterm{Pilot Director} and all
resource interaction is hidden behind that abstraction \mtnote{Is the Pilot
Director an abstraction or a (implemented) module of the Pilot Framework?}. The
Pilot Director is reported \mtnote{Should we just say that it is able? Do we
have doubts about what is reported?} to be able to interact with different
batch systems like PBS/Torque, LSF, SGE and BQS.

\paragraph{Overlay Management}

\pilots are being launched by the \impterm{Pilot Directors} to the supported
DCIs. The DIRAC \pilots create a resource overlay that presents a homogeneous
layer to the other components. The user has no control over where \pilots are
started.

\paragraph{Workload Semantics}

The user \vocab{Workload} (in DIRAC terminology: \impterm{payloads}),
consisting of independent \vocab{Tasks} have varying levels of priorities for
LHCb as a whole, as well as a variety of requirements for them to execute.
Additionally, users can specify requirements like needed input data for their
tasks and this will be taken into account when placing the task on a
resource\mtnote{pilot?}.

\paragraph{Task Binding Characteristics}

Pilots implement a pull scheduling paradigm. Once they are running on the
computing resource, they contact central WMS servers for a late binding of the
resource to the payload.

When \vocab{Tasks} are assigned to the \impterm{WMS}, a consistency check is
performed and optional input data requirements are
translated\mtnote{transferred? Staged?} to resource candidates.

Once this is done, \vocab{Tasks} are placed into \impterm{TaskQueues}.
\impterm{TaskQueues} are sorted groups of \impterm{Payloads} with identical
requirements (e.g., user identity, number of cores, etc.) that are ready to be
executed.

\impterm{TasqQueue Directors} direct the workload from the \impterm{TaskQueues}
for execution to the Job Agents\mtnote{Are these the same as DIRAC Agents?}.

\paragraph{Task Execution Modes}

Once the \vocab{Task} has been pulled by a pilot, it is executed by means of a
\impterm{Job Wrapper}. Given the nature of the resources DIRAC is aimed at,
\vocab{Task} are generally single or few-core tasks. Work on supporting MPI is
ongoing\cite{}.

\paragraph{Coordination and Communication}

The \impterm{DIRAC Agent} is responsible for sending the \impterm{payload}
request to the central DIRAC WMS server and for the subsequent execution of the
received \impterm{payload}. The client/service communication is performed with
a light-weight protocol (XML-RPC) with additional standard-based authentication
mechanisms.

\paragraph{Architecture}

The design of DIRAC relies on three main components: the \impterm{DIRAC Agent}
\impterm{TaskQueue}, and \impterm{TaskQueue Director}.\mtnote{Should we mode
this at the beginning of the description and add all the other components we
use in the subsections?}

\paragraph{Interface}

The DIRAC project provides many command line tools as well as a comprehensive
Python API to be used in a scripting environment. In DIRAC3 a full-featured
graphical Web Portal was developed.

\paragraph{Interoperability}

DIRAC is not inter-operable with other \pilot systems but supports execution of
\pilots on a variety of computing resources by means of the \impterm{Pilot
Director}.

\paragraph{Multitenancy}

The community nature of the \impterm{DIRAC WMS} makes it an intrinsically
multi-user system. The WMS holds the workload for multiple users and the pilot
agents are shared amongst the workload of multiple user.

\paragraph{Robustness}

The \impterm{Job Wrapper} creates a uniform environment to execute
\vocab{Tasks} independent of the \vocab{DCI} where they run.
Furthermore, it retrieves the input sandbox, checks availability of required
input data and software, executes the payload, reports success or failure of
the execution, and finally uploads output sandbox and output data if required.

At the same time it also instantiates a \impterm{Watchdog} to monitor the
proper behavior of the \impterm{Job Wrapper}. The watchdog checks periodically
the situation of the \impterm{Job Wrapper}, takes actions in case the disk or
available CPU is about to be exhausted or the payload stalls, and reports to
the central \impterm{WMS}. It can also execute management commands received
from the central service, e.g. to terminate the \impterm{Payload} execution.

\paragraph{Security}

The DIRAC Secure client/service framework called \impterm{DISET} (DIRAC SEcure
Transport) is full-featured to implement efficient distributed systems in the
grid environment\cite{}. It provides X509 based authentication and a
fine-grained authorization scheme. Additionally, the security framework
includes logging service that provides history up to 90 days to investigate
security incidents.

\paragraph{Files and Data}

Tasks within DIRAC that specify their input data are specifically managed. The
WMS ensures that tasks are only started on resources where that data is
available and it makes sure that that data becomes available.

\paragraph{Performance and Scalability}

DIRAC has well documented numbers about scaling in supporting many users, on
many resources, for a variety of applications over a long period of
time.\mtnote{Should we summarize these numbers?}

\paragraph{Development Model}

DIRAC is actively development and used by the LHCb community\cite{}. More
recently, they\mtnote{Who?} have been reaching out to other communities
too\cite{}. From a development perspective, there is ongoing work on enriching
the data capabilities\cite{} and to be able to execute MPI applications\cite{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Falkon
%
\subsubsection{Falkon}
\label{sec:falkon}
% Falkon refers to \pilots as the so called provisioner, which are created
% using the Globus GRAM service. The provisioner spawns a set of executor
% processes on the allocated resources, which are then responsible for managing
% the execution of SUs. \cus are submitted via a so called dispatcher service.
% Similar to Coaster, Falkon utilizes a M/W coordination model, i.\,e.\ the
% executors periodically query the dispatcher for new SUs. Web services are
% used for communication.

% \mrnote{Main Falkon ref:
% \url{http://dev.globus.org/images/7/78/Falkon_SC07_v42.pdf}}
% \mrnote{Best Falkon ref: \url{http://dev.globus.org/wiki/Incubator/Falkon}}

The Fast and Light-weight tasK executiON framework (Falkon)~\cite{1362680} was
created with the primary objective of enabling many independent tasks to run on
large computer clusters (an objective shared by most \pilot systems).
Performance and time-to-completion for jobs on such clusters drove Falkon
development. In addition to being \textit{fast}, Falkon, as its name suggests,
also focused on lightweight deployment schemes.

\paragraph{Pilot Resource Capabilities}

Falkon exposes resource as a set of cores as tasks are by definition single
core only.

\paragraph{Resource Interaction}

Falkon was originally developed for use on large computer clusters in a grid
environment, but has since been expanded to work on other types of
DCR. Falkon has been shown to run on TeraGrid (now XSEDE),
TeraPort, Amazon EC2, IBM Blue Gene/L, SiCortex, and Workspace
Service~\cite{1362680}.

\paragraph{Overlay Management}

The Dispatcher service in Falkon is implemented by means of a web service.
This Dispatcher implements a factory/instance deployment scenario.
When a new client sends task submission information, a new instance
of a Dispatcher is created.

\paragraph{Workload Semantics}

Falkon has been integrated with the Karajan workflow language and execution
engine, meaning that applications that utilize Karajan to describe their
workflow will be able to be executed by Falkon.

The Swift parallel programming system~\cite{Wilde2011} was integrated
with Falkon for the purpose of task dispatching.

\paragraph{Task Binding Characteristics}

Cores and tasks are considered homogeneous. Tasks are pulled to the nodes
and are thereby of late-binding nature.

\paragraph{Task Execution Modes}

Falkon does not support MPI jobs, a limiting factor in its adoption for certain
scientific applications.

\paragraph{Coordination and Communication}

Interaction between the components is by use of the Globus Web Services model.

\paragraph{Architecture}

Falkon's architecture relies on the use of multi-level scheduling as well as
efficient dispatching of tasks to heterogeneous DCIs. As mentioned above, there
are two main components of Falkon: (i) the Dispatcher for farming out tasks and
(ii) the Provisioner for acquiring resources.

The overall task submission mechanism can be considered a 2-tier architecture;
the Dispatcher (using the terminology defined in Section~\ref{sec:termsdefs},
this is the \pilot Manager) and the Executor (the \pilot Agent).

The Dispatcher is a GRAM4 web service whose primary function is to take task
submission as input and farm out these tasks to the executors. The Executor
runs on each local resource and is responsible for the actual task execution.
Falkon also utilizes \textit{provisioning} capabilities with its Provisioner.

The Falkon Provisioner is the closest analogous entity to a \pilot: it is the
creator and destroyer of Executors, and is capable of providing both static and
dynamic resource acquisition and release.

Further, in order to process more complex workflows, Falkon has been integrated
with the Karajan workflow execution engine~\cite{karajan}. This integration
allows Falkon to accept more complex workflow-based scientific applications as
input to its \pilot-like job execution mechanism.\mtnote{repetition?}

\paragraph{Interface}

Simpler task execution can be achieved without modifying the existing
executables. Sufficient task description in the web service is all that is
required to utilize the Falkon system.

\paragraph{Interoperability}

For launching to resources Falkon relies on the availability of GRAM4.
This abstracts the resource details but also limits the use of other resources.

\paragraph{Multitenancy}

Each instantiation of the Dispatcher maintains its own task queue and state -
in this way, Falkon can be considered a single-user deployment scheme, wherein
the ``user'' in this case refers to an individual client request.

\paragraph{Robustness}

Falkon supports a fault tolerance mechanism which suspends and dynamically
readjusts for host failures.

\paragraph{Security}

Thanks to the use of Globus transport channels, Falkon allows for both
encrypted and non-encrypted operation. The non encrypted version was used to
gain most throughput, obviously lowering the security of real-world use.

\paragraph{Files and Data}

The data management capabilities of Falkon extend beyond the core
\pilotjob functionalities as described in Section~\ref{sec:coreprops}. Falkon
encompasses advanced data-scheduling and caching but also a data diffusion
approach. Using this approach, resources for both compute and data are acquire
dynamically and compute is scheduled as close as possible to the data it
requires. If necessary, the data diffusion approach replicates data in response
to changing demands~\cite{raicu2008accelerating}.

\paragraph{Performance and Scalability}

As previously stated, the design of Falkon was centered around the goal of
providing support to run efficiently large numbers of jobs  on large clusters
and grids. Falkon realizes this goal through the use of: (i) a dispatcher to
reduce the time to actually place tasks as jobs onto specific resources (such a
feature was developed to account for different issues amongst distributed
cyberinfrastructure, such as multiple queues, different task priorities,
allocations, etc); (ii) a provisioner which is responsible for resource
management; and (iii) data caching in a remote environment~\cite{1362680}.

Falkon has been tested for throughput and performance with multiple
applications: fMRI (medical imaging), Montage (astronomy workflows), and MolDyn
(molecular dynamics simulation). Falkon shown favorable results in terms of
overall execution time when compared to GRAM and GRAM/Clustering
methods~\cite{1362680}.

The per task overhead of Falkon execution has been shown to be in the
millisecond range. Furthermore, Falkon has been demonstrated to achieve
throughput in the range of hundreds to thousands of tasks per second for very
fine-grained tasks.

\paragraph{Development Model}

The Falkon project ran from 2006 to 2011 and the source code is not publicly
available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% GWPilot
%
%\subsubsection{GWPilot}
%\label{sec:gwpilot}
%% \aznote{Considering the new direction of the paper, I am not sure whether
%% GWPilot~\cite{gwpilot} requires its own section for analysis...}
% msnote: To be reconsidered: but not now
%% %\begin{lstlisting}[breaklines]
%% %\url{https://indico.egi.eu/indico/materialDisplay.py?contribId=18&sessionId=46&materialId=slides&confId=1019}
%% %\url{http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266981}
%% \begin{itemize}
%% \item Integrates with GridWay metascheduler
%% \item Pilots advertise to GridWay, GridWay scheduler schedules pilots
%% \item Pilots pull tasks from scheduler
%% \item Installation as a GridWay driver -- written in Python
%% \item Interoperability managed by GridWay drivers (DRMAA, JDSL, BES, more?)
%% \item Using GWPilot requires only adding a single line to their GridWay task
%% \item ``Lightweight and scalable''
%% \end{itemize}
%% %\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% HTCondor
%
\subsubsection{HTCondor}
\label{sec:htcondor}

HTCondor can be considered one of the most prevalent distributed computing
projects of all time in terms of its pervasiveness and size of its user
community. Is often cited as the project that introduced the \pilot
concept\cite{}. Similar in many respects to other batch queuing systems,
HTCondor puts special emphasis on high-throughput computing (HTC) and
opportunistic computing. HTC is defined as providing large scale computational
power on a long term scale. Opportunistic computing is about making pragmatic
use of computing resources whenever they are available, without requiring full
availability. The former can be can be achieved by applying the latter.

% But despite this fact, describing HTCondor along the lines of a \pilotjob
% system has turned out to be difficult, if not partly impossible for several
% reasons.

% The \pilot concept is in its heart centered around Condor Glideins.
% Many Workload Management Systems make use of the core functionality, ranging
% from many specific WMS's to the ``generic`` GlideinWMS.


\paragraph{Architecture}

HTCondor is a high-throughput distributed batch computing system.

%
% Core HTCondor (ca 1988)
%
The core components of HTCondor are collectively named the \impterm{Condor
Kernel}:\mtnote{I do not understand the use of the colon here. Is `Agent'
synonym of `Condor Kernel'?} \impterm{Agent} (also referred to as
\impterm{schedd}), \impterm{Resource} (also referred to as \impterm{startd}),
\impterm{Shadow}, \impterm{Sandbox} (also referred to as \impterm{starter}),
and \impterm{Matchmaker} (also referred to as \impterm{central manager}).

The user submits \vocab{Tasks} (in HTCondor called \impterm{Jobs}) to the
\impterm{Agent}. \impterm{Agents} provide persistent storage for the user's
\impterm{Jobs} and tries to find computing \vocab{resources} \impterm{Resource}
to execute them.

Both \impterm{Agents} and \impterm{Resources} advertise themselves to the
\impterm{Matchmaker}, which is then responsible to match the latter to the
former. The \impterm{Matchmaker} notifies the \impterm{Agent} of a potential
match, and the \impterm{Agent} is in turn responsible to contact the
\impterm{Resource} and validate the suggested match.

Once the match is confirmed by the \impterm{Resource}, both \impterm{Resource}
and \impterm{Agent} have to start a new process to make the execution of the
\impterm{Job} happen. The \impterm{Agent} starts a \impterm{Shadow} which
is responsible for providing all the details required to execute a
\impterm{Job} (the executable, the arguments, the environment, the input
files).

The \impterm{Resource} creates a \impterm{Sandbox} which is an appropriate
execution environment (the \impterm{Sand} for the \vocab{Task} while at the
same time protecting the \impterm{Resource} from a misbehaving \impterm{Job}
(the \impterm{Box}).\mtnote{Is there a missing parenthesis?}

Following on from this, an \impterm{Agent}, \impterm{Matchmaker} and a (set of)
\impterm{Resources} together form a \impterm{Pool}. \vocab{Resources} in a pool
can span a wide spectrum of systems. While some \impterm{Pools} are comprised
of regular desktop PCs (sometimes collectively called a campus grid), other
\impterm{Pools} incorporate large HPC clusters and cloud resources.  Hybrid
pools with heterogeneous types of resources are also possible.

Within the \impterm{Pool} the \impterm{Matchmaker} is responsible for enforcing
the policies defined by the \impterm{Community}.

%The matchmaker, also called the \impterm{central manager}, is another system
%service that realizes the concept of late binding by matching user tasks with
%one or more of the resources available to an HTCondor pool.
%The matchmaking process is based on \impterm{ClassAds} a description language
%that can capture both, resource capabilities as well as task requirements.

%
% Gateway Flocking (ca 1994)
%
While technically a \impterm{Pool} could grow as big as needed, reality was
that (which was also only natural)\mtnote{I am not sure what `natural' means in
this context. I would eliminate the parenthetical sentence altogether or
explain it better.} many independent \impterm{Pools} were created. As a user
(or \impterm{Agent}) could only belong to one \impterm{Matchmaker}, the
\impterm{Resources} that a user could access were therefore limited to one
\impterm{Pool}. This lead to the concept of \impterm{gateway
flocking}~\cite{Epema:1996:flocking}. With this, an individual \impterm{Pool}
would be interfaced to another \impterm{Pool} by letting associated
\impterm{gateway} machines expose information about the \impterm{Pool's}
participants to another \impterm{Pool}, e.g. about idle \impterm{Resources} or
\impterm{Agents}. These \impterm{gateway} nodes can be configured to adhere to
policies and are not necessarily bi-directional.

% It is possible for two or more HTCondor pools to ``collaborate'' so that one
% pool has access to the resources of another pool and vice versa. In HTCondor,
% this concept is called and allows agents to query matchmakers outside their
% own pool for compatible resources. Flocking is used to implement
% load-balancing between multiple pools but also to provide a broader set of
% heterogeneous resources to user communities.

In this model, however, \impterm{flocking} is still bounded to single
\impterm{organizations}/\impterm{communities}, and a user (or \impterm{Agent})
could not join multiple of them at the same time.
%
% Direct Flocking (ca 1998)
%
This limit was overcome by introducing \impterm{direct flocking} in which an
\impterm{Agent} can report itself to multiple \impterm{Matchmakers}
While both forms of \impterm{flocking} had their \textit{raison d'\^{e}tre},
and they could even be mixed, \impterm{gateway} flocking did not stand the test
of time as with scaling up, both technical overhead and organizational burden
became too high.

%
% Condor-G (ca 2000)
%
Around that time (1998) the vision of the grid began to surface. One of the
initial activities was a uniform interface for batch execution. This was
achieved by the Globus project with the design of the Grid Resource Access and
Management (GRAM) protocol.

The HTCondor \impterm{Agent} was extended to speak GRAM and Condor-G
\cite{condor-g} was born. GRAM services are deployed as remote job submission
endpoints on top of HPC cluster batch queuing systems. Condor-G allows a user
(an \impterm{Agent}) to incorporate those HPC resources temporarily to an
HTCondor pool.

Although Condor-G opened up the execution of Condor \impterm{Jobs} at any
GRAM-enabled computing resource, the drawback of this approach was that
resource allocation and job execution became coupled again, i.e. the
\impterm{Agent} had to direct the \impterm{Job} to a specific GRAM endpoint
without (potentially) knowing about the availability of resources on that
endpoint. Because GRAM was an abstraction of so many batch queuing systems, it
could only implement the greatest common denominator of all the functionalities
of these various implementations. This implied that in this mode of operation
there was no \impterm{Matchmaker} as the required functionality at the
endpoints for that was not provided by GRAM.

%
% Condor-G and Gliding In (ca 2001)
%
\impterm{Gliding In} was engineered to undo this coupling by offering a
personal Condor \impterm{Pool} out of GRAM enabled resources. \impterm{Gliding
In} consist of a three-step procedure: (i) a Condor-G agent uses the GRAM
protocol to launch HTCondor servers ad hoc via a GRAM endpoint service on a
remote system; (ii) the Condor servers, once gotten through the batch queue,
connect to the \impterm{Matchmaker} started by the \impterm{Agent} and thereby
form a personal Condor \impterm{Pool}; and (iii) \impterm{Jobs} submitted by
the user to the \impterm{Agent} which are then matched to and executed on the
\impterm{Resources} within the \impterm{Pool}.

\paragraph{Pilot Resource Capabilities}

The resources managed by a single \pilot are generally limited to compute
resources only and more specifically to single compute nodes.

\paragraph{Resource Interaction}

In its native mode as a batch queuing system, HTCondor is a middleware, and
therefore the discussion of resource interaction does not apply. With the
advent of Condor-G, Condor can be deployed to any site that offers a GRAM
interface.

\paragraph{Overlay Management}

%However, the provisioning, allocation and usage of resources within a pool can
%differentiate between different pools and multiple different approaches and
%software systems have emerged over time, all under the umbrella of the wider
%HTCondor project.

% \paragraph{glideinWMS -- Automated \pilot Provisioning}
GlideinWMS, a workload management system (WMS)~\cite{1742-6596-119-6-062044}
based on Condor GlideIns, introduces advanced \pilot capabilities to HTCondor
by providing automated \pilot (\impterm{startd}) provisioning based on the
state of an HTCondor pool.

\paragraph{Workload Semantics}

The upper most layer of the Condor stack is the \impterm{problem solver}.
A \impterm{problem solver} is a application layer structure built on top of the
Condor agent. Two problem solvers are supplied in the distribution:
master-worker and the directed acyclic graph manager (DAGMan).
Other problem solvers can be built using the public interfaces of the agent.
The task dependencies that are defined e.g. with DAGMAN are managed outside of
the Agent and the lower layers are not aware of the workload semantics.

\paragraph{Task Binding Characteristics}

%Gliding-In implements \vocab{late-binding} on top of GRAM/HPC
%systems: the (user-)agent can assign tasks to \impterm{startd}s after
%they have been scheduled and started through the HPC queueing system. This
%effectively decouples resource allocation (\impterm{startd} scheduling through
%GRAM) and task assignment.

None of the details of the \impterm{Job} are made known outside of the
\impterm{Agent} (\impterm{Shadow}) until the actual moment of execution.
This allows for the \impterm{agent} to defer placement decisions until the last
possible moment. If the \impterm{Agent} submits requests for resources to
several matchmakers, it may award the highest priority job to the first
resource that becomes available, without breaking any previous commitments.

\paragraph{Task Execution Modes}

A \impterm{universe} in HTCondor defines an execution environment. There are
distinct implementations of these universes available: the \impterm{Standard
Universe} provides checkpointing (relevant because of the opportunistic mode of
Condor) and remote system calls, which requires special compilation of the
application though; the \impterm{Java Universe} hides the details of setting up
the remote JVM; the \impterm{Vanilla Universe} is for applications that can't
be modified; the \impterm{Grid Universe} is the mode for Condor-G; the
\impterm{VM Universe} for exploiting VMware and XEN virtual machines; and the
\impterm{Parallel Universe} for executing MPI applications. It should be noted
that these \impterm{Universes} don't mix and that there is no (real) support
for MPI jobs in the Condor-G mode.

\paragraph{Coordination and Communication}

The various kernel elements can be deployed in multiple configurations and can
all be run on a single machine but also fully distributed. The HTCondor system
has their history in institutional environments and assume not too many network
restrictions.\mtnote{I do not understand the previous sentence.} The
communication between the kernel components consist of both UDP and TCP based
proprietary protocols that uses an array of dynamically allocated ports. With
Condor-G the requirements were significantly reduced and the main communication
is with the GRAM service.

%
% TODO: Condor-C aka Condor-to-Condor, when the LRMS is Condor, and using
% Condor-G, there is no need for additional Condor daemons, and the two
% Condor instances can be coupled.
%

\paragraph{Interface}

In a pool, user tasks are represented through job description files and
submitted to a (user-)\impterm{agent} via command-line tools. The main HTCondor
distribution provided command line utils to setup these Glideins. These are now
replaced by Bosco.

% -----------------------------------------------------------------------------
%
%\subsubsection{Corral}
%\onote{Corral seems to be a component in the glidein-WMS landscape and
%not an independent pilot-job implementation. I don't think that we should
%dedicate an extra section to it.}
%\aznote{SJ asked for an investigation of Corral -- not sure that this
%deserves a full analysis at this point on a technical level despite
%being commonly used, open to suggestions}

%Corral is designed to allow hybrid HTC/HPC execution, in which
%many small jobs may be executed in conjunction with larger runs.
%\cite{Rynge:2011:EUG:2116259.2116599}
%\aznote{Back this up w/ paper refs -- paper is somewhat dated,
%verify this is still true today}.  Corral operates as a Glidein
%WMS frontend\aznote{main reason that I think we shouldn't include
%Corral in its own section...}, where GlideinWMS manages the size
%of Condor glide-in pools.
%\begin{itemize}
%\item Workflow - handled by Pegasus workflow management system
%  \aznote{True in the paper I am using, but not handled by Corral itself, so should we include this?}
%\item Placeholder - handled by multislot requests
%  \aznote{multislot request requests a single large
%    GRAM job, and then starts glideins within this container}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% -----------------------------------------------------------------------------
%
%\paragraph{BoSCO}

%\onote{It seems that BOSCO is some sort of a user-space version of Condor.
%It can interface with single clusters as well as complex HTC grids (GlideinWMS).
%Multiple resource-scanrios are possible. There is a bosco-submit node which
%holds the user jobs. Bosco uses Condor glidein (-agents) internally as a
%resource overlay. The glideins pull data from the bosco-submit node. The
%main difference between BOSCO and Condor (even though both expose the same
%user API) is that BOSCO allows ad-hoc usage, while Condor requires a rather
%complex setup. In that regard, BOSCO is somehwat similar to BigJob.
%This page is somehwat insightful: http://bosco.opensciencegrid.org/about/}

%\subsubsection{Bosco}
%% \\
%% \begin{itemize}
%% \item Condor used as batch system/user interface
%% \item Single submit model for different cluster types (LSF/PBS/SGE/etc) via SSH with \texttt{BLAHPD}
%% \item Campus Factory (condor overlay generator) creates glideins, checks users queue for idle jobs,
%%   enforces submission policies
%% \item Bosco = ``BLAHPD Over SSH Condor Overlay''
%% \item Workstation-based (run Bosco client locally)
%% \item Multi-user (Bosco workstation install can be used by multiple researchers)
%% \item Supports multiple cluster submission (but what about coordination...)

BoSCO is a user-space job submission system based on HTCondor. BoSCO was
designed to allow individual users to utilize heterogeneous HPC and grid
computing resources through a uniform interface. Supported backends include
PBS, LSF and GridEngine clusters as well as other grid resource pools managed
by HTCondor. BoSCO supports an agent-based (\textit{glidein}/worker) and a
native job execution mode through a single user-interface.

BoSCO exposes the same \impterm{ClassAd}-based user interface as HTCondor.
However, the backend implementation for job management and resource
provisioning is significantly more lightweight than in HTCondor and it
explicitly allows for \textit{ad hoc} user-space deployment. BoSCO provides a
\pilot system that does not require the user to have access to a
centrally-administered HTCondor campus grid or resource pool. The user has
direct control over \pilot agent provisioning (via the \impterm{bosco\_cluster}
command) and job-to-resource binding via \impterm{ClassAd} requirements.

The overall architecture of BoSCO is very similar to that of HTCondor. The
\impterm{BoSCO submit-node} (analogous to Condor \impterm{schedd}) provides the
central job submission service and manages the job queue as well as the worker
agent pool. Worker agents communicate with the \impterm{BoSCO submit-node} via
pull-requests (TCP). They can be dynamically added and removed to a
\impterm{BoSCO submit-node} by the user. BoSCO can be installed in user-space
as well as in system space. In the former case, worker agents are exclusively
available to a single user, while in the latter case, worker agents can be
shared among multiple users. The client-side tools to submit, control and
monitor BoSCO jobs are the same as in Condor (\impterm{condor\_submit},
\impterm{condor\_q}, etc).

%\impterm{CorralWMS:}
CorralWMS is an alternative frontend for GlideinWMS-based DCRs. It
replaces or complements the regular GlideinWMS frontend with an alternative
API which is targeted towards workflow execution. Corral was initially designed
as a standalone pilot (glidein) provisioning system for the Pegasus workflow
system where user workflows often produced workloads consisting of many
short-running jobs as well as mixed workloads consisting of HTC and HPC jobs.

Over time, Corral has been integrated into the GlideinWMS stack as CorralWMS.
While CorralWMS still provides the same user interface as the initial, stand-
alone version of Corral, the underlying pilot (glidein) provisioning is now
handled by the GlideinWMS factory.

%\paragraph{Interoperability}

\paragraph{Multitenancy}

The main differences between the GlideinWMS and the CorralWMS front-ends lie
with identity management and resource sharing. While GlideinWMS pilots
(glidins) are provisioned on a per-VO base and shared/re-used among members of
that VO, CorralWMS pilots (glideins) are bound to one specific user via
personal X.509 certificates. This enables explicit resource provisioning in
non-VO centric environments, which includes many of the HPC clusters that are
part of U.S. national cyberinfrastructure (e.g., XSEDE).

\paragraph{Robustness}

As stated before, one of the initial drivers for Condor was the heterogenuous
environments and the need to provide a robust runtime environment for the
application by means of the sandbox.
Additionally, for applications that support it, Condor provides checkpointing
which allows applications to be restarted in case of failure or migrated to
another resource.

\paragraph{Security}

HTCondor inter-component communication is based on the home grown CEDAR
message-based communication library. It allows client and servers to negotiate
an appropriate security protocol. Regarding the execution of tasks, there is a
level of trust assumed between resource and application.

\paragraph{Files and Data}

The handling of files and data was initially not a primary design goal and did
not extend well beyond staging relatively small data into the sandbox. The
\impterm{Standard Universe} offers the ability to re-route file I/O back to the
\impterm{Shadow} of the \impterm{Agent}. Over the years, HTCondor has been
integrated with various data tools. Out of the same stable as HTCondor comes
NeST\cite{}.\mtnote{I do not understand the previous sentence.} NeST is a
software-only storage appliance that supports a myriad of storage protocols and
integrates with the \impterm{Matchmaker} to announce its availability. With
storage systems like NeST being available, one can then use a system like
Stork\cite{} to manage the transfers to and from this storage resources. Stork
can make educated decisions about data placement by coordinating with the
\impterm{Matchmaker}. Finally, Parrot\cite{} is a mechanism that offers a POSIX
I/O interface to applications for files that are otherwise only available
remotely.

%\paragraph{Performance and Scalability}

\paragraph{Development Model}

HTCondor is used in multiple different contexts: the HTCondor project, the
HTCondor software, and HTCondor grids. But even if we only look at the software
parts of the landscape, we are faced with a plethora of concepts, components,
and services that have been grown and curated for the past 20 years.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MyCluster
%
\subsubsection{MyCluster}
\label{sec:mycluster}

MyCluster~\cite{1652061,mycluster-clade,mycluster-cluster} was developed to allow users to submit and manage jobs
across heterogeneous NSF TeraGrid resources in a uniform, on-demand manner.
TeraGrid, the predecessor of XSEDE, existed as a group of compute clusters
connected by high-bandwidth links facilitating the movement of data, but with
many different middlewares requiring cluster-specific submission scripts.
MyCluster allowed all cluster resources to be aggregated into one personal
cluster with a unified interface, being either SGE, OpenPBS or Condor. This
enhancement to user control was envisioned as a means of allowing users to
submit and manage thousands of jobs at once and across heterogeneous DCRs, while
providing a familiar and homogeneous interface for submission.

Applications are launched via MyCluster in a ``traditional'' HPC manner, via a
\impterm{virtual login session} which contains usual queuing commands to submit
and manage jobs. This means that applications do not need to be explicitly
rewritten to make use of MyCluster functionality; rather, MyCluster provides
user-level \pilot capabilities which users can then use to schedule their
applications.

MyCluster was designed for and successfully executed on NSF TeraGrid resources,
enabling large-scale cross-site submission of ensemble job submissions via its
virtualized cluster interface. This approach makes the \vocab{multi-level
scheduling} abilities of \pilot implicit. Rather than directly \prop{binding}
tasks to individual TeraGrid resources, users allow the virtual grid overlay to
\vocab{schedule} tasks to multiple allocated TeraGrid sites presented as a
single cohesive, unified resource.

\paragraph{Task Execution Modes}

MyCluster's \prop{Task Execution Modes} are explicitly limited to
``serial'' tasks although it is unspecified whether this also confines
it to single core \vocab{tasks} only.

\paragraph{Pilot Resource Capabilities}

As per the earlier description, MyCluster was specifically targeted to
TeraGrid resources and thereby the \prop{Pilot Resource Capabilities} are those
of the TeraGrid, being HPC resources. As the \vocab{Task Execution Modes} are
limited to serial tasks, the only relevant resource that is exposed is the
``CPU'' or ``core''.

\paragraph{Resource Interaction}

The \prop{Resource Interaction} is exclusively by the \impterm{Agent Manager}
through Globus GRAM to start a \impterm{Proxy Manager} at each site.

\paragraph{Workload Semantics}

All \vocab{tasks} are considered independent and no other \prop{Workload
Semantic} is described.

\paragraph{Task Binding Characteristics}

As the \pilots are not exposed (and the \vocab{tasks} are considered
homogeneous) there are no explicitly controllable \vocab{Task Binding
Characteristics}. This in effect makes it a \vocab{Late Binding} mechanism.

\paragraph{Overlay Management}

With respect to \vocab{Overlay Management} MyCluster allows the user to
configure the number of \impterm{Proxies} per site, the number of CPUs per
\impterm{Proxy} and the list of sites to submit to. All resources that are
acquired through the \impterm{Proxy Managers} are pooled together. These
resource specifications are just requests, it depends on the behavior of the
queue whether (and when) these resources are actually acquired. In addition to
these static resource specifications, MyCluster includes a mode of operation
where \impterm{Proxies} can be \impterm{Migrated} to other sites. The rationale
for \impterm{migration} is that the weight of the resource requests can be
moved to the site with the shortest queuing time.

\paragraph{Architecture}

When the user starts a session via the \impterm{vo-login} command a
\impterm{Agent Manager} is instantiated. The \impterm{Agent Manager} in turn
starts a \impterm{Proxy Manager} at every resource site gateway host through
Globus GRAM and a \impterm{Master Node Manager} at the local client machine.
The \impterm{Proxy Manager} has an accompanying \impterm{Submission Agent} that
also runs on the resource gateway. The \impterm{Submission Agent} interacts
with the local queuing systems and submits the \impterm{Job Proxy} that
launches a \impterm{Task Manager} process on a compute host. The \impterm{Task
Manager} starts one or more \impterm{Slave Node Manager} processes on all the
allocates compute hosts. Ultimately, the \impterm{Slave Node Manager} in
control of the worker node. Depending on the configuration, the \impterm{Slave
Node Manager} starts the Condor, SGE, or OpenPBS job-starter daemons, which in
turn connect back to their master daemon started earlier on the local client
machine by the \impterm{Master Node Manager}.

\paragraph{Coordination and Communication}

The communication between \impterm{Proxy Manager} and \impterm{Agent Manager}
is over TCP.

\paragraph{Security}

The TCP communication channels are not encrypted but do use GSI-based
authentication and has measures in place to prevent replay attacks.

\paragraph{Robustness}

The \impterm{Agent Manager} maintains little state. It is assumed that ``lost''
\impterm{Proxy Managers} will re-connect back when they recover.
The \impterm{Proxy Manager} is restartable and will try to re-establish the
\impterm{Proxies} based on a last-known state stored on disk when they got
lost due to exceeding wallclock limitations or node reboots;

\paragraph{Interoperability}

The primary goal of MyCluster was \prop{Interoperability} over multiple
TeraGrid resources.

\paragraph{Interface}

The \prop{Interface} of MyCluster depends on the choice of the underlying
system as that one is exposed through the \impterm{Virtual Login
Session}.\mtnote{I do not understand the previous sentence, especially `as that
one'.} The interface also allows for users to interactively monitor the status
of their \vocab{\pilots} and \vocab{tasks}.

\paragraph{Multitenancy}

The created cluster resides in userspace, and may be used to marshal multiple
resources ranging from small local resource pools (e.g.  departmental Condor or
SGE systems) to large HPC installations. The clusters can be created per user,
per experiment or to contribute to the resources of a local cluster.

\paragraph{Files and Data}

MyCluster doest not offer and file staging capabilities other than those of the
underlying systems provide. This means that it exposes the file capabilities of
Condor, but doesn't offer any file staging capabilities when using SGE.

\paragraph{Development Model}

MyCluster is no longer developed or supported after the TeraGrid project came
to a conclusion in 2011. \mtnote{Should we order the descriptions of the pilot
systems in 4.2 starting from those that are not available any more?}

\paragraph{Conclusion}

MyCluster illustrates how an approach aimed at \vocab{multi-level scheduling}
by marshaling multiple heterogeneous resources lends itself to\mtnote{is
consistent with?} a \pilot-based approach. The fact that the researchers behind
it developed a complete \pilot system while working toward
interoperability/uniform access is a testament to the usefulness of \pilots in
addressing these problems. The end result is a complete \pilot system, despite
the authors of the system being constructed\mtnote{I do not understand what
this means} not having used the word ``pilot'' once in their main publication.

While not an official product, a recent and similar approach has been adopted
at NERSC. The operators of the Hopper cluster provide a tool called MySGE which
allows for the user to provision a personal SGE cluster on Hopper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% PanDA
%
\subsubsection{\panda}
\label{sec:panda}

\panda (Production and Distributed Analysis)~\cite{1742-6596-331-7-072069} was
developed to provide a multi-user workload management system (WMS) for
ATLAS~\cite{aad2008atlas}. ATLAS is a particle detector at the Large Hadron
Collider at CERN that requires a WMS to handle large numbers of jobs for their
data-driven processing workloads. In addition to the logistics of handling
large-scale job execution, ATLAS needed also integrated monitoring for analysis
of system state and a high degree of automation to reduce the need for
user/administrative intervention.

\panda has been initially deployed as an HTC-oriented, multi-user WMS system
for ATLAS, consisting of ~100 heterogeneous computing
sites~\cite{maeno_pd2p:_2012}. Recent improvements to \panda have extended the
range of deployment scenarios to HPC and Cloud-based DCRs making \panda a
general-use \pilot system~\cite{nilsson2012recent}.

\paragraph{Architecture}

\panda's central component is called the \impterm{\panda server}. It keeps track
of jobs, matches jobs with pilots and resources, dispatches jobs to resources,
and coordinates data management.
The \panda server is implemented as a stateless multi-process REST web service,
with a database backend, to which it communicates through the \impterm{Bamboo}
interface.
A central job queue allows users to submit jobs to distributed resources in a
uniform manner.
\panda's \vocab{\pilot} is called, appropriately enough, a \impterm{Pilot}, and
handles the execution environment.
When pilots are launched, they collects information about their worker nodes
that is sent back to the job dispatcher.
The main functionality of the \pilot is the so-called multi-job loop.  It
connects to the \panda server to ask for jobs to run.  If a matching job does
not exist, the pilot ends. If a job does exist, the pilot forks a separate
thread (\impterm{runJob}) for the job and starts monitoring its execution from
the \impterm{monitor}.

\paragraph{Pilot Deployment}

An independent component, AutoPyFactory\cite{Caballero:2012ka}, manages the
delivery of \pilots to worker nodes via a number of schedulers (\impterm{pilot
factories}) serving the sites at which \panda operates.
This component handles \pilot submission, management, and monitoring.
AutoPyFactory supersedes the first generation component called
\impterm{PandaJobScheduler}, as well the second generation one called
\impterm{AutoPilot}).

\paragraph{Task Binding}

Workload jobs are assigned to activated and validated pilots by the \panda
server based on brokerage criteria like data locality and resource
characteristics.

\paragraph{Workload Semantics}

On the \pilot level there are no rich workload semantics.
Workload semantics exists only in the \impterm{ProdSys} component that feeds
jobs into the database from which the \panda server retrieves its tasks.

\paragraph{Task Execution Modes}

Given the nature of the application workload and the targeted DCR,
\panda is targeted \mtnote{exclusively?} towards sequential program execution.

\paragraph{Pilot Resources}

\panda began as a specialized \pilot for the LCG Grid, and has been
extended into a generalized \pilot which is capable of working across other
grids as well as HPC\cite{} and cloud\cite{} resources.

\paragraph{Resource Interaction}

\panda traditionally relies on Condor-G for its interaction with DCIs.
For recent endeavors in HPC \panda uses SAGA to interface with the queuing
systems.

\paragraph{Coordination and Communication}

Communication between the client and the server is based on HTTP/S RESTful
communication.
Coordination of the workflow is maximally asynchronous.

\paragraph{Interface}

Jobs are submitted to \panda via a simple Python client.
The client needs to define job sets, their associated datasets as well as
the input and output files at submission.
The client API has been used to implement the PanDA front ends for ATLAS
production, distributed analysis and US regional production.

\paragraph{Multitenancy}

\panda is a \textit{true} multi-user system, as both the WMS manages workloads
for multiple users (the whole ATLAS community), but also the \pilots are
capable of executing tasks on the resources for multiple users by means of
glExec\cite{}.
The latter enables the re-use of \pilots amonst users which has a positive
effect on latency from a user perspective.

\paragraph{Files and Data}

\texttt{\panda Dynamic Data Placement} \cite{maeno_pd2p:_2012} allows for
additional, automatic data management by replicating popular or backlogged
input data to underutilized resources for later computations and enables jobs
to be placed where the data already exists.

\paragraph{Robustness}

The wrapper of the \pilot takes care of disk space monitoring, and enables job
recovery of failed jobs and restarting from crash whenever possible.

For production jobs the majority of the errors are site or system related,
while for user analysis jobs the most common issues are related to application
software.  Pilot mechanisms like job recovery contribute to the robustness
against site related failures.

A comprehensive monitoring system supporting production and analysis
operations, including site and data management information for problem
diagnostics, usage and quota accounting, and health and performance monitoring
of \panda subsystems and the computing facilities being utilized.

\paragraph{Interoperability}

\panda enables the concurrent execution of workload on heterogeneous resources.

\paragraph{Security}

Job specifications from the client are sent to the PanDA server via a secure
HTTPS connection authenticated with a GSI certificate proxy.
gLExec is used to use the job user's identity instead of Pilot credentials for
executing tasks.

\paragraph{Performance and Scalability}

\panda has been used to process approximately a million jobs a
day~\cite{pandapresentation2013-06} which handle simulation, analysis, and
other work~\cite{maeno_pd2p:_2012}. The ATLAS experiment itself produces
several petabytes of data a year which must be processed and analyzed.

The \panda system is serving over 100k production jobs and 35k user analysis
jobs concurrently.

%PanDA manages \O(102) sites, \O(105) cores, O(108) jobs per year, O(103) users,
%and ATLAS data volume is O(1017) bytes.

\paragraph{Development Model}

\panda is a multi-stakeholder project that has been active for many years. As
such, it went through many iterations. We try to refer to the current state as
much as possible.
Current funding for \panda enables the project to reach out to new user
communities.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% RADICAL-Pilot
%
\subsubsection{RADICAL-Pilot}
\label{sec:radicalpilot}

The authors of this paper (the RADICAL group) have been engaged in theoretical
and practical aspects of \pilot systems for the past several years. In addition
to formulating the P* Model\cite{} which by most accounts is the first complete
conceptual model of \pilots, the RADICAL group is responsible for the
development and maintenance of RADICAL-Pilot\cite{}. RADICAL-Pilot is the
groups long-term effort for creating a production level \pilot system. The
effort is build upon the experience gained from developing, maintaining and
using BigJob\cite{}, an initial prototype of \pilot system.

\paragraph{Pilot Resource Capabilities}

RADICAL-Pilot is mainly tailored towards HPC environments such as the resources
of XSEDE and NERSC\cite{}. The primary \vocab{resource} that is exposed is the
compute node, or more specifically the cores within such a node.

\paragraph{Resource Interaction}

RADICAL-Pilot uses the Simple API for Grid Applications
(SAGA)~\cite{ogf-gfd-90, sagastuff} in order to interface with different DCIs.
Thanks to SAGA, RADICAL-Pilot submits the Pilot Agent as a \vocab{job} through
the reservation or queuing system of the DCI. Once the \impterm{Agent} is
started, there is no direct \prop{resource interaction} with the DCI except for
monitoring of the state of the \impterm{Agent} process. Adopting SAGA has
enabled RADICAL-Pilot to expand to many of the changing and evolving
architectures and middleware.

\paragraph{Overlay Management}

The programming interface of RADICAL-Pilot enables (and requires) the explicit
configuration of the \vocab{overlay}. The user is expected to specify the size,
type and destination of the pilot(s) he wants to add to the \vocab{overlay}.
Through the concept of \impterm{UnitManagers} on the client side, the user can
group \pilots together and foster them under a single scheduler or have
multiple independent \impterm{UnitManagers} and \pilots, all with their own
\impterm{scheduler}. On HPC style systems there is typically one \pilot that
manages all the resources that are allocated to that specific run, but there
can be as many \pilots per resource as the policies allows concurrently running
jobs.

\paragraph{Workload Semantics}

The \vocab{workload} within RADICAL-Pilot consists of \impterm{ComputeUnits}
that represent \vocab{tasks}. From the perspective of RADICAL-Pilot there are
no dependencies between these \impterm{ComputeUnits} and once a
\impterm{ComputeUnit} is given to the control of RADICAL-Pilot it is assumed to
be ready to be executed. RADICAL-Pilot includes the concept of \impterm{kernel
abstractions}. These are generic application descriptions that can be
configured on a per-source basis. Once an \impterm{application kernel} is
configured for a specific resource and available in the repository, the user
only needs to refer to the \impterm{application kernel}. RADICAL-Pilot then
takes care of setting up the right environment and executing the right
executable. This facility is especially useful in the case of
\vocab{late-binding}, when it is unknown on which \pilot (and thereby resource)
a task will run at the time of its submission.

\paragraph{Task Binding Characteristics}

\vocab{Task} to \vocab{resource} \vocab{binding} can be done either implicitly
or explicitly. A user can bind a task explicitly to a pilot, and thereby to the
resource the pilot is scheduled to, making this a form of early binding. The
user can also submit a task to a unit manager that schedules units to multiple
pilots. In this case it is the semantics of the scheduler that decides on the
task binding. RADICAL-Pilot supports multiple schedulers that can be selected
at runtime. Two schedulers are currently implemented: (i) a round-robin
scheduler that binds incoming tasks to associated pilots in a rotating fashion,
irrespective of their state, and thereby performing \vocab{early binding}. (ii)
a BackFilling scheduler that binds tasks to pilots once the pilot is active and
has available resources, thereby making it a \vocab{late binding} scheduler.

\paragraph{Task Execution Modes}

RADICAL-Pilot supports two type of \vocab{tasks}. One is the generic single
node task, that can be single core, or multi-core threaded/OpenMP applications.
In addition, RADICAL-Pilot has extensive support for MPI applications, where it
supports a large variety of launch methods that are required to successfully
run MPI applications on a wide range of HPC systems. The \impterm{launch
methods} are a modular system and new \impterm{launch methods} can be added
when required.

\paragraph{Architecture}

From a high level perspective RADICAL-Pilot consist of two components. A client
side python module that is programmable through an API and is used by
scripts/applications, and the agent (or multiple agents) that runs on a
resource and executes tasks.

One of the primary features of RADICAL-Pilot is that it completely lives in
user-space and thereby requires no collaboration from the resource owner /
administrator. Other than an online database that maintains state during the
lifetime of a session, there are no other (persistent) service components that
RADICAL-Pilot relies on.

\paragraph{Coordination and Communication}

MongoDB is the bridge between the client side and agent. MongoDB is a
document-oriented database that needs to run in a location that is accessible
for both type of components. The client side publishes the \vocab{workload} in
the MongoDB which is picked up and then executed by the agent. The agent in
turn publishes the status and the output of the \impterm{ComputeUnit} back to
the MongoDB which can be retrieved by the client side. The main advantage of
this model is that it works in situations where there is no direct
communication channel between the client host and the compute resource, which
is a very common scenario. The MongoDB database also offers (some) persistency
which allows the client side to re-connect to active sessions stored in the
database. The main drawback of the use of a database for communication is that
all communication is by definition indirect with potential performance
bottlenecks as a consequence.

\paragraph{Interface}

The client side RADICAL-Pilot is a Python library that is programmable through
the so called \impterm{Pilot-API}. The application-level programmability that
RADICAL-Pilot offers was incorporated as a means of giving to the end-user
control over their job management. Users define their pilots and their tasks
and submit them through the \impterm{Unit Manager}. They can query the state of
pilots and units or rely on a \impterm{callback} mechanism to get notified of
any state change that they are interested in. Users define their pilots and
tasks in a declarative way, but the flow of activity is more of an imperative
style.\mtnote{Should we mention that all this is implemented in Python?}

\paragraph{Interoperability}

RADICAL-Pilot allows for \prop{interoperability} with multiples types of
resources (heterogeneous schedulers and clouds/grids/clusters) at one time. It
does not have \prop{interoperability} across different \pilot systems.

\paragraph{Multitenancy}

RADICAL-Pilot is installable by a user onto the resource of his or her choice.
It is capable of running only in ``single user'' mode; that is, a \pilot
belongs to the user who spawned it and cannot be accessed by other users.

\paragraph{Robustness}

For fault-tolerance RADICAL-Pilot relies on the user to write the application
logic to achieve \prop{Robustness}, to assist in that process, it does report
task failures.

\paragraph{Security}

The compute resource \prop{Security} aspects of RADICAL-Pilot are that it
follows security measures of the respective scheduler systems (i.e. policies
like allocations, etc).\mtnote{I do not understand the previous sentence.}

\paragraph{Files and Data}

RADICAL-Pilot supports many modes of \impterm{data staging} although all of
them are exclusively file based. Files can be staged at both \pilot and
\impterm{ComputeUnit} level, and they can be transferred to and from the
staging area of the pilot and of the compute unit \impterm{sandbox} from and to
the client machine. RADICAL-Pilot also allows the transfer of files from third
party locations onto the compute resource. In addition, the ComputeUnits can be
instructed to use files from the pilot sandbox. In this way, the pilot sandbox
is turned into a shared file storage for all its compute units. The user has
the option to Move, Copy or Link data based on the performance and usage
criteria.

\paragraph{Performance and Scalability}

RADICAL-Pilot is one of the largest pilot based consumers of resources on
XSEDE\cite{}. Many thousands concurrent ComputeUnits can be managed by each
\pilot Agent and scalability can be achieved in many dimensions as multiple
pilots can easily be aggregated over multiple distinct DCIs.

\paragraph{Development Model}

RADICAL-Pilot is an Open Source project released under the MIT license. Its
development is public and takes place on a public Github repository. The
project is under active funding and development and has a number of external
projects that use it as a foundation layer for job execution.

%%%%%%%%%%%%%%%%%
